
[32m2022-10-10T22:51:02 | mmf.utils.checkpoint: [39mLoading checkpoint
[31m[5mWARNING[39m[25m [32m2022-10-10T22:51:05 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-10T22:51:05 | mmf: [39mKey distributed is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-10T22:51:05 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-10T22:51:05 | mmf: [39mKey distributed is not present in registry, returning default value of None
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mPretrained model loaded
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCurrent num updates: 0
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCurrent iteration: 0
[32m2022-10-10T22:51:05 | mmf.utils.checkpoint: [39mCurrent epoch: 0
[32m2022-10-10T22:51:05 | mmf.trainers.mmf_trainer: [39m===== Model x=====
[32m2022-10-10T22:51:05 | mmf.trainers.mmf_trainer: [39mVisualBERT(
  (model): VisualBERTForClassification(
    (bert): VisualBERTBase(
      (embeddings): BertVisioLinguisticEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (token_type_embeddings_visual): Embedding(2, 768)
        (position_embeddings_visual): Embedding(512, 768)
        (projection): Linear(in_features=2048, out_features=768, bias=True)
      )
      (encoder): BertEncoderJit(
        (layer): ModuleList(
          (0): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Sequential(
      (0): BertPredictionHeadTransform(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (1): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-10-10T22:51:05 | mmf.utils.general: [39mTotal Parameters: 112044290. Trained Parameters: 593666
[32m2022-10-10T22:51:05 | mmf.trainers.core.training_loop: [39mStarting training...
[32m2022-10-10T22:51:59 | mmf.trainers.callbacks.logistics: [39mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7353, train/hateful_memes/cross_entropy/avg: 0.7353, train/total_loss: 0.7353, train/total_loss/avg: 0.7353, max mem: 750.0, experiment: hmd, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 526ms, time_since_start: 01m 03s 884ms, eta: 03h 25m 23s 449ms
[32m2022-10-10T22:52:53 | mmf.trainers.callbacks.logistics: [39mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.7353, train/hateful_memes/cross_entropy/avg: 0.7443, train/total_loss: 0.7353, train/total_loss/avg: 0.7443, max mem: 801.0, experiment: hmd, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.89, time: 53s 456ms, time_since_start: 01m 57s 341ms, eta: 03h 20m 26s 387ms
[32m2022-10-10T22:53:46 | mmf.trainers.callbacks.logistics: [39mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.7353, train/hateful_memes/cross_entropy/avg: 0.7154, train/total_loss: 0.7353, train/total_loss/avg: 0.7154, max mem: 863.0, experiment: hmd, epoch: 1, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.89, time: 53s 360ms, time_since_start: 02m 50s 702ms, eta: 03h 19m 09s 857ms
[32m2022-10-10T22:54:39 | mmf.trainers.callbacks.logistics: [39mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6576, train/hateful_memes/cross_entropy/avg: 0.6909, train/total_loss: 0.6576, train/total_loss/avg: 0.6909, max mem: 914.0, experiment: hmd, epoch: 1, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.89, time: 53s 408ms, time_since_start: 03m 44s 110ms, eta: 03h 18m 25s 295ms
[32m2022-10-10T22:55:32 | mmf.trainers.callbacks.logistics: [39mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6576, train/hateful_memes/cross_entropy/avg: 0.6797, train/total_loss: 0.6576, train/total_loss/avg: 0.6797, max mem: 939.0, experiment: hmd, epoch: 1, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.92, time: 52s 927ms, time_since_start: 04m 37s 037ms, eta: 03h 15m 43s 636ms
[32m2022-10-10T22:56:14 | mmf.trainers.callbacks.logistics: [39mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6576, train/hateful_memes/cross_entropy/avg: 0.6816, train/total_loss: 0.6576, train/total_loss/avg: 0.6816, max mem: 939.0, experiment: hmd, epoch: 2, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 2.38, time: 42s 069ms, time_since_start: 05m 19s 107ms, eta: 02h 34m 50s 909ms
[32m2022-10-10T22:56:54 | mmf.trainers.callbacks.logistics: [39mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6810, train/hateful_memes/cross_entropy/avg: 0.6815, train/total_loss: 0.6810, train/total_loss/avg: 0.6815, max mem: 977.0, experiment: hmd, epoch: 2, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 2.56, time: 39s 969ms, time_since_start: 05m 59s 076ms, eta: 02h 26m 25s 915ms
[32m2022-10-10T22:57:37 | mmf.trainers.callbacks.logistics: [39mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.6576, train/hateful_memes/cross_entropy/avg: 0.6763, train/total_loss: 0.6576, train/total_loss/avg: 0.6763, max mem: 977.0, experiment: hmd, epoch: 2, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 2.38, time: 42s 526ms, time_since_start: 06m 41s 603ms, eta: 02h 35m 04s 114ms
[32m2022-10-10T22:58:24 | mmf.trainers.callbacks.logistics: [39mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6576, train/hateful_memes/cross_entropy/avg: 0.6644, train/total_loss: 0.6576, train/total_loss/avg: 0.6644, max mem: 1027.0, experiment: hmd, epoch: 2, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 099ms, time_since_start: 07m 28s 702ms, eta: 02h 50m 56s 014ms
[32m2022-10-10T22:59:14 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T22:59:14 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T22:59:16 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T22:59:16 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T22:59:16 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.6396, train/hateful_memes/cross_entropy/avg: 0.6592, train/total_loss: 0.6396, train/total_loss/avg: 0.6592, max mem: 1039.0, experiment: hmd, epoch: 2, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.92, time: 52s 392ms, time_since_start: 08m 21s 095ms, eta: 03h 09m 14s 610ms
[32m2022-10-10T22:59:16 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T22:59:16 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T22:59:16 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T22:59:29 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T22:59:29 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T22:59:29 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T22:59:32 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-10T22:59:32 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T22:59:35 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T22:59:35 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7995, val/total_loss: 0.7995, val/hateful_memes/accuracy: 0.5060, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5512, num_updates: 1000, epoch: 2, iterations: 1000, max_updates: 22000, val_time: 18s 787ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.551151
[32m2022-10-10T23:00:23 | mmf.trainers.callbacks.logistics: [39mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.6396, train/hateful_memes/cross_entropy/avg: 0.6498, train/total_loss: 0.6396, train/total_loss/avg: 0.6498, max mem: 1039.0, experiment: hmd, epoch: 3, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 665ms, time_since_start: 09m 27s 549ms, eta: 02h 51m 20s 819ms
[32m2022-10-10T23:00:56 | mmf.trainers.callbacks.logistics: [39mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.6396, train/hateful_memes/cross_entropy/avg: 0.6638, train/total_loss: 0.6396, train/total_loss/avg: 0.6638, max mem: 1039.0, experiment: hmd, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 3.03, time: 33s 161ms, time_since_start: 10m 711ms, eta: 01h 58m 38s 345ms
[32m2022-10-10T23:01:31 | mmf.trainers.callbacks.logistics: [39mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.6576, train/hateful_memes/cross_entropy/avg: 0.6728, train/total_loss: 0.6576, train/total_loss/avg: 0.6728, max mem: 1039.0, experiment: hmd, epoch: 3, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 2.94, time: 34s 643ms, time_since_start: 10m 35s 355ms, eta: 02h 03m 20s 708ms
[32m2022-10-10T23:02:07 | mmf.trainers.callbacks.logistics: [39mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.6396, train/hateful_memes/cross_entropy/avg: 0.6648, train/total_loss: 0.6396, train/total_loss/avg: 0.6648, max mem: 1039.0, experiment: hmd, epoch: 3, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 2.78, time: 36s 322ms, time_since_start: 11m 11s 677ms, eta: 02h 08m 41s 801ms
[32m2022-10-10T23:02:48 | mmf.trainers.callbacks.logistics: [39mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.6576, train/hateful_memes/cross_entropy/avg: 0.6653, train/total_loss: 0.6576, train/total_loss/avg: 0.6653, max mem: 1039.0, experiment: hmd, epoch: 3, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 2.44, time: 41s 427ms, time_since_start: 11m 53s 104ms, eta: 02h 26m 04s 410ms
[32m2022-10-10T23:03:34 | mmf.trainers.callbacks.logistics: [39mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.6576, train/hateful_memes/cross_entropy/avg: 0.6693, train/total_loss: 0.6576, train/total_loss/avg: 0.6693, max mem: 1039.0, experiment: hmd, epoch: 4, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 2.22, time: 45s 788ms, time_since_start: 12m 38s 893ms, eta: 02h 40m 39s 862ms
[32m2022-10-10T23:03:54 | mmf.trainers.callbacks.logistics: [39mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.6576, train/hateful_memes/cross_entropy/avg: 0.6659, train/total_loss: 0.6576, train/total_loss/avg: 0.6659, max mem: 1039.0, experiment: hmd, epoch: 4, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 5.26, time: 19s 494ms, time_since_start: 12m 58s 388ms, eta: 01h 08m 04s 006ms
[32m2022-10-10T23:04:15 | mmf.trainers.callbacks.logistics: [39mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.6396, train/hateful_memes/cross_entropy/avg: 0.6604, train/total_loss: 0.6396, train/total_loss/avg: 0.6604, max mem: 1039.0, experiment: hmd, epoch: 4, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 4.76, time: 21s 129ms, time_since_start: 13m 19s 518ms, eta: 01h 13m 24s 799ms
[32m2022-10-10T23:04:37 | mmf.trainers.callbacks.logistics: [39mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.6396, train/hateful_memes/cross_entropy/avg: 0.6567, train/total_loss: 0.6396, train/total_loss/avg: 0.6567, max mem: 1039.0, experiment: hmd, epoch: 4, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 4.55, time: 22s 018ms, time_since_start: 13m 41s 536ms, eta: 01h 16m 07s 387ms
[32m2022-10-10T23:05:01 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:05:01 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:05:02 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:05:05 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:05:05 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.6349, train/hateful_memes/cross_entropy/avg: 0.6516, train/total_loss: 0.6349, train/total_loss/avg: 0.6516, max mem: 1039.0, experiment: hmd, epoch: 4, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 3.57, time: 28s 065ms, time_since_start: 14m 09s 602ms, eta: 01h 36m 32s 669ms
[32m2022-10-10T23:05:05 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:05:05 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:05:05 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:05:18 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:05:18 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:05:18 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:05:21 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-10T23:05:24 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:05:26 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:05:26 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.7645, val/total_loss: 0.7645, val/hateful_memes/accuracy: 0.5060, val/hateful_memes/binary_f1: 0.0000, val/hateful_memes/roc_auc: 0.5590, num_updates: 2000, epoch: 4, iterations: 2000, max_updates: 22000, val_time: 21s 463ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.558977
[32m2022-10-10T23:06:08 | mmf.trainers.callbacks.logistics: [39mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.6173, train/hateful_memes/cross_entropy/avg: 0.6476, train/total_loss: 0.6173, train/total_loss/avg: 0.6476, max mem: 1039.0, experiment: hmd, epoch: 4, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 2.38, time: 42s 148ms, time_since_start: 15m 13s 216ms, eta: 02h 24m 15s 913ms
[32m2022-10-10T23:06:37 | mmf.trainers.callbacks.logistics: [39mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.6173, train/hateful_memes/cross_entropy/avg: 0.6532, train/total_loss: 0.6173, train/total_loss/avg: 0.6532, max mem: 1039.0, experiment: hmd, epoch: 5, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 3.57, time: 28s 161ms, time_since_start: 15m 41s 377ms, eta: 01h 35m 54s 336ms
[32m2022-10-10T23:07:01 | mmf.trainers.callbacks.logistics: [39mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.6173, train/hateful_memes/cross_entropy/avg: 0.6523, train/total_loss: 0.6173, train/total_loss/avg: 0.6523, max mem: 1039.0, experiment: hmd, epoch: 5, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 4.17, time: 24s 128ms, time_since_start: 16m 05s 505ms, eta: 01h 21m 45s 391ms
[32m2022-10-10T23:07:28 | mmf.trainers.callbacks.logistics: [39mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.6121, train/hateful_memes/cross_entropy/avg: 0.6501, train/total_loss: 0.6121, train/total_loss/avg: 0.6501, max mem: 1039.0, experiment: hmd, epoch: 5, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 3.85, time: 26s 963ms, time_since_start: 16m 32s 468ms, eta: 01h 30m 53s 870ms
[32m2022-10-10T23:07:58 | mmf.trainers.callbacks.logistics: [39mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.6121, train/hateful_memes/cross_entropy/avg: 0.6495, train/total_loss: 0.6121, train/total_loss/avg: 0.6495, max mem: 1039.0, experiment: hmd, epoch: 5, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 3.33, time: 30s 137ms, time_since_start: 17m 02s 606ms, eta: 01h 41m 04s 965ms
[32m2022-10-10T23:08:29 | mmf.trainers.callbacks.logistics: [39mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.6114, train/hateful_memes/cross_entropy/avg: 0.6461, train/total_loss: 0.6114, train/total_loss/avg: 0.6461, max mem: 1039.0, experiment: hmd, epoch: 5, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 3.23, time: 31s 587ms, time_since_start: 17m 34s 193ms, eta: 01h 45m 24s 006ms
[32m2022-10-10T23:08:56 | mmf.trainers.callbacks.logistics: [39mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.6114, train/hateful_memes/cross_entropy/avg: 0.6499, train/total_loss: 0.6114, train/total_loss/avg: 0.6499, max mem: 1039.0, experiment: hmd, epoch: 6, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 3.85, time: 26s 092ms, time_since_start: 18m 286ms, eta: 01h 26m 36s 927ms
[32m2022-10-10T23:09:11 | mmf.trainers.callbacks.logistics: [39mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.5999, train/hateful_memes/cross_entropy/avg: 0.6479, train/total_loss: 0.5999, train/total_loss/avg: 0.6479, max mem: 1039.0, experiment: hmd, epoch: 6, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 6.67, time: 15s 596ms, time_since_start: 18m 15s 882ms, eta: 51m 30s 304ms
[32m2022-10-10T23:09:27 | mmf.trainers.callbacks.logistics: [39mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.6114, train/hateful_memes/cross_entropy/avg: 0.6509, train/total_loss: 0.6114, train/total_loss/avg: 0.6509, max mem: 1039.0, experiment: hmd, epoch: 6, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 6.67, time: 15s 762ms, time_since_start: 18m 31s 645ms, eta: 51m 47s 000ms
[32m2022-10-10T23:09:43 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:09:43 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:09:43 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:09:46 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:09:46 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.5999, train/hateful_memes/cross_entropy/avg: 0.6473, train/total_loss: 0.5999, train/total_loss/avg: 0.6473, max mem: 1039.0, experiment: hmd, epoch: 6, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 5.56, time: 18s 978ms, time_since_start: 18m 50s 623ms, eta: 01h 02m 01s 230ms
[32m2022-10-10T23:09:46 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:09:46 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:09:46 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:09:57 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:09:57 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:09:57 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:10:00 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-10T23:10:03 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:10:05 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:10:05 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, val/hateful_memes/cross_entropy: 0.7464, val/total_loss: 0.7464, val/hateful_memes/accuracy: 0.5120, val/hateful_memes/binary_f1: 0.0317, val/hateful_memes/roc_auc: 0.5657, num_updates: 3000, epoch: 6, iterations: 3000, max_updates: 22000, val_time: 19s 421ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.565713
[32m2022-10-10T23:10:35 | mmf.trainers.callbacks.logistics: [39mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.5999, train/hateful_memes/cross_entropy/avg: 0.6448, train/total_loss: 0.5999, train/total_loss/avg: 0.6448, max mem: 1039.0, experiment: hmd, epoch: 6, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 3.45, time: 29s 855ms, time_since_start: 19m 39s 903ms, eta: 01h 37m 03s 298ms
[32m2022-10-10T23:11:09 | mmf.trainers.callbacks.logistics: [39mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6421, train/total_loss: 0.5943, train/total_loss/avg: 0.6421, max mem: 1039.0, experiment: hmd, epoch: 7, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 3.03, time: 33s 640ms, time_since_start: 20m 13s 543ms, eta: 01h 48m 46s 749ms
[32m2022-10-10T23:11:29 | mmf.trainers.callbacks.logistics: [39mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6424, train/total_loss: 0.5943, train/total_loss/avg: 0.6424, max mem: 1039.0, experiment: hmd, epoch: 7, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 5.26, time: 19s 702ms, time_since_start: 20m 33s 246ms, eta: 01h 03m 22s 357ms
[32m2022-10-10T23:11:50 | mmf.trainers.callbacks.logistics: [39mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.5999, train/hateful_memes/cross_entropy/avg: 0.6428, train/total_loss: 0.5999, train/total_loss/avg: 0.6428, max mem: 1039.0, experiment: hmd, epoch: 7, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 4.76, time: 21s 518ms, time_since_start: 20m 54s 764ms, eta: 01h 08m 50s 476ms
[32m2022-10-10T23:12:12 | mmf.trainers.callbacks.logistics: [39mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6402, train/total_loss: 0.5943, train/total_loss/avg: 0.6402, max mem: 1039.0, experiment: hmd, epoch: 7, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 4.55, time: 22s 335ms, time_since_start: 21m 17s 100ms, eta: 01h 11m 04s 307ms
[32m2022-10-10T23:12:35 | mmf.trainers.callbacks.logistics: [39mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.5937, train/hateful_memes/cross_entropy/avg: 0.6389, train/total_loss: 0.5937, train/total_loss/avg: 0.6389, max mem: 1039.0, experiment: hmd, epoch: 7, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 4.55, time: 22s 281ms, time_since_start: 21m 39s 382ms, eta: 01h 10m 31s 072ms
[32m2022-10-10T23:12:59 | mmf.trainers.callbacks.logistics: [39mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.5896, train/hateful_memes/cross_entropy/avg: 0.6365, train/total_loss: 0.5896, train/total_loss/avg: 0.6365, max mem: 1039.0, experiment: hmd, epoch: 7, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 4.17, time: 24s 095ms, time_since_start: 22m 03s 477ms, eta: 01h 15m 50s 617ms
[32m2022-10-10T23:13:17 | mmf.trainers.callbacks.logistics: [39mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.5937, train/hateful_memes/cross_entropy/avg: 0.6360, train/total_loss: 0.5937, train/total_loss/avg: 0.6360, max mem: 1039.0, experiment: hmd, epoch: 8, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 5.88, time: 17s 804ms, time_since_start: 22m 21s 281ms, eta: 55m 44s 050ms
[32m2022-10-10T23:13:32 | mmf.trainers.callbacks.logistics: [39mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6375, train/total_loss: 0.5943, train/total_loss/avg: 0.6375, max mem: 1039.0, experiment: hmd, epoch: 8, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 6.67, time: 15s 594ms, time_since_start: 22m 36s 876ms, eta: 48m 32s 857ms
[32m2022-10-10T23:13:48 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:13:48 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:13:48 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:13:51 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:13:51 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.5943, train/hateful_memes/cross_entropy/avg: 0.6345, train/total_loss: 0.5943, train/total_loss/avg: 0.6345, max mem: 1039.0, experiment: hmd, epoch: 8, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 5.56, time: 18s 999ms, time_since_start: 22m 55s 875ms, eta: 58m 49s 408ms
[32m2022-10-10T23:13:51 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:13:51 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:13:51 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:13:59 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:13:59 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:13:59 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:14:02 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-10T23:14:05 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:14:08 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:14:08 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, val/hateful_memes/cross_entropy: 0.7185, val/total_loss: 0.7185, val/hateful_memes/accuracy: 0.5180, val/hateful_memes/binary_f1: 0.1107, val/hateful_memes/roc_auc: 0.5659, num_updates: 4000, epoch: 8, iterations: 4000, max_updates: 22000, val_time: 16s 715ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.565938
[32m2022-10-10T23:14:32 | mmf.trainers.callbacks.logistics: [39mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.5999, train/hateful_memes/cross_entropy/avg: 0.6338, train/total_loss: 0.5999, train/total_loss/avg: 0.6338, max mem: 1039.0, experiment: hmd, epoch: 8, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 4.17, time: 24s 516ms, time_since_start: 23m 37s 113ms, eta: 01h 15m 28s 844ms
[32m2022-10-10T23:14:57 | mmf.trainers.callbacks.logistics: [39mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.5999, train/hateful_memes/cross_entropy/avg: 0.6342, train/total_loss: 0.5999, train/total_loss/avg: 0.6342, max mem: 1039.0, experiment: hmd, epoch: 8, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 4.17, time: 24s 326ms, time_since_start: 24m 01s 440ms, eta: 01h 14m 28s 706ms
[32m2022-10-10T23:15:19 | mmf.trainers.callbacks.logistics: [39mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.5999, train/hateful_memes/cross_entropy/avg: 0.6346, train/total_loss: 0.5999, train/total_loss/avg: 0.6346, max mem: 1039.0, experiment: hmd, epoch: 9, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 4.55, time: 22s 651ms, time_since_start: 24m 24s 091ms, eta: 01h 08m 57s 540ms
[32m2022-10-10T23:15:39 | mmf.trainers.callbacks.logistics: [39mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.6046, train/hateful_memes/cross_entropy/avg: 0.6373, train/total_loss: 0.6046, train/total_loss/avg: 0.6373, max mem: 1039.0, experiment: hmd, epoch: 9, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 5.26, time: 19s 722ms, time_since_start: 24m 43s 813ms, eta: 59m 42s 262ms
[32m2022-10-10T23:15:59 | mmf.trainers.callbacks.logistics: [39mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.6046, train/hateful_memes/cross_entropy/avg: 0.6370, train/total_loss: 0.6046, train/total_loss/avg: 0.6370, max mem: 1039.0, experiment: hmd, epoch: 9, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 5.00, time: 20s 145ms, time_since_start: 25m 03s 959ms, eta: 01h 38s 362ms
[32m2022-10-10T23:16:20 | mmf.trainers.callbacks.logistics: [39mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.6046, train/hateful_memes/cross_entropy/avg: 0.6352, train/total_loss: 0.6046, train/total_loss/avg: 0.6352, max mem: 1039.0, experiment: hmd, epoch: 9, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 253ms, time_since_start: 25m 25s 213ms, eta: 01h 03m 36s 471ms
[32m2022-10-10T23:16:42 | mmf.trainers.callbacks.logistics: [39mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.6046, train/hateful_memes/cross_entropy/avg: 0.6383, train/total_loss: 0.6046, train/total_loss/avg: 0.6383, max mem: 1039.0, experiment: hmd, epoch: 9, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 048ms, time_since_start: 25m 46s 262ms, eta: 01h 02m 37s 920ms
[32m2022-10-10T23:17:02 | mmf.trainers.callbacks.logistics: [39mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.6046, train/hateful_memes/cross_entropy/avg: 0.6333, train/total_loss: 0.6046, train/total_loss/avg: 0.6333, max mem: 1039.0, experiment: hmd, epoch: 10, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 5.00, time: 20s 169ms, time_since_start: 26m 06s 431ms, eta: 59m 40s 127ms
[32m2022-10-10T23:17:17 | mmf.trainers.callbacks.logistics: [39mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.6046, train/hateful_memes/cross_entropy/avg: 0.6334, train/total_loss: 0.6046, train/total_loss/avg: 0.6334, max mem: 1039.0, experiment: hmd, epoch: 10, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 6.67, time: 15s 601ms, time_since_start: 26m 22s 032ms, eta: 45m 53s 210ms
[32m2022-10-10T23:17:33 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:17:33 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:17:34 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:17:37 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:17:37 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.6199, train/hateful_memes/cross_entropy/avg: 0.6336, train/total_loss: 0.6199, train/total_loss/avg: 0.6336, max mem: 1039.0, experiment: hmd, epoch: 10, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 5.26, time: 19s 457ms, time_since_start: 26m 41s 489ms, eta: 56m 53s 562ms
[32m2022-10-10T23:17:37 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:17:37 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:17:37 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:17:50 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:17:50 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:17:50 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:17:53 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:17:56 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:17:56 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, val/hateful_memes/cross_entropy: 0.7142, val/total_loss: 0.7142, val/hateful_memes/accuracy: 0.5240, val/hateful_memes/binary_f1: 0.1439, val/hateful_memes/roc_auc: 0.5635, num_updates: 5000, epoch: 10, iterations: 5000, max_updates: 22000, val_time: 19s 173ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.565938
[32m2022-10-10T23:18:16 | mmf.trainers.callbacks.logistics: [39mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.6199, train/hateful_memes/cross_entropy/avg: 0.6308, train/total_loss: 0.6199, train/total_loss/avg: 0.6308, max mem: 1039.0, experiment: hmd, epoch: 10, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 5.26, time: 19s 779ms, time_since_start: 27m 20s 449ms, eta: 57m 29s 680ms
[32m2022-10-10T23:18:37 | mmf.trainers.callbacks.logistics: [39mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.6249, train/hateful_memes/cross_entropy/avg: 0.6317, train/total_loss: 0.6249, train/total_loss/avg: 0.6317, max mem: 1039.0, experiment: hmd, epoch: 10, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 5.00, time: 20s 910ms, time_since_start: 27m 41s 360ms, eta: 01h 25s 366ms
[32m2022-10-10T23:18:59 | mmf.trainers.callbacks.logistics: [39mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.6249, train/hateful_memes/cross_entropy/avg: 0.6334, train/total_loss: 0.6249, train/total_loss/avg: 0.6334, max mem: 1039.0, experiment: hmd, epoch: 10, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 4.55, time: 22s 858ms, time_since_start: 28m 04s 218ms, eta: 01h 05m 39s 533ms
[32m2022-10-10T23:19:18 | mmf.trainers.callbacks.logistics: [39mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.6199, train/hateful_memes/cross_entropy/avg: 0.6325, train/total_loss: 0.6199, train/total_loss/avg: 0.6325, max mem: 1039.0, experiment: hmd, epoch: 11, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 5.56, time: 18s 915ms, time_since_start: 28m 23s 134ms, eta: 54m 442ms
[32m2022-10-10T23:19:37 | mmf.trainers.callbacks.logistics: [39mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.6249, train/hateful_memes/cross_entropy/avg: 0.6329, train/total_loss: 0.6249, train/total_loss/avg: 0.6329, max mem: 1039.0, experiment: hmd, epoch: 11, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 5.56, time: 18s 693ms, time_since_start: 28m 41s 827ms, eta: 53m 03s 064ms
[32m2022-10-10T23:19:56 | mmf.trainers.callbacks.logistics: [39mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.6249, train/hateful_memes/cross_entropy/avg: 0.6310, train/total_loss: 0.6249, train/total_loss/avg: 0.6310, max mem: 1039.0, experiment: hmd, epoch: 11, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 5.26, time: 19s 375ms, time_since_start: 29m 01s 203ms, eta: 54m 39s 295ms
[32m2022-10-10T23:20:17 | mmf.trainers.callbacks.logistics: [39mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.6383, train/hateful_memes/cross_entropy/avg: 0.6318, train/total_loss: 0.6383, train/total_loss/avg: 0.6318, max mem: 1039.0, experiment: hmd, epoch: 11, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 5.00, time: 20s 540ms, time_since_start: 29m 21s 744ms, eta: 57m 35s 308ms
[32m2022-10-10T23:20:40 | mmf.trainers.callbacks.logistics: [39mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.6383, train/hateful_memes/cross_entropy/avg: 0.6311, train/total_loss: 0.6383, train/total_loss/avg: 0.6311, max mem: 1039.0, experiment: hmd, epoch: 11, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 4.35, time: 23s 381ms, time_since_start: 29m 45s 125ms, eta: 01h 05m 09s 050ms
[32m2022-10-10T23:21:00 | mmf.trainers.callbacks.logistics: [39mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.6249, train/hateful_memes/cross_entropy/avg: 0.6302, train/total_loss: 0.6249, train/total_loss/avg: 0.6302, max mem: 1039.0, experiment: hmd, epoch: 12, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 5.26, time: 19s 839ms, time_since_start: 30m 04s 964ms, eta: 54m 56s 327ms
[32m2022-10-10T23:21:16 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:21:16 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:21:16 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:21:19 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:21:19 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.6249, train/hateful_memes/cross_entropy/avg: 0.6291, train/total_loss: 0.6249, train/total_loss/avg: 0.6291, max mem: 1039.0, experiment: hmd, epoch: 12, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 5.56, time: 18s 891ms, time_since_start: 30m 23s 856ms, eta: 51m 59s 335ms
[32m2022-10-10T23:21:19 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:21:19 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:21:19 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:21:24 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:21:24 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:21:24 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:21:28 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-10T23:21:31 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:21:33 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:21:33 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, val/hateful_memes/cross_entropy: 0.7728, val/total_loss: 0.7728, val/hateful_memes/accuracy: 0.5100, val/hateful_memes/binary_f1: 0.0467, val/hateful_memes/roc_auc: 0.5668, num_updates: 6000, epoch: 12, iterations: 6000, max_updates: 22000, val_time: 14s 349ms, best_update: 6000, best_iteration: 6000, best_val/hateful_memes/roc_auc: 0.566754
[32m2022-10-10T23:21:55 | mmf.trainers.callbacks.logistics: [39mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.6383, train/hateful_memes/cross_entropy/avg: 0.6297, train/total_loss: 0.6383, train/total_loss/avg: 0.6297, max mem: 1039.0, experiment: hmd, epoch: 12, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 854ms, time_since_start: 31m 066ms, eta: 59m 46s 073ms
[32m2022-10-10T23:22:19 | mmf.trainers.callbacks.logistics: [39mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.6383, train/hateful_memes/cross_entropy/avg: 0.6306, train/total_loss: 0.6383, train/total_loss/avg: 0.6306, max mem: 1039.0, experiment: hmd, epoch: 12, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 4.35, time: 23s 489ms, time_since_start: 31m 23s 556ms, eta: 01h 03m 50s 175ms
[32m2022-10-10T23:22:46 | mmf.trainers.callbacks.logistics: [39mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.6249, train/hateful_memes/cross_entropy/avg: 0.6297, train/total_loss: 0.6249, train/total_loss/avg: 0.6297, max mem: 1039.0, experiment: hmd, epoch: 12, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 3.85, time: 26s 704ms, time_since_start: 31m 50s 260ms, eta: 01h 12m 06s 729ms
[32m2022-10-10T23:23:13 | mmf.trainers.callbacks.logistics: [39mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.6224, train/hateful_memes/cross_entropy/avg: 0.6296, train/total_loss: 0.6224, train/total_loss/avg: 0.6296, max mem: 1039.0, experiment: hmd, epoch: 13, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 3.70, time: 27s 084ms, time_since_start: 32m 17s 344ms, eta: 01h 12m 40s 313ms
[32m2022-10-10T23:23:30 | mmf.trainers.callbacks.logistics: [39mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.6224, train/hateful_memes/cross_entropy/avg: 0.6307, train/total_loss: 0.6224, train/total_loss/avg: 0.6307, max mem: 1039.0, experiment: hmd, epoch: 13, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 5.88, time: 17s 771ms, time_since_start: 32m 35s 115ms, eta: 47m 22s 663ms
[32m2022-10-10T23:23:48 | mmf.trainers.callbacks.logistics: [39mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.6251, train/hateful_memes/cross_entropy/avg: 0.6306, train/total_loss: 0.6251, train/total_loss/avg: 0.6306, max mem: 1039.0, experiment: hmd, epoch: 13, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 5.56, time: 18s 025ms, time_since_start: 32m 53s 141ms, eta: 47m 44s 719ms
[32m2022-10-10T23:24:07 | mmf.trainers.callbacks.logistics: [39mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.6251, train/hateful_memes/cross_entropy/avg: 0.6317, train/total_loss: 0.6251, train/total_loss/avg: 0.6317, max mem: 1039.0, experiment: hmd, epoch: 13, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 5.56, time: 18s 679ms, time_since_start: 33m 11s 820ms, eta: 49m 09s 455ms
[32m2022-10-10T23:24:26 | mmf.trainers.callbacks.logistics: [39mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.6383, train/hateful_memes/cross_entropy/avg: 0.6322, train/total_loss: 0.6383, train/total_loss/avg: 0.6322, max mem: 1039.0, experiment: hmd, epoch: 13, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 5.56, time: 18s 660ms, time_since_start: 33m 30s 481ms, eta: 48m 47s 184ms
[32m2022-10-10T23:24:45 | mmf.trainers.callbacks.logistics: [39mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.6251, train/hateful_memes/cross_entropy/avg: 0.6313, train/total_loss: 0.6251, train/total_loss/avg: 0.6313, max mem: 1039.0, experiment: hmd, epoch: 13, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 5.26, time: 19s 096ms, time_since_start: 33m 49s 578ms, eta: 49m 35s 834ms
[32m2022-10-10T23:25:01 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:25:01 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:25:02 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:25:05 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:25:05 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.6224, train/hateful_memes/cross_entropy/avg: 0.6286, train/total_loss: 0.6224, train/total_loss/avg: 0.6286, max mem: 1039.0, experiment: hmd, epoch: 14, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 5.26, time: 19s 683ms, time_since_start: 34m 09s 261ms, eta: 50m 46s 951ms
[32m2022-10-10T23:25:05 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:25:05 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:25:05 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:25:18 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:25:18 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:25:18 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:25:21 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-10T23:25:24 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:25:27 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:25:27 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, val/hateful_memes/cross_entropy: 0.7087, val/total_loss: 0.7087, val/hateful_memes/accuracy: 0.5280, val/hateful_memes/binary_f1: 0.2805, val/hateful_memes/roc_auc: 0.5676, num_updates: 7000, epoch: 14, iterations: 7000, max_updates: 22000, val_time: 21s 989ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.567650
[32m2022-10-10T23:25:47 | mmf.trainers.callbacks.logistics: [39mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.6251, train/hateful_memes/cross_entropy/avg: 0.6295, train/total_loss: 0.6251, train/total_loss/avg: 0.6295, max mem: 1039.0, experiment: hmd, epoch: 14, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 5.00, time: 20s 420ms, time_since_start: 34m 51s 674ms, eta: 52m 20s 088ms
[32m2022-10-10T23:26:08 | mmf.trainers.callbacks.logistics: [39mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.6224, train/hateful_memes/cross_entropy/avg: 0.6293, train/total_loss: 0.6224, train/total_loss/avg: 0.6293, max mem: 1039.0, experiment: hmd, epoch: 14, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 041ms, time_since_start: 35m 12s 715ms, eta: 53m 33s 722ms
[32m2022-10-10T23:26:31 | mmf.trainers.callbacks.logistics: [39mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.6182, train/hateful_memes/cross_entropy/avg: 0.6292, train/total_loss: 0.6182, train/total_loss/avg: 0.6292, max mem: 1039.0, experiment: hmd, epoch: 14, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 4.55, time: 22s 547ms, time_since_start: 35m 35s 262ms, eta: 57m 488ms
[32m2022-10-10T23:26:57 | mmf.trainers.callbacks.logistics: [39mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.6182, train/hateful_memes/cross_entropy/avg: 0.6285, train/total_loss: 0.6182, train/total_loss/avg: 0.6285, max mem: 1039.0, experiment: hmd, epoch: 14, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 3.85, time: 26s 919ms, time_since_start: 36m 02s 182ms, eta: 01h 07m 36s 091ms
[32m2022-10-10T23:27:22 | mmf.trainers.callbacks.logistics: [39mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.6182, train/hateful_memes/cross_entropy/avg: 0.6289, train/total_loss: 0.6182, train/total_loss/avg: 0.6289, max mem: 1039.0, experiment: hmd, epoch: 15, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 4.17, time: 24s 415ms, time_since_start: 36m 26s 598ms, eta: 01h 53s 552ms
[32m2022-10-10T23:27:40 | mmf.trainers.callbacks.logistics: [39mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.6182, train/hateful_memes/cross_entropy/avg: 0.6282, train/total_loss: 0.6182, train/total_loss/avg: 0.6282, max mem: 1039.0, experiment: hmd, epoch: 15, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 5.56, time: 18s 251ms, time_since_start: 36m 44s 849ms, eta: 45m 12s 347ms
[32m2022-10-10T23:27:58 | mmf.trainers.callbacks.logistics: [39mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.6182, train/hateful_memes/cross_entropy/avg: 0.6304, train/total_loss: 0.6182, train/total_loss/avg: 0.6304, max mem: 1039.0, experiment: hmd, epoch: 15, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 5.88, time: 17s 961ms, time_since_start: 37m 02s 811ms, eta: 44m 10s 661ms
[32m2022-10-10T23:28:16 | mmf.trainers.callbacks.logistics: [39mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.6182, train/hateful_memes/cross_entropy/avg: 0.6296, train/total_loss: 0.6182, train/total_loss/avg: 0.6296, max mem: 1039.0, experiment: hmd, epoch: 15, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 5.56, time: 18s 043ms, time_since_start: 37m 20s 854ms, eta: 44m 04s 177ms
[32m2022-10-10T23:28:34 | mmf.trainers.callbacks.logistics: [39mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.6182, train/hateful_memes/cross_entropy/avg: 0.6291, train/total_loss: 0.6182, train/total_loss/avg: 0.6291, max mem: 1039.0, experiment: hmd, epoch: 15, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 5.56, time: 18s 265ms, time_since_start: 37m 39s 120ms, eta: 44m 17s 812ms
[32m2022-10-10T23:28:52 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:28:52 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:28:53 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:28:56 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:28:56 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.6224, train/hateful_memes/cross_entropy/avg: 0.6298, train/total_loss: 0.6224, train/total_loss/avg: 0.6298, max mem: 1039.0, experiment: hmd, epoch: 16, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 4.76, time: 21s 497ms, time_since_start: 38m 617ms, eta: 51m 45s 935ms
[32m2022-10-10T23:28:56 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:28:56 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:28:56 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:29:05 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:29:05 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:29:05 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:29:08 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-10T23:29:11 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:29:14 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:29:14 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, val/hateful_memes/cross_entropy: 0.7799, val/total_loss: 0.7799, val/hateful_memes/accuracy: 0.5140, val/hateful_memes/binary_f1: 0.0471, val/hateful_memes/roc_auc: 0.5695, num_updates: 8000, epoch: 16, iterations: 8000, max_updates: 22000, val_time: 18s 099ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.569506
[32m2022-10-10T23:29:34 | mmf.trainers.callbacks.logistics: [39mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.6182, train/hateful_memes/cross_entropy/avg: 0.6292, train/total_loss: 0.6182, train/total_loss/avg: 0.6292, max mem: 1039.0, experiment: hmd, epoch: 16, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 5.26, time: 19s 784ms, time_since_start: 38m 38s 508ms, eta: 47m 18s 113ms
[32m2022-10-10T23:29:55 | mmf.trainers.callbacks.logistics: [39mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.6182, train/hateful_memes/cross_entropy/avg: 0.6296, train/total_loss: 0.6182, train/total_loss/avg: 0.6296, max mem: 1039.0, experiment: hmd, epoch: 16, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 5.00, time: 20s 751ms, time_since_start: 38m 59s 259ms, eta: 49m 15s 341ms
[32m2022-10-10T23:30:17 | mmf.trainers.callbacks.logistics: [39mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.6182, train/hateful_memes/cross_entropy/avg: 0.6277, train/total_loss: 0.6182, train/total_loss/avg: 0.6277, max mem: 1039.0, experiment: hmd, epoch: 16, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 4.55, time: 22s 692ms, time_since_start: 39m 21s 952ms, eta: 53m 28s 371ms
[32m2022-10-10T23:30:43 | mmf.trainers.callbacks.logistics: [39mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.6159, train/hateful_memes/cross_entropy/avg: 0.6272, train/total_loss: 0.6159, train/total_loss/avg: 0.6272, max mem: 1039.0, experiment: hmd, epoch: 16, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 3.85, time: 26s 031ms, time_since_start: 39m 47s 983ms, eta: 01h 53s 532ms
[32m2022-10-10T23:31:11 | mmf.trainers.callbacks.logistics: [39mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.5889, train/hateful_memes/cross_entropy/avg: 0.6257, train/total_loss: 0.5889, train/total_loss/avg: 0.6257, max mem: 1039.0, experiment: hmd, epoch: 16, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 3.70, time: 27s 464ms, time_since_start: 40m 15s 447ms, eta: 01h 03m 46s 331ms
[32m2022-10-10T23:31:28 | mmf.trainers.callbacks.logistics: [39mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.5889, train/hateful_memes/cross_entropy/avg: 0.6255, train/total_loss: 0.5889, train/total_loss/avg: 0.6255, max mem: 1039.0, experiment: hmd, epoch: 17, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 6.25, time: 16s 987ms, time_since_start: 40m 32s 435ms, eta: 39m 09s 145ms
[32m2022-10-10T23:31:44 | mmf.trainers.callbacks.logistics: [39mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.5889, train/hateful_memes/cross_entropy/avg: 0.6262, train/total_loss: 0.5889, train/total_loss/avg: 0.6262, max mem: 1039.0, experiment: hmd, epoch: 17, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 6.25, time: 16s 013ms, time_since_start: 40m 48s 448ms, eta: 36m 37s 940ms
[32m2022-10-10T23:32:00 | mmf.trainers.callbacks.logistics: [39mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.5889, train/hateful_memes/cross_entropy/avg: 0.6260, train/total_loss: 0.5889, train/total_loss/avg: 0.6260, max mem: 1039.0, experiment: hmd, epoch: 17, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 6.67, time: 15s 959ms, time_since_start: 41m 04s 408ms, eta: 36m 14s 113ms
[32m2022-10-10T23:32:16 | mmf.trainers.callbacks.logistics: [39mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.6077, train/hateful_memes/cross_entropy/avg: 0.6259, train/total_loss: 0.6077, train/total_loss/avg: 0.6259, max mem: 1039.0, experiment: hmd, epoch: 17, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 6.67, time: 15s 993ms, time_since_start: 41m 20s 402ms, eta: 36m 02s 211ms
[32m2022-10-10T23:32:32 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:32:32 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:32:32 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:32:35 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:32:35 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.6159, train/hateful_memes/cross_entropy/avg: 0.6260, train/total_loss: 0.6159, train/total_loss/avg: 0.6260, max mem: 1039.0, experiment: hmd, epoch: 17, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 5.26, time: 19s 185ms, time_since_start: 41m 39s 588ms, eta: 42m 53s 984ms
[32m2022-10-10T23:32:35 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:32:35 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:32:35 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:32:48 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:32:48 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:32:48 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:32:51 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-10T23:32:53 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:32:56 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:32:56 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, val/hateful_memes/cross_entropy: 0.7669, val/total_loss: 0.7669, val/hateful_memes/accuracy: 0.5160, val/hateful_memes/binary_f1: 0.0620, val/hateful_memes/roc_auc: 0.5718, num_updates: 9000, epoch: 17, iterations: 9000, max_updates: 22000, val_time: 21s 068ms, best_update: 9000, best_iteration: 9000, best_val/hateful_memes/roc_auc: 0.571810
[32m2022-10-10T23:33:20 | mmf.trainers.callbacks.logistics: [39mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.6077, train/hateful_memes/cross_entropy/avg: 0.6247, train/total_loss: 0.6077, train/total_loss/avg: 0.6247, max mem: 1039.0, experiment: hmd, epoch: 18, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 4.35, time: 23s 563ms, time_since_start: 42m 24s 255ms, eta: 52m 17s 015ms
[32m2022-10-10T23:33:40 | mmf.trainers.callbacks.logistics: [39mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.5889, train/hateful_memes/cross_entropy/avg: 0.6241, train/total_loss: 0.5889, train/total_loss/avg: 0.6241, max mem: 1039.0, experiment: hmd, epoch: 18, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 5.00, time: 20s 149ms, time_since_start: 42m 44s 404ms, eta: 44m 21s 656ms
[32m2022-10-10T23:34:01 | mmf.trainers.callbacks.logistics: [39mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.5889, train/hateful_memes/cross_entropy/avg: 0.6254, train/total_loss: 0.5889, train/total_loss/avg: 0.6254, max mem: 1039.0, experiment: hmd, epoch: 18, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 4.76, time: 21s 283ms, time_since_start: 43m 05s 688ms, eta: 46m 29s 494ms
[32m2022-10-10T23:34:26 | mmf.trainers.callbacks.logistics: [39mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.6033, train/hateful_memes/cross_entropy/avg: 0.6252, train/total_loss: 0.6033, train/total_loss/avg: 0.6252, max mem: 1039.0, experiment: hmd, epoch: 18, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 4.17, time: 24s 693ms, time_since_start: 43m 30s 381ms, eta: 53m 30s 890ms
[32m2022-10-10T23:34:56 | mmf.trainers.callbacks.logistics: [39mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.6033, train/hateful_memes/cross_entropy/avg: 0.6257, train/total_loss: 0.6033, train/total_loss/avg: 0.6257, max mem: 1039.0, experiment: hmd, epoch: 18, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 3.33, time: 30s 689ms, time_since_start: 44m 01s 070ms, eta: 01h 05m 58s 960ms
[32m2022-10-10T23:35:27 | mmf.trainers.callbacks.logistics: [39mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.6033, train/hateful_memes/cross_entropy/avg: 0.6247, train/total_loss: 0.6033, train/total_loss/avg: 0.6247, max mem: 1039.0, experiment: hmd, epoch: 19, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 3.33, time: 30s 945ms, time_since_start: 44m 32s 016ms, eta: 01h 06m 036ms
[32m2022-10-10T23:35:43 | mmf.trainers.callbacks.logistics: [39mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.6033, train/hateful_memes/cross_entropy/avg: 0.6250, train/total_loss: 0.6033, train/total_loss/avg: 0.6250, max mem: 1039.0, experiment: hmd, epoch: 19, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 6.67, time: 15s 522ms, time_since_start: 44m 47s 538ms, eta: 32m 50s 325ms
[32m2022-10-10T23:35:58 | mmf.trainers.callbacks.logistics: [39mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.6033, train/hateful_memes/cross_entropy/avg: 0.6237, train/total_loss: 0.6033, train/total_loss/avg: 0.6237, max mem: 1039.0, experiment: hmd, epoch: 19, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 6.67, time: 15s 605ms, time_since_start: 45m 03s 144ms, eta: 32m 44s 825ms
[32m2022-10-10T23:36:14 | mmf.trainers.callbacks.logistics: [39mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.6077, train/hateful_memes/cross_entropy/avg: 0.6243, train/total_loss: 0.6077, train/total_loss/avg: 0.6243, max mem: 1039.0, experiment: hmd, epoch: 19, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 6.67, time: 15s 642ms, time_since_start: 45m 18s 786ms, eta: 32m 33s 278ms
[32m2022-10-10T23:36:30 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:36:30 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:36:30 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:36:33 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:36:33 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.6077, train/hateful_memes/cross_entropy/avg: 0.6245, train/total_loss: 0.6077, train/total_loss/avg: 0.6245, max mem: 1039.0, experiment: hmd, epoch: 19, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 5.56, time: 18s 986ms, time_since_start: 45m 37s 773ms, eta: 39m 11s 336ms
[32m2022-10-10T23:36:33 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:36:33 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:36:33 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:36:42 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:36:42 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:36:43 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:36:46 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:36:50 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:36:50 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, val/hateful_memes/cross_entropy: 0.7510, val/total_loss: 0.7510, val/hateful_memes/accuracy: 0.5200, val/hateful_memes/binary_f1: 0.1045, val/hateful_memes/roc_auc: 0.5695, num_updates: 10000, epoch: 19, iterations: 10000, max_updates: 22000, val_time: 16s 561ms, best_update: 9000, best_iteration: 9000, best_val/hateful_memes/roc_auc: 0.571810
[32m2022-10-10T23:37:19 | mmf.trainers.callbacks.logistics: [39mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.6168, train/hateful_memes/cross_entropy/avg: 0.6251, train/total_loss: 0.6168, train/total_loss/avg: 0.6251, max mem: 1039.0, experiment: hmd, epoch: 19, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 3.45, time: 29s 639ms, time_since_start: 46m 23s 980ms, eta: 01h 39s 945ms
[32m2022-10-10T23:37:38 | mmf.trainers.callbacks.logistics: [39mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.6077, train/hateful_memes/cross_entropy/avg: 0.6248, train/total_loss: 0.6077, train/total_loss/avg: 0.6248, max mem: 1039.0, experiment: hmd, epoch: 20, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 5.26, time: 19s 155ms, time_since_start: 46m 43s 135ms, eta: 38m 52s 671ms
[32m2022-10-10T23:37:57 | mmf.trainers.callbacks.logistics: [39mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.6168, train/hateful_memes/cross_entropy/avg: 0.6257, train/total_loss: 0.6168, train/total_loss/avg: 0.6257, max mem: 1039.0, experiment: hmd, epoch: 20, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 5.56, time: 18s 667ms, time_since_start: 47m 01s 802ms, eta: 37m 33s 945ms
[32m2022-10-10T23:38:17 | mmf.trainers.callbacks.logistics: [39mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.6169, train/hateful_memes/cross_entropy/avg: 0.6257, train/total_loss: 0.6169, train/total_loss/avg: 0.6257, max mem: 1039.0, experiment: hmd, epoch: 20, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 5.00, time: 20s 248ms, time_since_start: 47m 22s 051ms, eta: 40m 23s 979ms
[32m2022-10-10T23:38:39 | mmf.trainers.callbacks.logistics: [39mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.6266, train/hateful_memes/cross_entropy/avg: 0.6274, train/total_loss: 0.6266, train/total_loss/avg: 0.6274, max mem: 1039.0, experiment: hmd, epoch: 20, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 4.76, time: 21s 586ms, time_since_start: 47m 43s 637ms, eta: 42m 41s 904ms
[32m2022-10-10T23:39:04 | mmf.trainers.callbacks.logistics: [39mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.6266, train/hateful_memes/cross_entropy/avg: 0.6272, train/total_loss: 0.6266, train/total_loss/avg: 0.6272, max mem: 1039.0, experiment: hmd, epoch: 20, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 4.00, time: 25s 169ms, time_since_start: 48m 08s 806ms, eta: 49m 21s 113ms
[32m2022-10-10T23:39:23 | mmf.trainers.callbacks.logistics: [39mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.6266, train/hateful_memes/cross_entropy/avg: 0.6275, train/total_loss: 0.6266, train/total_loss/avg: 0.6275, max mem: 1039.0, experiment: hmd, epoch: 21, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 5.26, time: 19s 182ms, time_since_start: 48m 27s 989ms, eta: 37m 17s 026ms
[32m2022-10-10T23:39:39 | mmf.trainers.callbacks.logistics: [39mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.6266, train/hateful_memes/cross_entropy/avg: 0.6258, train/total_loss: 0.6266, train/total_loss/avg: 0.6258, max mem: 1039.0, experiment: hmd, epoch: 21, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 6.67, time: 15s 636ms, time_since_start: 48m 43s 626ms, eta: 30m 07s 299ms
[32m2022-10-10T23:39:55 | mmf.trainers.callbacks.logistics: [39mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.6336, train/hateful_memes/cross_entropy/avg: 0.6285, train/total_loss: 0.6336, train/total_loss/avg: 0.6285, max mem: 1039.0, experiment: hmd, epoch: 21, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 6.67, time: 15s 727ms, time_since_start: 48m 59s 353ms, eta: 30m 01s 641ms
[32m2022-10-10T23:40:10 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:40:10 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:40:11 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:40:14 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:40:14 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.6266, train/hateful_memes/cross_entropy/avg: 0.6285, train/total_loss: 0.6266, train/total_loss/avg: 0.6285, max mem: 1039.0, experiment: hmd, epoch: 21, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 5.26, time: 19s 783ms, time_since_start: 49m 19s 137ms, eta: 37m 25s 820ms
[32m2022-10-10T23:40:14 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:40:14 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:40:14 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:40:27 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:40:27 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:40:27 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:40:31 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-10T23:40:35 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:40:38 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:40:38 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, val/hateful_memes/cross_entropy: 0.7198, val/total_loss: 0.7198, val/hateful_memes/accuracy: 0.5280, val/hateful_memes/binary_f1: 0.2133, val/hateful_memes/roc_auc: 0.5742, num_updates: 11000, epoch: 21, iterations: 11000, max_updates: 22000, val_time: 23s 887ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.574163
[32m2022-10-10T23:41:05 | mmf.trainers.callbacks.logistics: [39mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.6476, train/hateful_memes/cross_entropy/avg: 0.6287, train/total_loss: 0.6476, train/total_loss/avg: 0.6287, max mem: 1039.0, experiment: hmd, epoch: 21, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 3.85, time: 26s 799ms, time_since_start: 50m 09s 827ms, eta: 50m 14s 612ms
[32m2022-10-10T23:41:30 | mmf.trainers.callbacks.logistics: [39mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.6476, train/hateful_memes/cross_entropy/avg: 0.6283, train/total_loss: 0.6476, train/total_loss/avg: 0.6283, max mem: 1039.0, experiment: hmd, epoch: 22, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 4.00, time: 25s 023ms, time_since_start: 50m 34s 850ms, eta: 46m 28s 994ms
[32m2022-10-10T23:41:50 | mmf.trainers.callbacks.logistics: [39mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.6266, train/hateful_memes/cross_entropy/avg: 0.6270, train/total_loss: 0.6266, train/total_loss/avg: 0.6270, max mem: 1039.0, experiment: hmd, epoch: 22, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 5.26, time: 19s 907ms, time_since_start: 50m 54s 757ms, eta: 36m 38s 223ms
[32m2022-10-10T23:42:11 | mmf.trainers.callbacks.logistics: [39mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.6266, train/hateful_memes/cross_entropy/avg: 0.6267, train/total_loss: 0.6266, train/total_loss/avg: 0.6267, max mem: 1039.0, experiment: hmd, epoch: 22, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 5.00, time: 20s 471ms, time_since_start: 51m 15s 229ms, eta: 37m 19s 383ms
[32m2022-10-10T23:42:32 | mmf.trainers.callbacks.logistics: [39mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.6232, train/hateful_memes/cross_entropy/avg: 0.6265, train/total_loss: 0.6232, train/total_loss/avg: 0.6265, max mem: 1039.0, experiment: hmd, epoch: 22, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 4.76, time: 21s 701ms, time_since_start: 51m 36s 930ms, eta: 39m 11s 525ms
[32m2022-10-10T23:42:57 | mmf.trainers.callbacks.logistics: [39mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.6232, train/hateful_memes/cross_entropy/avg: 0.6252, train/total_loss: 0.6232, train/total_loss/avg: 0.6252, max mem: 1039.0, experiment: hmd, epoch: 22, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 4.17, time: 24s 727ms, time_since_start: 52m 01s 657ms, eta: 44m 13s 982ms
[32m2022-10-10T23:43:23 | mmf.trainers.callbacks.logistics: [39mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.6232, train/hateful_memes/cross_entropy/avg: 0.6256, train/total_loss: 0.6232, train/total_loss/avg: 0.6256, max mem: 1039.0, experiment: hmd, epoch: 22, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 3.85, time: 26s 242ms, time_since_start: 52m 27s 900ms, eta: 46m 29s 444ms
[32m2022-10-10T23:43:39 | mmf.trainers.callbacks.logistics: [39mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.6232, train/hateful_memes/cross_entropy/avg: 0.6252, train/total_loss: 0.6232, train/total_loss/avg: 0.6252, max mem: 1039.0, experiment: hmd, epoch: 23, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 6.67, time: 15s 776ms, time_since_start: 52m 43s 676ms, eta: 27m 40s 697ms
[32m2022-10-10T23:43:55 | mmf.trainers.callbacks.logistics: [39mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.6232, train/hateful_memes/cross_entropy/avg: 0.6259, train/total_loss: 0.6232, train/total_loss/avg: 0.6259, max mem: 1039.0, experiment: hmd, epoch: 23, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 6.67, time: 15s 663ms, time_since_start: 52m 59s 340ms, eta: 27m 12s 651ms
[32m2022-10-10T23:44:10 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:44:10 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:44:11 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:44:14 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:44:14 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.6064, train/hateful_memes/cross_entropy/avg: 0.6252, train/total_loss: 0.6064, train/total_loss/avg: 0.6252, max mem: 1039.0, experiment: hmd, epoch: 23, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 5.26, time: 19s 261ms, time_since_start: 53m 18s 601ms, eta: 33m 07s 799ms
[32m2022-10-10T23:44:14 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:44:14 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:44:14 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:44:21 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:44:21 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:44:21 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:44:24 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:44:27 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:44:27 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, val/hateful_memes/cross_entropy: 0.7049, val/total_loss: 0.7049, val/hateful_memes/accuracy: 0.5340, val/hateful_memes/binary_f1: 0.3086, val/hateful_memes/roc_auc: 0.5735, num_updates: 12000, epoch: 23, iterations: 12000, max_updates: 22000, val_time: 12s 788ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.574163
[32m2022-10-10T23:44:49 | mmf.trainers.callbacks.logistics: [39mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.6031, train/hateful_memes/cross_entropy/avg: 0.6241, train/total_loss: 0.6031, train/total_loss/avg: 0.6241, max mem: 1039.0, experiment: hmd, epoch: 23, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 021ms, time_since_start: 53m 53s 417ms, eta: 37m 29s 934ms
[32m2022-10-10T23:45:13 | mmf.trainers.callbacks.logistics: [39mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.6064, train/hateful_memes/cross_entropy/avg: 0.6241, train/total_loss: 0.6064, train/total_loss/avg: 0.6241, max mem: 1039.0, experiment: hmd, epoch: 23, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 4.17, time: 24s 558ms, time_since_start: 54m 17s 976ms, eta: 41m 23s 789ms
[32m2022-10-10T23:45:34 | mmf.trainers.callbacks.logistics: [39mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.6064, train/hateful_memes/cross_entropy/avg: 0.6244, train/total_loss: 0.6064, train/total_loss/avg: 0.6244, max mem: 1039.0, experiment: hmd, epoch: 24, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 5.00, time: 20s 857ms, time_since_start: 54m 38s 834ms, eta: 34m 47s 925ms
[32m2022-10-10T23:45:52 | mmf.trainers.callbacks.logistics: [39mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.6031, train/hateful_memes/cross_entropy/avg: 0.6242, train/total_loss: 0.6031, train/total_loss/avg: 0.6242, max mem: 1039.0, experiment: hmd, epoch: 24, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 183ms, time_since_start: 54m 57s 018ms, eta: 30m 01s 491ms
[32m2022-10-10T23:46:12 | mmf.trainers.callbacks.logistics: [39mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.6031, train/hateful_memes/cross_entropy/avg: 0.6255, train/total_loss: 0.6031, train/total_loss/avg: 0.6255, max mem: 1039.0, experiment: hmd, epoch: 24, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 465ms, time_since_start: 55m 16s 483ms, eta: 31m 48s 359ms
[32m2022-10-10T23:46:32 | mmf.trainers.callbacks.logistics: [39mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.6064, train/hateful_memes/cross_entropy/avg: 0.6258, train/total_loss: 0.6064, train/total_loss/avg: 0.6258, max mem: 1039.0, experiment: hmd, epoch: 24, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 5.00, time: 20s 688ms, time_since_start: 55m 37s 171ms, eta: 33m 26s 915ms
[32m2022-10-10T23:46:54 | mmf.trainers.callbacks.logistics: [39mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.5948, train/hateful_memes/cross_entropy/avg: 0.6252, train/total_loss: 0.5948, train/total_loss/avg: 0.6252, max mem: 1039.0, experiment: hmd, epoch: 24, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 4.76, time: 21s 638ms, time_since_start: 55m 58s 809ms, eta: 34m 36s 774ms
[32m2022-10-10T23:47:14 | mmf.trainers.callbacks.logistics: [39mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.5948, train/hateful_memes/cross_entropy/avg: 0.6244, train/total_loss: 0.5948, train/total_loss/avg: 0.6244, max mem: 1039.0, experiment: hmd, epoch: 25, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 573ms, time_since_start: 56m 18s 383ms, eta: 30m 58s 358ms
[32m2022-10-10T23:47:29 | mmf.trainers.callbacks.logistics: [39mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.5948, train/hateful_memes/cross_entropy/avg: 0.6250, train/total_loss: 0.5948, train/total_loss/avg: 0.6250, max mem: 1039.0, experiment: hmd, epoch: 25, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 6.67, time: 15s 575ms, time_since_start: 56m 33s 958ms, eta: 24m 22s 708ms
[32m2022-10-10T23:47:45 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:47:45 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:47:45 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:47:48 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:47:48 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.5912, train/hateful_memes/cross_entropy/avg: 0.6246, train/total_loss: 0.5912, train/total_loss/avg: 0.6246, max mem: 1039.0, experiment: hmd, epoch: 25, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 920ms, time_since_start: 56m 52s 878ms, eta: 29m 17s 319ms
[32m2022-10-10T23:47:48 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:47:48 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:47:48 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:47:56 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:47:56 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:47:56 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:47:59 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:48:01 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:48:01 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, val/hateful_memes/cross_entropy: 0.7702, val/total_loss: 0.7702, val/hateful_memes/accuracy: 0.5180, val/hateful_memes/binary_f1: 0.0837, val/hateful_memes/roc_auc: 0.5732, num_updates: 13000, epoch: 25, iterations: 13000, max_updates: 22000, val_time: 13s 161ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.574163
[32m2022-10-10T23:48:21 | mmf.trainers.callbacks.logistics: [39mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.5912, train/hateful_memes/cross_entropy/avg: 0.6249, train/total_loss: 0.5912, train/total_loss/avg: 0.6249, max mem: 1039.0, experiment: hmd, epoch: 25, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 5.00, time: 20s 021ms, time_since_start: 57m 26s 064ms, eta: 30m 38s 921ms
[32m2022-10-10T23:48:43 | mmf.trainers.callbacks.logistics: [39mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.5948, train/hateful_memes/cross_entropy/avg: 0.6256, train/total_loss: 0.5948, train/total_loss/avg: 0.6256, max mem: 1039.0, experiment: hmd, epoch: 25, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 4.76, time: 21s 654ms, time_since_start: 57m 47s 718ms, eta: 32m 46s 546ms
[32m2022-10-10T23:49:07 | mmf.trainers.callbacks.logistics: [39mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.5948, train/hateful_memes/cross_entropy/avg: 0.6253, train/total_loss: 0.5948, train/total_loss/avg: 0.6253, max mem: 1039.0, experiment: hmd, epoch: 25, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 4.17, time: 24s 151ms, time_since_start: 58m 11s 870ms, eta: 36m 08s 459ms
[32m2022-10-10T23:49:25 | mmf.trainers.callbacks.logistics: [39mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.6064, train/hateful_memes/cross_entropy/avg: 0.6254, train/total_loss: 0.6064, train/total_loss/avg: 0.6254, max mem: 1039.0, experiment: hmd, epoch: 26, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 5.88, time: 17s 783ms, time_since_start: 58m 29s 653ms, eta: 26m 18s 323ms
[32m2022-10-10T23:49:43 | mmf.trainers.callbacks.logistics: [39mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.5948, train/hateful_memes/cross_entropy/avg: 0.6243, train/total_loss: 0.5948, train/total_loss/avg: 0.6243, max mem: 1039.0, experiment: hmd, epoch: 26, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 198ms, time_since_start: 58m 47s 851ms, eta: 26m 36s 338ms
[32m2022-10-10T23:50:02 | mmf.trainers.callbacks.logistics: [39mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.5948, train/hateful_memes/cross_entropy/avg: 0.6237, train/total_loss: 0.5948, train/total_loss/avg: 0.6237, max mem: 1039.0, experiment: hmd, epoch: 26, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 015ms, time_since_start: 59m 06s 867ms, eta: 27m 28s 424ms
[32m2022-10-10T23:50:23 | mmf.trainers.callbacks.logistics: [39mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.5948, train/hateful_memes/cross_entropy/avg: 0.6241, train/total_loss: 0.5948, train/total_loss/avg: 0.6241, max mem: 1039.0, experiment: hmd, epoch: 26, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 4.76, time: 21s 044ms, time_since_start: 59m 27s 911ms, eta: 30m 02s 555ms
[32m2022-10-10T23:50:46 | mmf.trainers.callbacks.logistics: [39mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.6302, train/hateful_memes/cross_entropy/avg: 0.6248, train/total_loss: 0.6302, train/total_loss/avg: 0.6248, max mem: 1039.0, experiment: hmd, epoch: 26, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 576ms, time_since_start: 59m 50s 488ms, eta: 31m 50s 514ms
[32m2022-10-10T23:51:03 | mmf.trainers.callbacks.logistics: [39mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.5948, train/hateful_memes/cross_entropy/avg: 0.6240, train/total_loss: 0.5948, train/total_loss/avg: 0.6240, max mem: 1039.0, experiment: hmd, epoch: 27, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 5.88, time: 17s 692ms, time_since_start: 01h 08s 180ms, eta: 24m 38s 913ms
[32m2022-10-10T23:51:19 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:51:19 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:51:20 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:51:23 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:51:23 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.6302, train/hateful_memes/cross_entropy/avg: 0.6250, train/total_loss: 0.6302, train/total_loss/avg: 0.6250, max mem: 1039.0, experiment: hmd, epoch: 27, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 118ms, time_since_start: 01h 27s 298ms, eta: 26m 18s 398ms
[32m2022-10-10T23:51:23 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:51:23 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:51:23 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:51:32 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:51:32 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:51:32 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:51:34 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-10T23:51:37 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:51:40 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:51:40 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, val/hateful_memes/cross_entropy: 0.7768, val/total_loss: 0.7768, val/hateful_memes/accuracy: 0.5240, val/hateful_memes/binary_f1: 0.1053, val/hateful_memes/roc_auc: 0.5754, num_updates: 14000, epoch: 27, iterations: 14000, max_updates: 22000, val_time: 17s 577ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.575379
[32m2022-10-10T23:52:03 | mmf.trainers.callbacks.logistics: [39mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.6302, train/hateful_memes/cross_entropy/avg: 0.6242, train/total_loss: 0.6302, train/total_loss/avg: 0.6242, max mem: 1039.0, experiment: hmd, epoch: 27, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 434ms, time_since_start: 01h 01m 07s 315ms, eta: 30m 29s 030ms
[32m2022-10-10T23:52:27 | mmf.trainers.callbacks.logistics: [39mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.6302, train/hateful_memes/cross_entropy/avg: 0.6256, train/total_loss: 0.6302, train/total_loss/avg: 0.6256, max mem: 1039.0, experiment: hmd, epoch: 27, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 4.17, time: 24s 324ms, time_since_start: 01h 01m 31s 640ms, eta: 32m 38s 027ms
[32m2022-10-10T23:52:54 | mmf.trainers.callbacks.logistics: [39mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.6302, train/hateful_memes/cross_entropy/avg: 0.6259, train/total_loss: 0.6302, train/total_loss/avg: 0.6259, max mem: 1039.0, experiment: hmd, epoch: 27, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 3.85, time: 26s 636ms, time_since_start: 01h 01m 58s 276ms, eta: 35m 16s 630ms
[32m2022-10-10T23:53:19 | mmf.trainers.callbacks.logistics: [39mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.6423, train/hateful_memes/cross_entropy/avg: 0.6260, train/total_loss: 0.6423, train/total_loss/avg: 0.6260, max mem: 1039.0, experiment: hmd, epoch: 28, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 4.00, time: 25s 527ms, time_since_start: 01h 02m 23s 804ms, eta: 33m 22s 180ms
[32m2022-10-10T23:53:38 | mmf.trainers.callbacks.logistics: [39mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.6423, train/hateful_memes/cross_entropy/avg: 0.6262, train/total_loss: 0.6423, train/total_loss/avg: 0.6262, max mem: 1039.0, experiment: hmd, epoch: 28, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 056ms, time_since_start: 01h 02m 42s 860ms, eta: 24m 34s 974ms
[32m2022-10-10T23:53:57 | mmf.trainers.callbacks.logistics: [39mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.6423, train/hateful_memes/cross_entropy/avg: 0.6281, train/total_loss: 0.6423, train/total_loss/avg: 0.6281, max mem: 1039.0, experiment: hmd, epoch: 28, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 634ms, time_since_start: 01h 03m 01s 495ms, eta: 23m 43s 115ms
[32m2022-10-10T23:54:16 | mmf.trainers.callbacks.logistics: [39mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.6423, train/hateful_memes/cross_entropy/avg: 0.6281, train/total_loss: 0.6423, train/total_loss/avg: 0.6281, max mem: 1039.0, experiment: hmd, epoch: 28, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 536ms, time_since_start: 01h 03m 21s 031ms, eta: 24m 31s 778ms
[32m2022-10-10T23:54:35 | mmf.trainers.callbacks.logistics: [39mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.6423, train/hateful_memes/cross_entropy/avg: 0.6276, train/total_loss: 0.6423, train/total_loss/avg: 0.6276, max mem: 1039.0, experiment: hmd, epoch: 28, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 101ms, time_since_start: 01h 03m 40s 133ms, eta: 23m 39s 334ms
[32m2022-10-10T23:54:54 | mmf.trainers.callbacks.logistics: [39mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.6302, train/hateful_memes/cross_entropy/avg: 0.6270, train/total_loss: 0.6302, train/total_loss/avg: 0.6270, max mem: 1039.0, experiment: hmd, epoch: 29, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 082ms, time_since_start: 01h 03m 59s 215ms, eta: 23m 18s 187ms
[32m2022-10-10T23:55:10 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:55:10 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:55:11 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:55:14 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:55:14 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.6411, train/hateful_memes/cross_entropy/avg: 0.6271, train/total_loss: 0.6411, train/total_loss/avg: 0.6271, max mem: 1039.0, experiment: hmd, epoch: 29, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 082ms, time_since_start: 01h 04m 18s 298ms, eta: 22m 58s 526ms
[32m2022-10-10T23:55:14 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:55:14 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:55:14 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:55:26 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:55:26 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:55:26 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:55:29 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:55:32 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:55:32 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, val/hateful_memes/cross_entropy: 0.7328, val/total_loss: 0.7328, val/hateful_memes/accuracy: 0.5340, val/hateful_memes/binary_f1: 0.2048, val/hateful_memes/roc_auc: 0.5753, num_updates: 15000, epoch: 29, iterations: 15000, max_updates: 22000, val_time: 18s 654ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.575379
[32m2022-10-10T23:55:51 | mmf.trainers.callbacks.logistics: [39mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.6302, train/hateful_memes/cross_entropy/avg: 0.6265, train/total_loss: 0.6302, train/total_loss/avg: 0.6265, max mem: 1039.0, experiment: hmd, epoch: 29, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 263ms, time_since_start: 01h 04m 56s 223ms, eta: 22m 51s 702ms
[32m2022-10-10T23:56:11 | mmf.trainers.callbacks.logistics: [39mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.6302, train/hateful_memes/cross_entropy/avg: 0.6273, train/total_loss: 0.6302, train/total_loss/avg: 0.6273, max mem: 1039.0, experiment: hmd, epoch: 29, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 726ms, time_since_start: 01h 05m 15s 949ms, eta: 23m 04s 320ms
[32m2022-10-10T23:56:31 | mmf.trainers.callbacks.logistics: [39mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.6302, train/hateful_memes/cross_entropy/avg: 0.6270, train/total_loss: 0.6302, train/total_loss/avg: 0.6270, max mem: 1039.0, experiment: hmd, epoch: 29, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 5.00, time: 20s 161ms, time_since_start: 01h 05m 36s 110ms, eta: 23m 14s 014ms
[32m2022-10-10T23:56:54 | mmf.trainers.callbacks.logistics: [39mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.6258, train/hateful_memes/cross_entropy/avg: 0.6256, train/total_loss: 0.6258, train/total_loss/avg: 0.6256, max mem: 1039.0, experiment: hmd, epoch: 29, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 882ms, time_since_start: 01h 05m 58s 993ms, eta: 25m 58s 545ms
[32m2022-10-10T23:57:13 | mmf.trainers.callbacks.logistics: [39mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.6258, train/hateful_memes/cross_entropy/avg: 0.6254, train/total_loss: 0.6258, train/total_loss/avg: 0.6254, max mem: 1039.0, experiment: hmd, epoch: 30, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 133ms, time_since_start: 01h 06m 18s 126ms, eta: 21m 23s 447ms
[32m2022-10-10T23:57:32 | mmf.trainers.callbacks.logistics: [39mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.6262, train/hateful_memes/cross_entropy/avg: 0.6254, train/total_loss: 0.6262, train/total_loss/avg: 0.6254, max mem: 1039.0, experiment: hmd, epoch: 30, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 501ms, time_since_start: 01h 06m 36s 627ms, eta: 20m 21s 988ms
[32m2022-10-10T23:57:51 | mmf.trainers.callbacks.logistics: [39mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.6258, train/hateful_memes/cross_entropy/avg: 0.6243, train/total_loss: 0.6258, train/total_loss/avg: 0.6243, max mem: 1039.0, experiment: hmd, epoch: 30, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 640ms, time_since_start: 01h 06m 55s 268ms, eta: 20m 11s 933ms
[32m2022-10-10T23:58:09 | mmf.trainers.callbacks.logistics: [39mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.5972, train/hateful_memes/cross_entropy/avg: 0.6236, train/total_loss: 0.5972, train/total_loss/avg: 0.6236, max mem: 1039.0, experiment: hmd, epoch: 30, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 773ms, time_since_start: 01h 07m 14s 041ms, eta: 20m 01s 199ms
[32m2022-10-10T23:58:28 | mmf.trainers.callbacks.logistics: [39mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.6258, train/hateful_memes/cross_entropy/avg: 0.6237, train/total_loss: 0.6258, train/total_loss/avg: 0.6237, max mem: 1039.0, experiment: hmd, epoch: 30, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 595ms, time_since_start: 01h 07m 32s 637ms, eta: 19m 30s 644ms
[32m2022-10-10T23:58:46 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-10T23:58:46 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:58:46 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:58:49 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:58:49 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.6044, train/hateful_memes/cross_entropy/avg: 0.6236, train/total_loss: 0.6044, train/total_loss/avg: 0.6236, max mem: 1039.0, experiment: hmd, epoch: 31, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 5.00, time: 20s 936ms, time_since_start: 01h 07m 53s 573ms, eta: 21m 36s 365ms
[32m2022-10-10T23:58:49 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-10T23:58:49 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-10T23:58:49 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-10T23:58:58 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-10T23:58:58 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-10T23:58:58 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-10T23:59:01 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-10T23:59:04 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-10T23:59:04 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, val/hateful_memes/cross_entropy: 0.7662, val/total_loss: 0.7662, val/hateful_memes/accuracy: 0.5260, val/hateful_memes/binary_f1: 0.1255, val/hateful_memes/roc_auc: 0.5745, num_updates: 16000, epoch: 31, iterations: 16000, max_updates: 22000, val_time: 15s 206ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.575379
[32m2022-10-10T23:59:23 | mmf.trainers.callbacks.logistics: [39mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.6258, train/hateful_memes/cross_entropy/avg: 0.6236, train/total_loss: 0.6258, train/total_loss/avg: 0.6236, max mem: 1039.0, experiment: hmd, epoch: 31, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 5.56, time: 18s 559ms, time_since_start: 01h 08m 27s 342ms, eta: 18m 50s 050ms
[32m2022-10-10T23:59:41 | mmf.trainers.callbacks.logistics: [39mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.6044, train/hateful_memes/cross_entropy/avg: 0.6231, train/total_loss: 0.6044, train/total_loss/avg: 0.6231, max mem: 1039.0, experiment: hmd, epoch: 31, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 5.56, time: 18s 546ms, time_since_start: 01h 08m 45s 888ms, eta: 18m 30s 100ms
[32m2022-10-11T00:00:01 | mmf.trainers.callbacks.logistics: [39mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.5972, train/hateful_memes/cross_entropy/avg: 0.6228, train/total_loss: 0.5972, train/total_loss/avg: 0.6228, max mem: 1039.0, experiment: hmd, epoch: 31, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 5.26, time: 19s 594ms, time_since_start: 01h 09m 05s 483ms, eta: 19m 12s 649ms
[32m2022-10-11T00:00:24 | mmf.trainers.callbacks.logistics: [39mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.5866, train/hateful_memes/cross_entropy/avg: 0.6220, train/total_loss: 0.5866, train/total_loss/avg: 0.6220, max mem: 1039.0, experiment: hmd, epoch: 31, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 4.35, time: 23s 540ms, time_since_start: 01h 09m 29s 024ms, eta: 22m 40s 469ms
[32m2022-10-11T00:00:52 | mmf.trainers.callbacks.logistics: [39mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.5866, train/hateful_memes/cross_entropy/avg: 0.6222, train/total_loss: 0.5866, train/total_loss/avg: 0.6222, max mem: 1039.0, experiment: hmd, epoch: 32, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 3.70, time: 27s 803ms, time_since_start: 01h 09m 56s 827ms, eta: 26m 18s 128ms
[32m2022-10-11T00:01:09 | mmf.trainers.callbacks.logistics: [39mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.5866, train/hateful_memes/cross_entropy/avg: 0.6221, train/total_loss: 0.5866, train/total_loss/avg: 0.6221, max mem: 1039.0, experiment: hmd, epoch: 32, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 6.25, time: 16s 471ms, time_since_start: 01h 10m 13s 299ms, eta: 15m 17s 919ms
[32m2022-10-11T00:01:25 | mmf.trainers.callbacks.logistics: [39mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.5814, train/hateful_memes/cross_entropy/avg: 0.6209, train/total_loss: 0.5814, train/total_loss/avg: 0.6209, max mem: 1039.0, experiment: hmd, epoch: 32, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 6.25, time: 16s 617ms, time_since_start: 01h 10m 29s 916ms, eta: 15m 08s 894ms
[32m2022-10-11T00:01:42 | mmf.trainers.callbacks.logistics: [39mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.5841, train/hateful_memes/cross_entropy/avg: 0.6206, train/total_loss: 0.5841, train/total_loss/avg: 0.6206, max mem: 1039.0, experiment: hmd, epoch: 32, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 6.25, time: 16s 860ms, time_since_start: 01h 10m 46s 776ms, eta: 15m 04s 805ms
[32m2022-10-11T00:01:59 | mmf.trainers.callbacks.logistics: [39mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.5866, train/hateful_memes/cross_entropy/avg: 0.6205, train/total_loss: 0.5866, train/total_loss/avg: 0.6205, max mem: 1039.0, experiment: hmd, epoch: 32, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 6.25, time: 16s 863ms, time_since_start: 01h 11m 03s 640ms, eta: 14m 47s 581ms
[32m2022-10-11T00:02:16 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T00:02:16 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:02:16 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:02:19 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:02:19 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.5841, train/hateful_memes/cross_entropy/avg: 0.6199, train/total_loss: 0.5841, train/total_loss/avg: 0.6199, max mem: 1039.0, experiment: hmd, epoch: 32, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 5.00, time: 20s 139ms, time_since_start: 01h 11m 23s 780ms, eta: 17m 19s 215ms
[32m2022-10-11T00:02:19 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T00:02:19 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T00:02:19 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T00:02:32 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T00:02:32 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T00:02:32 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:02:35 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-11T00:02:38 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:02:41 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:02:41 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, val/hateful_memes/cross_entropy: 0.7788, val/total_loss: 0.7788, val/hateful_memes/accuracy: 0.5200, val/hateful_memes/binary_f1: 0.0909, val/hateful_memes/roc_auc: 0.5774, num_updates: 17000, epoch: 32, iterations: 17000, max_updates: 22000, val_time: 22s 221ms, best_update: 17000, best_iteration: 17000, best_val/hateful_memes/roc_auc: 0.577363
[32m2022-10-11T00:03:04 | mmf.trainers.callbacks.logistics: [39mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.5866, train/hateful_memes/cross_entropy/avg: 0.6212, train/total_loss: 0.5866, train/total_loss/avg: 0.6212, max mem: 1039.0, experiment: hmd, epoch: 33, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 4.35, time: 23s 159ms, time_since_start: 01h 12m 09s 167ms, eta: 19m 31s 124ms
[32m2022-10-11T00:03:25 | mmf.trainers.callbacks.logistics: [39mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.5866, train/hateful_memes/cross_entropy/avg: 0.6213, train/total_loss: 0.5866, train/total_loss/avg: 0.6213, max mem: 1039.0, experiment: hmd, epoch: 33, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 5.00, time: 20s 960ms, time_since_start: 01h 12m 30s 128ms, eta: 17m 18s 324ms
[32m2022-10-11T00:03:48 | mmf.trainers.callbacks.logistics: [39mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.5941, train/hateful_memes/cross_entropy/avg: 0.6223, train/total_loss: 0.5941, train/total_loss/avg: 0.6223, max mem: 1039.0, experiment: hmd, epoch: 33, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 4.55, time: 22s 942ms, time_since_start: 01h 12m 53s 071ms, eta: 18m 32s 822ms
[32m2022-10-11T00:04:14 | mmf.trainers.callbacks.logistics: [39mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.5972, train/hateful_memes/cross_entropy/avg: 0.6226, train/total_loss: 0.5972, train/total_loss/avg: 0.6226, max mem: 1039.0, experiment: hmd, epoch: 33, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 4.00, time: 25s 311ms, time_since_start: 01h 13m 18s 382ms, eta: 20m 01s 582ms
[32m2022-10-11T00:04:46 | mmf.trainers.callbacks.logistics: [39mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.5941, train/hateful_memes/cross_entropy/avg: 0.6220, train/total_loss: 0.5941, train/total_loss/avg: 0.6220, max mem: 1039.0, experiment: hmd, epoch: 33, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 3.12, time: 32s 462ms, time_since_start: 01h 13m 50s 845ms, eta: 25m 07s 572ms
[32m2022-10-11T00:05:12 | mmf.trainers.callbacks.logistics: [39mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.5941, train/hateful_memes/cross_entropy/avg: 0.6220, train/total_loss: 0.5941, train/total_loss/avg: 0.6220, max mem: 1039.0, experiment: hmd, epoch: 34, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 3.85, time: 26s 368ms, time_since_start: 01h 14m 17s 213ms, eta: 19m 57s 335ms
[32m2022-10-11T00:05:28 | mmf.trainers.callbacks.logistics: [39mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.6044, train/hateful_memes/cross_entropy/avg: 0.6220, train/total_loss: 0.6044, train/total_loss/avg: 0.6220, max mem: 1039.0, experiment: hmd, epoch: 34, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 6.67, time: 15s 611ms, time_since_start: 01h 14m 32s 825ms, eta: 11m 32s 771ms
[32m2022-10-11T00:05:44 | mmf.trainers.callbacks.logistics: [39mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.6134, train/hateful_memes/cross_entropy/avg: 0.6221, train/total_loss: 0.6134, train/total_loss/avg: 0.6221, max mem: 1039.0, experiment: hmd, epoch: 34, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 6.67, time: 15s 657ms, time_since_start: 01h 14m 48s 483ms, eta: 11m 18s 667ms
[32m2022-10-11T00:05:59 | mmf.trainers.callbacks.logistics: [39mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.6134, train/hateful_memes/cross_entropy/avg: 0.6220, train/total_loss: 0.6134, train/total_loss/avg: 0.6220, max mem: 1039.0, experiment: hmd, epoch: 34, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 6.67, time: 15s 731ms, time_since_start: 01h 15m 04s 214ms, eta: 11m 05s 635ms
[32m2022-10-11T00:06:15 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T00:06:15 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:06:16 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:06:18 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:06:19 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.6134, train/hateful_memes/cross_entropy/avg: 0.6219, train/total_loss: 0.6134, train/total_loss/avg: 0.6219, max mem: 1039.0, experiment: hmd, epoch: 34, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 5.26, time: 19s 007ms, time_since_start: 01h 15m 23s 222ms, eta: 13m 04s 647ms
[32m2022-10-11T00:06:19 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T00:06:19 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T00:06:19 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T00:06:25 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T00:06:25 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T00:06:25 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:06:28 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-11T00:06:31 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:06:34 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:06:34 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, val/hateful_memes/cross_entropy: 0.7431, val/total_loss: 0.7431, val/hateful_memes/accuracy: 0.5320, val/hateful_memes/binary_f1: 0.1875, val/hateful_memes/roc_auc: 0.5778, num_updates: 18000, epoch: 34, iterations: 18000, max_updates: 22000, val_time: 15s 393ms, best_update: 18000, best_iteration: 18000, best_val/hateful_memes/roc_auc: 0.577795
[32m2022-10-11T00:07:04 | mmf.trainers.callbacks.logistics: [39mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.6134, train/hateful_memes/cross_entropy/avg: 0.6219, train/total_loss: 0.6134, train/total_loss/avg: 0.6219, max mem: 1039.0, experiment: hmd, epoch: 35, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 3.33, time: 30s 429ms, time_since_start: 01h 16m 09s 051ms, eta: 20m 24s 734ms
[32m2022-10-11T00:07:24 | mmf.trainers.callbacks.logistics: [39mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.6173, train/hateful_memes/cross_entropy/avg: 0.6220, train/total_loss: 0.6173, train/total_loss/avg: 0.6220, max mem: 1039.0, experiment: hmd, epoch: 35, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 5.26, time: 19s 453ms, time_since_start: 01h 16m 28s 505ms, eta: 12m 42s 887ms
[32m2022-10-11T00:07:44 | mmf.trainers.callbacks.logistics: [39mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.6174, train/hateful_memes/cross_entropy/avg: 0.6225, train/total_loss: 0.6174, train/total_loss/avg: 0.6225, max mem: 1039.0, experiment: hmd, epoch: 35, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 5.00, time: 20s 326ms, time_since_start: 01h 16m 48s 831ms, eta: 12m 56s 141ms
[32m2022-10-11T00:08:06 | mmf.trainers.callbacks.logistics: [39mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.6208, train/hateful_memes/cross_entropy/avg: 0.6228, train/total_loss: 0.6208, train/total_loss/avg: 0.6228, max mem: 1039.0, experiment: hmd, epoch: 35, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 4.55, time: 22s 201ms, time_since_start: 01h 17m 11s 033ms, eta: 13m 44s 847ms
[32m2022-10-11T00:08:31 | mmf.trainers.callbacks.logistics: [39mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.6208, train/hateful_memes/cross_entropy/avg: 0.6231, train/total_loss: 0.6208, train/total_loss/avg: 0.6231, max mem: 1039.0, experiment: hmd, epoch: 35, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 4.17, time: 24s 799ms, time_since_start: 01h 17m 35s 833ms, eta: 14m 55s 763ms
[32m2022-10-11T00:08:57 | mmf.trainers.callbacks.logistics: [39mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.6208, train/hateful_memes/cross_entropy/avg: 0.6226, train/total_loss: 0.6208, train/total_loss/avg: 0.6226, max mem: 1039.0, experiment: hmd, epoch: 35, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 3.85, time: 26s 259ms, time_since_start: 01h 18m 02s 092ms, eta: 15m 21s 379ms
[32m2022-10-11T00:09:15 | mmf.trainers.callbacks.logistics: [39mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.6276, train/hateful_memes/cross_entropy/avg: 0.6228, train/total_loss: 0.6276, train/total_loss/avg: 0.6228, max mem: 1039.0, experiment: hmd, epoch: 36, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 5.88, time: 17s 227ms, time_since_start: 01h 18m 19s 319ms, eta: 09m 46s 697ms
[32m2022-10-11T00:09:30 | mmf.trainers.callbacks.logistics: [39mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.6276, train/hateful_memes/cross_entropy/avg: 0.6227, train/total_loss: 0.6276, train/total_loss/avg: 0.6227, max mem: 1039.0, experiment: hmd, epoch: 36, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 6.67, time: 15s 601ms, time_since_start: 01h 18m 34s 921ms, eta: 08m 35s 232ms
[32m2022-10-11T00:09:46 | mmf.trainers.callbacks.logistics: [39mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.6329, train/hateful_memes/cross_entropy/avg: 0.6231, train/total_loss: 0.6329, train/total_loss/avg: 0.6231, max mem: 1039.0, experiment: hmd, epoch: 36, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 6.67, time: 15s 619ms, time_since_start: 01h 18m 50s 540ms, eta: 08m 19s 694ms
[32m2022-10-11T00:10:01 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T00:10:01 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:10:02 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:10:05 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:10:05 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.6329, train/hateful_memes/cross_entropy/avg: 0.6224, train/total_loss: 0.6329, train/total_loss/avg: 0.6224, max mem: 1039.0, experiment: hmd, epoch: 36, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 5.26, time: 19s 197ms, time_since_start: 01h 19m 09s 738ms, eta: 09m 54s 356ms
[32m2022-10-11T00:10:05 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T00:10:05 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T00:10:05 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T00:10:18 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T00:10:18 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T00:10:18 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:10:21 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-11T00:10:23 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:10:26 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:10:26 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, val/hateful_memes/cross_entropy: 0.7939, val/total_loss: 0.7939, val/hateful_memes/accuracy: 0.5200, val/hateful_memes/binary_f1: 0.0769, val/hateful_memes/roc_auc: 0.5789, num_updates: 19000, epoch: 36, iterations: 19000, max_updates: 22000, val_time: 21s 018ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.578931
[32m2022-10-11T00:10:53 | mmf.trainers.callbacks.logistics: [39mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.6329, train/hateful_memes/cross_entropy/avg: 0.6229, train/total_loss: 0.6329, train/total_loss/avg: 0.6229, max mem: 1039.0, experiment: hmd, epoch: 36, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 3.85, time: 26s 575ms, time_since_start: 01h 19m 57s 338ms, eta: 13m 15s 344ms
[32m2022-10-11T00:11:16 | mmf.trainers.callbacks.logistics: [39mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.6276, train/hateful_memes/cross_entropy/avg: 0.6219, train/total_loss: 0.6276, train/total_loss/avg: 0.6219, max mem: 1039.0, experiment: hmd, epoch: 37, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 4.35, time: 23s 835ms, time_since_start: 01h 20m 21s 174ms, eta: 11m 28s 763ms
[32m2022-10-11T00:11:37 | mmf.trainers.callbacks.logistics: [39mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.6208, train/hateful_memes/cross_entropy/avg: 0.6212, train/total_loss: 0.6208, train/total_loss/avg: 0.6212, max mem: 1039.0, experiment: hmd, epoch: 37, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 5.00, time: 20s 092ms, time_since_start: 01h 20m 41s 266ms, eta: 09m 19s 866ms
[32m2022-10-11T00:11:58 | mmf.trainers.callbacks.logistics: [39mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.6174, train/hateful_memes/cross_entropy/avg: 0.6206, train/total_loss: 0.6174, train/total_loss/avg: 0.6206, max mem: 1039.0, experiment: hmd, epoch: 37, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 4.76, time: 21s 102ms, time_since_start: 01h 21m 02s 369ms, eta: 09m 26s 226ms
[32m2022-10-11T00:12:22 | mmf.trainers.callbacks.logistics: [39mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.6208, train/hateful_memes/cross_entropy/avg: 0.6208, train/total_loss: 0.6208, train/total_loss/avg: 0.6208, max mem: 1039.0, experiment: hmd, epoch: 37, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 4.17, time: 24s 229ms, time_since_start: 01h 21m 26s 598ms, eta: 10m 25s 117ms
[32m2022-10-11T00:12:49 | mmf.trainers.callbacks.logistics: [39mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.6208, train/hateful_memes/cross_entropy/avg: 0.6211, train/total_loss: 0.6208, train/total_loss/avg: 0.6211, max mem: 1039.0, experiment: hmd, epoch: 37, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 3.85, time: 26s 803ms, time_since_start: 01h 21m 53s 402ms, eta: 11m 03s 866ms
[32m2022-10-11T00:13:14 | mmf.trainers.callbacks.logistics: [39mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.6174, train/hateful_memes/cross_entropy/avg: 0.6207, train/total_loss: 0.6174, train/total_loss/avg: 0.6207, max mem: 1039.0, experiment: hmd, epoch: 38, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 4.00, time: 25s 213ms, time_since_start: 01h 22m 18s 615ms, eta: 09m 58s 465ms
[32m2022-10-11T00:13:29 | mmf.trainers.callbacks.logistics: [39mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.6174, train/hateful_memes/cross_entropy/avg: 0.6211, train/total_loss: 0.6174, train/total_loss/avg: 0.6211, max mem: 1039.0, experiment: hmd, epoch: 38, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 6.67, time: 15s 563ms, time_since_start: 01h 22m 34s 179ms, eta: 05m 53s 361ms
[32m2022-10-11T00:13:45 | mmf.trainers.callbacks.logistics: [39mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.6174, train/hateful_memes/cross_entropy/avg: 0.6209, train/total_loss: 0.6174, train/total_loss/avg: 0.6209, max mem: 1039.0, experiment: hmd, epoch: 38, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 6.67, time: 15s 614ms, time_since_start: 01h 22m 49s 793ms, eta: 05m 38s 396ms
[32m2022-10-11T00:14:01 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T00:14:01 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:14:01 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:14:04 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:14:04 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.6397, train/hateful_memes/cross_entropy/avg: 0.6211, train/total_loss: 0.6397, train/total_loss/avg: 0.6211, max mem: 1039.0, experiment: hmd, epoch: 38, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 5.26, time: 19s 301ms, time_since_start: 01h 23m 09s 095ms, eta: 06m 38s 377ms
[32m2022-10-11T00:14:04 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T00:14:04 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T00:14:04 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T00:14:14 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T00:14:14 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T00:14:14 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:14:17 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:14:20 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:14:20 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, val/hateful_memes/cross_entropy: 0.7519, val/total_loss: 0.7519, val/hateful_memes/accuracy: 0.5400, val/hateful_memes/binary_f1: 0.1844, val/hateful_memes/roc_auc: 0.5783, num_updates: 20000, epoch: 38, iterations: 20000, max_updates: 22000, val_time: 15s 620ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.578931
[32m2022-10-11T00:14:42 | mmf.trainers.callbacks.logistics: [39mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.6397, train/hateful_memes/cross_entropy/avg: 0.6204, train/total_loss: 0.6397, train/total_loss/avg: 0.6204, max mem: 1039.0, experiment: hmd, epoch: 38, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 4.55, time: 22s 389ms, time_since_start: 01h 23m 47s 107ms, eta: 07m 19s 016ms
[32m2022-10-11T00:15:08 | mmf.trainers.callbacks.logistics: [39mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.6108, train/hateful_memes/cross_entropy/avg: 0.6204, train/total_loss: 0.6108, train/total_loss/avg: 0.6204, max mem: 1039.0, experiment: hmd, epoch: 38, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 4.00, time: 25s 725ms, time_since_start: 01h 24m 12s 833ms, eta: 07m 57s 878ms
[32m2022-10-11T00:15:27 | mmf.trainers.callbacks.logistics: [39mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.6108, train/hateful_memes/cross_entropy/avg: 0.6203, train/total_loss: 0.6108, train/total_loss/avg: 0.6203, max mem: 1039.0, experiment: hmd, epoch: 39, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 5.26, time: 19s 269ms, time_since_start: 01h 24m 32s 102ms, eta: 05m 38s 069ms
[32m2022-10-11T00:15:45 | mmf.trainers.callbacks.logistics: [39mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.6026, train/hateful_memes/cross_entropy/avg: 0.6196, train/total_loss: 0.6026, train/total_loss/avg: 0.6196, max mem: 1039.0, experiment: hmd, epoch: 39, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 5.56, time: 18s 011ms, time_since_start: 01h 24m 50s 114ms, eta: 04m 57s 401ms
[32m2022-10-11T00:16:05 | mmf.trainers.callbacks.logistics: [39mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.5783, train/hateful_memes/cross_entropy/avg: 0.6193, train/total_loss: 0.5783, train/total_loss/avg: 0.6193, max mem: 1039.0, experiment: hmd, epoch: 39, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 5.26, time: 19s 363ms, time_since_start: 01h 25m 09s 477ms, eta: 04m 59s 750ms
[32m2022-10-11T00:16:25 | mmf.trainers.callbacks.logistics: [39mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.6026, train/hateful_memes/cross_entropy/avg: 0.6196, train/total_loss: 0.6026, train/total_loss/avg: 0.6196, max mem: 1039.0, experiment: hmd, epoch: 39, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 5.00, time: 20s 506ms, time_since_start: 01h 25m 29s 984ms, eta: 04m 56s 281ms
[32m2022-10-11T00:16:48 | mmf.trainers.callbacks.logistics: [39mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.5783, train/hateful_memes/cross_entropy/avg: 0.6193, train/total_loss: 0.5783, train/total_loss/avg: 0.6193, max mem: 1039.0, experiment: hmd, epoch: 39, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 4.55, time: 22s 834ms, time_since_start: 01h 25m 52s 818ms, eta: 05m 06s 344ms
[32m2022-10-11T00:17:07 | mmf.trainers.callbacks.logistics: [39mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.5783, train/hateful_memes/cross_entropy/avg: 0.6192, train/total_loss: 0.5783, train/total_loss/avg: 0.6192, max mem: 1039.0, experiment: hmd, epoch: 40, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 5.56, time: 18s 625ms, time_since_start: 01h 26m 11s 444ms, eta: 03m 50s 655ms
[32m2022-10-11T00:17:22 | mmf.trainers.callbacks.logistics: [39mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.5783, train/hateful_memes/cross_entropy/avg: 0.6191, train/total_loss: 0.5783, train/total_loss/avg: 0.6191, max mem: 1039.0, experiment: hmd, epoch: 40, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 6.67, time: 15s 616ms, time_since_start: 01h 26m 27s 060ms, eta: 02m 57s 274ms
[32m2022-10-11T00:17:38 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T00:17:38 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:17:39 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:17:41 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:17:41 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.5954, train/hateful_memes/cross_entropy/avg: 0.6193, train/total_loss: 0.5954, train/total_loss/avg: 0.6193, max mem: 1039.0, experiment: hmd, epoch: 40, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 5.26, time: 19s 123ms, time_since_start: 01h 26m 46s 184ms, eta: 03m 17s 359ms
[32m2022-10-11T00:17:41 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T00:17:41 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T00:17:41 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T00:17:49 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T00:17:49 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T00:17:49 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:17:52 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:17:55 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:17:55 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, val/hateful_memes/cross_entropy: 0.7787, val/total_loss: 0.7787, val/hateful_memes/accuracy: 0.5200, val/hateful_memes/binary_f1: 0.0977, val/hateful_memes/roc_auc: 0.5784, num_updates: 21000, epoch: 40, iterations: 21000, max_updates: 22000, val_time: 13s 614ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.578931
[32m2022-10-11T00:18:16 | mmf.trainers.callbacks.logistics: [39mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.5954, train/hateful_memes/cross_entropy/avg: 0.6202, train/total_loss: 0.5954, train/total_loss/avg: 0.6202, max mem: 1039.0, experiment: hmd, epoch: 40, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 5.00, time: 20s 430ms, time_since_start: 01h 27m 20s 235ms, eta: 03m 09s 760ms
[32m2022-10-11T00:18:38 | mmf.trainers.callbacks.logistics: [39mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.5954, train/hateful_memes/cross_entropy/avg: 0.6197, train/total_loss: 0.5954, train/total_loss/avg: 0.6197, max mem: 1039.0, experiment: hmd, epoch: 40, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 4.55, time: 22s 398ms, time_since_start: 01h 27m 42s 634ms, eta: 03m 04s 919ms
[32m2022-10-11T00:19:01 | mmf.trainers.callbacks.logistics: [39mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.5954, train/hateful_memes/cross_entropy/avg: 0.6193, train/total_loss: 0.5954, train/total_loss/avg: 0.6193, max mem: 1039.0, experiment: hmd, epoch: 41, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 4.35, time: 23s 000ms, time_since_start: 01h 28m 05s 634ms, eta: 02m 46s 152ms
[32m2022-10-11T00:19:19 | mmf.trainers.callbacks.logistics: [39mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.5989, train/hateful_memes/cross_entropy/avg: 0.6194, train/total_loss: 0.5989, train/total_loss/avg: 0.6194, max mem: 1039.0, experiment: hmd, epoch: 41, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 5.56, time: 18s 241ms, time_since_start: 01h 28m 23s 875ms, eta: 01m 52s 950ms
[32m2022-10-11T00:19:38 | mmf.trainers.callbacks.logistics: [39mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.5954, train/hateful_memes/cross_entropy/avg: 0.6193, train/total_loss: 0.5954, train/total_loss/avg: 0.6193, max mem: 1039.0, experiment: hmd, epoch: 41, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 5.56, time: 18s 369ms, time_since_start: 01h 28m 42s 245ms, eta: 01m 34s 787ms
[32m2022-10-11T00:19:57 | mmf.trainers.callbacks.logistics: [39mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.5884, train/hateful_memes/cross_entropy/avg: 0.6192, train/total_loss: 0.5884, train/total_loss/avg: 0.6192, max mem: 1039.0, experiment: hmd, epoch: 41, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 5.26, time: 19s 129ms, time_since_start: 01h 29m 01s 374ms, eta: 01m 18s 966ms
[32m2022-10-11T00:20:18 | mmf.trainers.callbacks.logistics: [39mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.5954, train/hateful_memes/cross_entropy/avg: 0.6196, train/total_loss: 0.5954, train/total_loss/avg: 0.6196, max mem: 1039.0, experiment: hmd, epoch: 41, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 4.76, time: 21s 122ms, time_since_start: 01h 29m 22s 497ms, eta: 01m 05s 395ms
[32m2022-10-11T00:20:41 | mmf.trainers.callbacks.logistics: [39mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.5884, train/hateful_memes/cross_entropy/avg: 0.6190, train/total_loss: 0.5884, train/total_loss/avg: 0.6190, max mem: 1039.0, experiment: hmd, epoch: 41, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 4.35, time: 23s 147ms, time_since_start: 01h 29m 45s 645ms, eta: 47s 776ms
[32m2022-10-11T00:20:57 | mmf.trainers.callbacks.logistics: [39mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.5954, train/hateful_memes/cross_entropy/avg: 0.6195, train/total_loss: 0.5954, train/total_loss/avg: 0.6195, max mem: 1039.0, experiment: hmd, epoch: 42, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 6.25, time: 16s 144ms, time_since_start: 01h 30m 01s 789ms, eta: 16s 660ms
[32m2022-10-11T00:21:13 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T00:21:13 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:21:13 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:21:16 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:21:16 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.5954, train/hateful_memes/cross_entropy/avg: 0.6204, train/total_loss: 0.5954, train/total_loss/avg: 0.6204, max mem: 1039.0, experiment: hmd, epoch: 42, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 5.26, time: 19s 199ms, time_since_start: 01h 30m 20s 988ms, eta: 0ms
[32m2022-10-11T00:21:16 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T00:21:16 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T00:21:16 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T00:21:25 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T00:21:25 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T00:21:25 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:21:28 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:21:31 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:21:31 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, val/hateful_memes/cross_entropy: 0.7589, val/total_loss: 0.7589, val/hateful_memes/accuracy: 0.5280, val/hateful_memes/binary_f1: 0.1511, val/hateful_memes/roc_auc: 0.5775, num_updates: 22000, epoch: 42, iterations: 22000, max_updates: 22000, val_time: 14s 735ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.578931
[32m2022-10-11T00:21:31 | mmf.trainers.core.training_loop: [39mStepping into final validation check
[32m2022-10-11T00:21:31 | mmf.utils.checkpoint: [39mRestoring checkpoint
[32m2022-10-11T00:21:31 | mmf.utils.checkpoint: [39mLoading checkpoint
[32m2022-10-11T00:21:34 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-11T00:21:34 | mmf.utils.checkpoint: [39mCurrent num updates: 19000
[32m2022-10-11T00:21:34 | mmf.utils.checkpoint: [39mCurrent iteration: 19000
[32m2022-10-11T00:21:34 | mmf.utils.checkpoint: [39mCurrent epoch: 36
[32m2022-10-11T00:21:34 | mmf.utils.checkpoint: [39mSalvando o modelo final..
[32m2022-10-11T00:21:34 | mmf.trainers.mmf_trainer: [39mStarting inference on val set
[32m2022-10-11T00:21:34 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T00:21:34 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False

 78% 25/32 [00:03<00:00,  7.20it/s]
[32m2022-10-11T00:21:39 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T00:21:39 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T00:21:39 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, val/hateful_memes/cross_entropy: 0.7939, val/total_loss: 0.7939, val/hateful_memes/accuracy: 0.5200, val/hateful_memes/binary_f1: 0.0769, val/hateful_memes/roc_auc: 0.5789

100% 32/32 [00:04<00:00,  7.37it/s]
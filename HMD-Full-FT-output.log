
[32m2022-10-11T00:22:46 | mmf.utils.checkpoint: [39mLoading checkpoint
[31m[5mWARNING[39m[25m [32m2022-10-11T00:22:50 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-11T00:22:50 | mmf: [39mKey distributed is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-11T00:22:50 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-11T00:22:50 | mmf: [39mKey distributed is not present in registry, returning default value of None
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mPretrained model loaded
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCurrent num updates: 0
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCurrent iteration: 0
[32m2022-10-11T00:22:50 | mmf.utils.checkpoint: [39mCurrent epoch: 0
[32m2022-10-11T00:22:50 | mmf.trainers.mmf_trainer: [39m===== Model x=====
[32m2022-10-11T00:22:50 | mmf.trainers.mmf_trainer: [39mVisualBERT(
  (model): VisualBERTForClassification(
    (bert): VisualBERTBase(
      (embeddings): BertVisioLinguisticEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (token_type_embeddings_visual): Embedding(2, 768)
        (position_embeddings_visual): Embedding(512, 768)
        (projection): Linear(in_features=2048, out_features=768, bias=True)
      )
      (encoder): BertEncoderJit(
        (layer): ModuleList(
          (0): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Sequential(
      (0): BertPredictionHeadTransform(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (1): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-10-11T00:22:50 | mmf.utils.general: [39mTotal Parameters: 112044290. Trained Parameters: 112044290
[32m2022-10-11T00:22:50 | mmf.trainers.core.training_loop: [39mStarting training...
[32m2022-10-11T00:23:44 | mmf.trainers.callbacks.logistics: [39mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7333, train/hateful_memes/cross_entropy/avg: 0.7333, train/total_loss: 0.7333, train/total_loss/avg: 0.7333, max mem: 5554.0, experiment: hmd-ft, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 512ms, time_since_start: 01m 05s 451ms, eta: 03h 25m 20s 281ms
[32m2022-10-11T00:24:40 | mmf.trainers.callbacks.logistics: [39mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.7101, train/hateful_memes/cross_entropy/avg: 0.7217, train/total_loss: 0.7101, train/total_loss/avg: 0.7217, max mem: 5606.0, experiment: hmd-ft, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 947ms, time_since_start: 02m 01s 399ms, eta: 03h 29m 46s 916ms
[32m2022-10-11T00:25:37 | mmf.trainers.callbacks.logistics: [39mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.7101, train/hateful_memes/cross_entropy/avg: 0.6997, train/total_loss: 0.7101, train/total_loss/avg: 0.6997, max mem: 5655.0, experiment: hmd-ft, epoch: 1, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 376ms, time_since_start: 02m 57s 775ms, eta: 03h 30m 25s 105ms
[32m2022-10-11T00:26:33 | mmf.trainers.callbacks.logistics: [39mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6555, train/hateful_memes/cross_entropy/avg: 0.6718, train/total_loss: 0.6555, train/total_loss/avg: 0.6718, max mem: 5681.0, experiment: hmd-ft, epoch: 1, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 819ms, time_since_start: 03m 54s 594ms, eta: 03h 31m 05s 684ms
[32m2022-10-11T00:27:34 | mmf.trainers.callbacks.logistics: [39mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6555, train/hateful_memes/cross_entropy/avg: 0.6643, train/total_loss: 0.6555, train/total_loss/avg: 0.6643, max mem: 5681.0, experiment: hmd-ft, epoch: 1, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.67, time: 01m 727ms, time_since_start: 04m 55s 322ms, eta: 03h 44m 34s 309ms
[32m2022-10-11T00:28:26 | mmf.trainers.callbacks.logistics: [39mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6342, train/hateful_memes/cross_entropy/avg: 0.6143, train/total_loss: 0.6342, train/total_loss/avg: 0.6143, max mem: 5681.0, experiment: hmd-ft, epoch: 2, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 1.96, time: 51s 448ms, time_since_start: 05m 46s 770ms, eta: 03h 09m 22s 230ms
[32m2022-10-11T00:29:12 | mmf.trainers.callbacks.logistics: [39mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6342, train/hateful_memes/cross_entropy/avg: 0.5648, train/total_loss: 0.6342, train/total_loss/avg: 0.5648, max mem: 5693.0, experiment: hmd-ft, epoch: 2, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 273ms, time_since_start: 06m 33s 043ms, eta: 02h 49m 31s 550ms
[32m2022-10-11T00:29:58 | mmf.trainers.callbacks.logistics: [39mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.5884, train/hateful_memes/cross_entropy/avg: 0.5602, train/total_loss: 0.5884, train/total_loss/avg: 0.5602, max mem: 5693.0, experiment: hmd-ft, epoch: 2, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 300ms, time_since_start: 07m 19s 344ms, eta: 02h 48m 49s 890ms
[32m2022-10-11T00:30:47 | mmf.trainers.callbacks.logistics: [39mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.5884, train/hateful_memes/cross_entropy/avg: 0.5411, train/total_loss: 0.5884, train/total_loss/avg: 0.5411, max mem: 5743.0, experiment: hmd-ft, epoch: 2, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 797ms, time_since_start: 08m 08s 141ms, eta: 02h 57m 05s 675ms
[32m2022-10-11T00:31:39 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T00:31:39 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:31:44 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:31:47 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:31:47 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.5282, train/hateful_memes/cross_entropy/avg: 0.5014, train/total_loss: 0.5282, train/total_loss/avg: 0.5014, max mem: 5794.0, experiment: hmd-ft, epoch: 2, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.69, time: 59s 700ms, time_since_start: 09m 07s 842ms, eta: 03h 35m 38s 208ms
[32m2022-10-11T00:31:47 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T00:31:47 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T00:31:47 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T00:32:13 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T00:32:13 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T00:32:13 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:32:22 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-11T00:32:23 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:32:35 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:32:35 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.8731, val/total_loss: 0.8731, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.4783, val/hateful_memes/roc_auc: 0.6987, num_updates: 1000, epoch: 2, iterations: 1000, max_updates: 22000, val_time: 48s 070ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.698709
[32m2022-10-11T00:33:47 | mmf.trainers.callbacks.logistics: [39mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.5282, train/hateful_memes/cross_entropy/avg: 0.4775, train/total_loss: 0.5282, train/total_loss/avg: 0.4775, max mem: 5794.0, experiment: hmd-ft, epoch: 3, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 1.39, time: 01m 12s 198ms, time_since_start: 11m 08s 116ms, eta: 04h 19m 32s 353ms
[32m2022-10-11T00:34:46 | mmf.trainers.callbacks.logistics: [39mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.5282, train/hateful_memes/cross_entropy/avg: 0.4820, train/total_loss: 0.5282, train/total_loss/avg: 0.4820, max mem: 5794.0, experiment: hmd-ft, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 1.69, time: 59s 355ms, time_since_start: 12m 07s 471ms, eta: 03h 32m 20s 918ms
[32m2022-10-11T00:35:46 | mmf.trainers.callbacks.logistics: [39mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.5282, train/hateful_memes/cross_entropy/avg: 0.4674, train/total_loss: 0.5282, train/total_loss/avg: 0.4674, max mem: 5794.0, experiment: hmd-ft, epoch: 3, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 1.69, time: 59s 746ms, time_since_start: 13m 07s 217ms, eta: 03h 32m 43s 346ms
[32m2022-10-11T00:36:46 | mmf.trainers.callbacks.logistics: [39mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.3878, train/hateful_memes/cross_entropy/avg: 0.4526, train/total_loss: 0.3878, train/total_loss/avg: 0.4526, max mem: 5794.0, experiment: hmd-ft, epoch: 3, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 1.69, time: 59s 851ms, time_since_start: 14m 07s 069ms, eta: 03h 32m 04s 014ms
[32m2022-10-11T00:37:45 | mmf.trainers.callbacks.logistics: [39mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.3878, train/hateful_memes/cross_entropy/avg: 0.4444, train/total_loss: 0.3878, train/total_loss/avg: 0.4444, max mem: 5794.0, experiment: hmd-ft, epoch: 3, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 1.69, time: 59s 264ms, time_since_start: 15m 06s 333ms, eta: 03h 28m 57s 930ms
[32m2022-10-11T00:38:43 | mmf.trainers.callbacks.logistics: [39mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.3642, train/hateful_memes/cross_entropy/avg: 0.4325, train/total_loss: 0.3642, train/total_loss/avg: 0.4325, max mem: 5794.0, experiment: hmd-ft, epoch: 4, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 1.75, time: 57s 752ms, time_since_start: 16m 04s 086ms, eta: 03h 22m 38s 418ms
[32m2022-10-11T00:39:25 | mmf.trainers.callbacks.logistics: [39mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.3642, train/hateful_memes/cross_entropy/avg: 0.4082, train/total_loss: 0.3642, train/total_loss/avg: 0.4082, max mem: 5794.0, experiment: hmd-ft, epoch: 4, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 2.44, time: 41s 884ms, time_since_start: 16m 45s 970ms, eta: 02h 26m 14s 646ms
[32m2022-10-11T00:40:07 | mmf.trainers.callbacks.logistics: [39mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.3642, train/hateful_memes/cross_entropy/avg: 0.4161, train/total_loss: 0.3642, train/total_loss/avg: 0.4161, max mem: 5794.0, experiment: hmd-ft, epoch: 4, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 2.38, time: 42s 120ms, time_since_start: 17m 28s 091ms, eta: 02h 26m 20s 622ms
[32m2022-10-11T00:40:49 | mmf.trainers.callbacks.logistics: [39mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.3642, train/hateful_memes/cross_entropy/avg: 0.4030, train/total_loss: 0.3642, train/total_loss/avg: 0.4030, max mem: 5794.0, experiment: hmd-ft, epoch: 4, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 2.38, time: 42s 183ms, time_since_start: 18m 10s 274ms, eta: 02h 25m 50s 152ms
[32m2022-10-11T00:41:31 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T00:41:31 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:41:33 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:41:48 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:41:48 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.3295, train/hateful_memes/cross_entropy/avg: 0.3918, train/total_loss: 0.3295, train/total_loss/avg: 0.3918, max mem: 5794.0, experiment: hmd-ft, epoch: 4, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.69, time: 59s 009ms, time_since_start: 19m 09s 284ms, eta: 03h 22m 59s 641ms
[32m2022-10-11T00:41:48 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T00:41:48 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T00:41:48 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T00:41:57 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T00:41:57 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T00:41:57 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:42:05 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-11T00:42:13 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:42:21 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:42:21 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.9627, val/total_loss: 0.9627, val/hateful_memes/accuracy: 0.6740, val/hateful_memes/binary_f1: 0.6510, val/hateful_memes/roc_auc: 0.7439, num_updates: 2000, epoch: 4, iterations: 2000, max_updates: 22000, val_time: 32s 843ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T00:43:25 | mmf.trainers.callbacks.logistics: [39mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.2926, train/hateful_memes/cross_entropy/avg: 0.3776, train/total_loss: 0.2926, train/total_loss/avg: 0.3776, max mem: 5794.0, experiment: hmd-ft, epoch: 4, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 1.56, time: 01m 04s 059ms, time_since_start: 20m 46s 193ms, eta: 03h 39m 15s 820ms
[32m2022-10-11T00:44:24 | mmf.trainers.callbacks.logistics: [39mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.2680, train/hateful_memes/cross_entropy/avg: 0.3693, train/total_loss: 0.2680, train/total_loss/avg: 0.3693, max mem: 5794.0, experiment: hmd-ft, epoch: 5, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 1.72, time: 58s 862ms, time_since_start: 21m 45s 056ms, eta: 03h 20m 27s 759ms
[32m2022-10-11T00:45:20 | mmf.trainers.callbacks.logistics: [39mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.2600, train/hateful_memes/cross_entropy/avg: 0.3587, train/total_loss: 0.2600, train/total_loss/avg: 0.3587, max mem: 5794.0, experiment: hmd-ft, epoch: 5, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 1.79, time: 56s 353ms, time_since_start: 22m 41s 409ms, eta: 03h 10m 56s 924ms
[32m2022-10-11T00:46:17 | mmf.trainers.callbacks.logistics: [39mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.2537, train/hateful_memes/cross_entropy/avg: 0.3451, train/total_loss: 0.2537, train/total_loss/avg: 0.3451, max mem: 5794.0, experiment: hmd-ft, epoch: 5, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 1.79, time: 56s 770ms, time_since_start: 23m 38s 180ms, eta: 03h 11m 23s 129ms
[32m2022-10-11T00:47:13 | mmf.trainers.callbacks.logistics: [39mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.2387, train/hateful_memes/cross_entropy/avg: 0.3403, train/total_loss: 0.2387, train/total_loss/avg: 0.3403, max mem: 5794.0, experiment: hmd-ft, epoch: 5, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 1.79, time: 56s 384ms, time_since_start: 24m 34s 564ms, eta: 03h 09m 06s 777ms
[32m2022-10-11T00:48:11 | mmf.trainers.callbacks.logistics: [39mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.2348, train/hateful_memes/cross_entropy/avg: 0.3363, train/total_loss: 0.2348, train/total_loss/avg: 0.3363, max mem: 5794.0, experiment: hmd-ft, epoch: 5, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 1.75, time: 57s 403ms, time_since_start: 25m 31s 968ms, eta: 03h 11m 32s 673ms
[32m2022-10-11T00:49:01 | mmf.trainers.callbacks.logistics: [39mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.2348, train/hateful_memes/cross_entropy/avg: 0.3400, train/total_loss: 0.2348, train/total_loss/avg: 0.3400, max mem: 5794.0, experiment: hmd-ft, epoch: 6, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 2.04, time: 49s 935ms, time_since_start: 26m 21s 903ms, eta: 02h 45m 45s 900ms
[32m2022-10-11T00:49:43 | mmf.trainers.callbacks.logistics: [39mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.2253, train/hateful_memes/cross_entropy/avg: 0.3282, train/total_loss: 0.2253, train/total_loss/avg: 0.3282, max mem: 5794.0, experiment: hmd-ft, epoch: 6, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 2.44, time: 41s 896ms, time_since_start: 27m 03s 799ms, eta: 02h 18m 21s 468ms
[32m2022-10-11T00:50:25 | mmf.trainers.callbacks.logistics: [39mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.1954, train/hateful_memes/cross_entropy/avg: 0.3191, train/total_loss: 0.1954, train/total_loss/avg: 0.3191, max mem: 5794.0, experiment: hmd-ft, epoch: 6, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 2.44, time: 41s 966ms, time_since_start: 27m 45s 766ms, eta: 02h 17m 52s 039ms
[32m2022-10-11T00:51:07 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T00:51:07 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:51:08 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:51:24 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:51:24 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.1954, train/hateful_memes/cross_entropy/avg: 0.3147, train/total_loss: 0.1954, train/total_loss/avg: 0.3147, max mem: 5794.0, experiment: hmd-ft, epoch: 6, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.72, time: 58s 993ms, time_since_start: 28m 44s 759ms, eta: 03h 12m 47s 394ms
[32m2022-10-11T00:51:24 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T00:51:24 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T00:51:24 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T00:51:33 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T00:51:33 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T00:51:33 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T00:51:42 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T00:51:50 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T00:51:50 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.4502, val/total_loss: 1.4502, val/hateful_memes/accuracy: 0.6260, val/hateful_memes/binary_f1: 0.5092, val/hateful_memes/roc_auc: 0.7245, num_updates: 3000, epoch: 6, iterations: 3000, max_updates: 22000, val_time: 26s 236ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T00:52:45 | mmf.trainers.callbacks.logistics: [39mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1847, train/hateful_memes/cross_entropy/avg: 0.3070, train/total_loss: 0.1847, train/total_loss/avg: 0.3070, max mem: 5794.0, experiment: hmd-ft, epoch: 6, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 1.82, time: 55s 337ms, time_since_start: 30m 06s 334ms, eta: 02h 59m 53s 406ms
[32m2022-10-11T00:53:42 | mmf.trainers.callbacks.logistics: [39mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1788, train/hateful_memes/cross_entropy/avg: 0.2989, train/total_loss: 0.1788, train/total_loss/avg: 0.2989, max mem: 5794.0, experiment: hmd-ft, epoch: 7, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 1.79, time: 56s 545ms, time_since_start: 31m 02s 880ms, eta: 03h 02m 50s 710ms
[32m2022-10-11T00:54:33 | mmf.trainers.callbacks.logistics: [39mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.1666, train/hateful_memes/cross_entropy/avg: 0.2921, train/total_loss: 0.1666, train/total_loss/avg: 0.2921, max mem: 5794.0, experiment: hmd-ft, epoch: 7, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 2.00, time: 50s 838ms, time_since_start: 31m 53s 718ms, eta: 02h 43m 30s 930ms
[32m2022-10-11T00:55:26 | mmf.trainers.callbacks.logistics: [39mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.1252, train/hateful_memes/cross_entropy/avg: 0.2854, train/total_loss: 0.1252, train/total_loss/avg: 0.2854, max mem: 5794.0, experiment: hmd-ft, epoch: 7, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 1.89, time: 53s 210ms, time_since_start: 32m 46s 929ms, eta: 02h 50m 13s 958ms
[32m2022-10-11T00:56:19 | mmf.trainers.callbacks.logistics: [39mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.1168, train/hateful_memes/cross_entropy/avg: 0.2805, train/total_loss: 0.1168, train/total_loss/avg: 0.2805, max mem: 5794.0, experiment: hmd-ft, epoch: 7, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 1.92, time: 52s 997ms, time_since_start: 33m 39s 926ms, eta: 02h 48m 38s 238ms
[32m2022-10-11T00:57:12 | mmf.trainers.callbacks.logistics: [39mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0944, train/hateful_memes/cross_entropy/avg: 0.2729, train/total_loss: 0.0944, train/total_loss/avg: 0.2729, max mem: 5794.0, experiment: hmd-ft, epoch: 7, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 1.92, time: 52s 946ms, time_since_start: 34m 32s 873ms, eta: 02h 47m 33s 904ms
[32m2022-10-11T00:58:05 | mmf.trainers.callbacks.logistics: [39mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0944, train/hateful_memes/cross_entropy/avg: 0.2675, train/total_loss: 0.0944, train/total_loss/avg: 0.2675, max mem: 5794.0, experiment: hmd-ft, epoch: 7, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 1.89, time: 53s 303ms, time_since_start: 35m 26s 176ms, eta: 02h 47m 46s 733ms
[32m2022-10-11T00:58:50 | mmf.trainers.callbacks.logistics: [39mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0773, train/hateful_memes/cross_entropy/avg: 0.2606, train/total_loss: 0.0773, train/total_loss/avg: 0.2606, max mem: 5794.0, experiment: hmd-ft, epoch: 8, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 2.27, time: 44s 437ms, time_since_start: 36m 10s 614ms, eta: 02h 19m 06s 465ms
[32m2022-10-11T00:59:32 | mmf.trainers.callbacks.logistics: [39mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0766, train/hateful_memes/cross_entropy/avg: 0.2541, train/total_loss: 0.0766, train/total_loss/avg: 0.2541, max mem: 5794.0, experiment: hmd-ft, epoch: 8, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 2.38, time: 42s 168ms, time_since_start: 36m 52s 783ms, eta: 02h 11m 16s 775ms
[32m2022-10-11T01:00:14 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T01:00:14 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T01:00:15 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T01:00:27 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T01:00:27 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0732, train/hateful_memes/cross_entropy/avg: 0.2488, train/total_loss: 0.0732, train/total_loss/avg: 0.2488, max mem: 5794.0, experiment: hmd-ft, epoch: 8, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.82, time: 55s 240ms, time_since_start: 37m 48s 023ms, eta: 02h 51m 01s 494ms
[32m2022-10-11T01:00:27 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T01:00:27 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T01:00:27 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T01:00:41 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T01:00:41 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T01:00:41 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T01:00:50 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T01:00:59 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T01:00:59 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.8046, val/total_loss: 1.8046, val/hateful_memes/accuracy: 0.6400, val/hateful_memes/binary_f1: 0.5263, val/hateful_memes/roc_auc: 0.7214, num_updates: 4000, epoch: 8, iterations: 4000, max_updates: 22000, val_time: 31s 614ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T01:01:54 | mmf.trainers.callbacks.logistics: [39mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0671, train/hateful_memes/cross_entropy/avg: 0.2428, train/total_loss: 0.0671, train/total_loss/avg: 0.2428, max mem: 5794.0, experiment: hmd-ft, epoch: 8, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 1.82, time: 55s 921ms, time_since_start: 39m 15s 561ms, eta: 02h 52m 10s 183ms
[32m2022-10-11T01:02:53 | mmf.trainers.callbacks.logistics: [39mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0617, train/hateful_memes/cross_entropy/avg: 0.2384, train/total_loss: 0.0617, train/total_loss/avg: 0.2384, max mem: 5794.0, experiment: hmd-ft, epoch: 8, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 1.72, time: 58s 973ms, time_since_start: 40m 14s 534ms, eta: 03h 33s 149ms
[32m2022-10-11T01:03:51 | mmf.trainers.callbacks.logistics: [39mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0561, train/hateful_memes/cross_entropy/avg: 0.2329, train/total_loss: 0.0561, train/total_loss/avg: 0.2329, max mem: 5794.0, experiment: hmd-ft, epoch: 9, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 1.75, time: 57s 423ms, time_since_start: 41m 11s 958ms, eta: 02h 54m 49s 285ms
[32m2022-10-11T01:04:44 | mmf.trainers.callbacks.logistics: [39mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0561, train/hateful_memes/cross_entropy/avg: 0.2283, train/total_loss: 0.0561, train/total_loss/avg: 0.2283, max mem: 5794.0, experiment: hmd-ft, epoch: 9, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 1.89, time: 53s 433ms, time_since_start: 42m 05s 391ms, eta: 02h 41m 45s 240ms
[32m2022-10-11T01:05:38 | mmf.trainers.callbacks.logistics: [39mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0467, train/hateful_memes/cross_entropy/avg: 0.2235, train/total_loss: 0.0467, train/total_loss/avg: 0.2235, max mem: 5794.0, experiment: hmd-ft, epoch: 9, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 1.89, time: 53s 996ms, time_since_start: 42m 59s 388ms, eta: 02h 42m 31s 768ms
[32m2022-10-11T01:06:33 | mmf.trainers.callbacks.logistics: [39mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0430, train/hateful_memes/cross_entropy/avg: 0.2188, train/total_loss: 0.0430, train/total_loss/avg: 0.2188, max mem: 5794.0, experiment: hmd-ft, epoch: 9, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 1.85, time: 54s 300ms, time_since_start: 43m 53s 689ms, eta: 02h 42m 30s 676ms
[32m2022-10-11T01:07:27 | mmf.trainers.callbacks.logistics: [39mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0326, train/hateful_memes/cross_entropy/avg: 0.2148, train/total_loss: 0.0326, train/total_loss/avg: 0.2148, max mem: 5794.0, experiment: hmd-ft, epoch: 9, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 1.85, time: 54s 646ms, time_since_start: 44m 48s 335ms, eta: 02h 42m 36s 401ms
[32m2022-10-11T01:08:19 | mmf.trainers.callbacks.logistics: [39mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0326, train/hateful_memes/cross_entropy/avg: 0.2103, train/total_loss: 0.0326, train/total_loss/avg: 0.2103, max mem: 5794.0, experiment: hmd-ft, epoch: 10, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 1.92, time: 52s 187ms, time_since_start: 45m 40s 522ms, eta: 02h 34m 23s 407ms
[32m2022-10-11T01:09:01 | mmf.trainers.callbacks.logistics: [39mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0326, train/hateful_memes/cross_entropy/avg: 0.2069, train/total_loss: 0.0326, train/total_loss/avg: 0.2069, max mem: 5794.0, experiment: hmd-ft, epoch: 10, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 2.44, time: 41s 773ms, time_since_start: 46m 22s 296ms, eta: 02h 02m 51s 916ms
[32m2022-10-11T01:09:43 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T01:09:43 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T01:09:45 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T01:09:56 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T01:09:57 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0286, train/hateful_memes/cross_entropy/avg: 0.2029, train/total_loss: 0.0286, train/total_loss/avg: 0.2029, max mem: 5794.0, experiment: hmd-ft, epoch: 10, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.82, time: 55s 639ms, time_since_start: 47m 17s 936ms, eta: 02h 42m 41s 438ms
[32m2022-10-11T01:09:57 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T01:09:57 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T01:09:57 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T01:10:08 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T01:10:08 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T01:10:08 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T01:10:17 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T01:10:25 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T01:10:25 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.8814, val/total_loss: 1.8814, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.5347, val/hateful_memes/roc_auc: 0.7152, num_updates: 5000, epoch: 10, iterations: 5000, max_updates: 22000, val_time: 28s 223ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T01:11:19 | mmf.trainers.callbacks.logistics: [39mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0187, train/hateful_memes/cross_entropy/avg: 0.1993, train/total_loss: 0.0187, train/total_loss/avg: 0.1993, max mem: 5794.0, experiment: hmd-ft, epoch: 10, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 1.89, time: 53s 502ms, time_since_start: 48m 39s 667ms, eta: 02h 35m 31s 184ms
[32m2022-10-11T01:12:15 | mmf.trainers.callbacks.logistics: [39mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0187, train/hateful_memes/cross_entropy/avg: 0.1999, train/total_loss: 0.0187, train/total_loss/avg: 0.1999, max mem: 5794.0, experiment: hmd-ft, epoch: 10, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 1.79, time: 56s 207ms, time_since_start: 49m 35s 874ms, eta: 02h 42m 24s 994ms
[32m2022-10-11T01:13:15 | mmf.trainers.callbacks.logistics: [39mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0187, train/hateful_memes/cross_entropy/avg: 0.1998, train/total_loss: 0.0187, train/total_loss/avg: 0.1998, max mem: 5794.0, experiment: hmd-ft, epoch: 10, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 1.67, time: 01m 450ms, time_since_start: 50m 36s 325ms, eta: 02h 53m 38s 226ms
[32m2022-10-11T01:14:06 | mmf.trainers.callbacks.logistics: [39mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.1963, train/total_loss: 0.0120, train/total_loss/avg: 0.1963, max mem: 5794.0, experiment: hmd-ft, epoch: 11, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 2.00, time: 50s 479ms, time_since_start: 51m 26s 804ms, eta: 02h 24m 07s 732ms
[32m2022-10-11T01:14:55 | mmf.trainers.callbacks.logistics: [39mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.1929, train/total_loss: 0.0089, train/total_loss/avg: 0.1929, max mem: 5794.0, experiment: hmd-ft, epoch: 11, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 2.04, time: 49s 113ms, time_since_start: 52m 15s 918ms, eta: 02h 19m 23s 095ms
[32m2022-10-11T01:15:43 | mmf.trainers.callbacks.logistics: [39mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.1894, train/total_loss: 0.0089, train/total_loss/avg: 0.1894, max mem: 5794.0, experiment: hmd-ft, epoch: 11, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 068ms, time_since_start: 53m 03s 986ms, eta: 02h 15m 35s 514ms
[32m2022-10-11T01:16:32 | mmf.trainers.callbacks.logistics: [39mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.1868, train/total_loss: 0.0089, train/total_loss/avg: 0.1868, max mem: 5794.0, experiment: hmd-ft, epoch: 11, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 2.04, time: 49s 264ms, time_since_start: 53m 53s 251ms, eta: 02h 18m 07s 062ms
[32m2022-10-11T01:17:20 | mmf.trainers.callbacks.logistics: [39mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.1843, train/total_loss: 0.0120, train/total_loss/avg: 0.1843, max mem: 5794.0, experiment: hmd-ft, epoch: 11, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 286ms, time_since_start: 54m 41s 537ms, eta: 02h 14m 32s 737ms
[32m2022-10-11T01:18:05 | mmf.trainers.callbacks.logistics: [39mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.1813, train/total_loss: 0.0120, train/total_loss/avg: 0.1813, max mem: 5794.0, experiment: hmd-ft, epoch: 12, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 2.27, time: 44s 936ms, time_since_start: 55m 26s 474ms, eta: 02h 04m 26s 273ms
[32m2022-10-11T01:18:47 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T01:18:47 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T01:18:49 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T01:19:03 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T01:19:04 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.1783, train/total_loss: 0.0089, train/total_loss/avg: 0.1783, max mem: 5794.0, experiment: hmd-ft, epoch: 12, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.72, time: 58s 302ms, time_since_start: 56m 24s 776ms, eta: 02h 40m 26s 851ms
[32m2022-10-11T01:19:04 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T01:19:04 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T01:19:04 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T01:19:21 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T01:19:21 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T01:19:21 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T01:19:32 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T01:19:40 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T01:19:40 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.3625, val/total_loss: 2.3625, val/hateful_memes/accuracy: 0.6540, val/hateful_memes/binary_f1: 0.5910, val/hateful_memes/roc_auc: 0.7162, num_updates: 6000, epoch: 12, iterations: 6000, max_updates: 22000, val_time: 36s 676ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T01:20:35 | mmf.trainers.callbacks.logistics: [39mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.1755, train/total_loss: 0.0089, train/total_loss/avg: 0.1755, max mem: 5794.0, experiment: hmd-ft, epoch: 12, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 1.85, time: 54s 369ms, time_since_start: 57m 55s 829ms, eta: 02h 28m 41s 465ms
[32m2022-10-11T01:21:32 | mmf.trainers.callbacks.logistics: [39mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.1729, train/total_loss: 0.0089, train/total_loss/avg: 0.1729, max mem: 5794.0, experiment: hmd-ft, epoch: 12, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 1.79, time: 56s 890ms, time_since_start: 58m 52s 719ms, eta: 02h 34m 36s 315ms
[32m2022-10-11T01:22:32 | mmf.trainers.callbacks.logistics: [39mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0097, train/hateful_memes/cross_entropy/avg: 0.1703, train/total_loss: 0.0097, train/total_loss/avg: 0.1703, max mem: 5794.0, experiment: hmd-ft, epoch: 12, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 1.67, time: 01m 786ms, time_since_start: 59m 53s 505ms, eta: 02h 44m 08s 811ms
[32m2022-10-11T01:23:33 | mmf.trainers.callbacks.logistics: [39mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0089, train/hateful_memes/cross_entropy/avg: 0.1677, train/total_loss: 0.0089, train/total_loss/avg: 0.1677, max mem: 5794.0, experiment: hmd-ft, epoch: 13, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 1.67, time: 01m 601ms, time_since_start: 01h 54s 107ms, eta: 02h 42m 36s 414ms
[32m2022-10-11T01:24:21 | mmf.trainers.callbacks.logistics: [39mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1651, train/total_loss: 0.0088, train/total_loss/avg: 0.1651, max mem: 5794.0, experiment: hmd-ft, epoch: 13, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 150ms, time_since_start: 01h 01m 42s 257ms, eta: 02h 08m 22s 107ms
[32m2022-10-11T01:25:09 | mmf.trainers.callbacks.logistics: [39mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1627, train/total_loss: 0.0088, train/total_loss/avg: 0.1627, max mem: 5794.0, experiment: hmd-ft, epoch: 13, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 083ms, time_since_start: 01h 02m 30s 341ms, eta: 02h 07m 21s 819ms
[32m2022-10-11T01:25:57 | mmf.trainers.callbacks.logistics: [39mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1623, train/total_loss: 0.0088, train/total_loss/avg: 0.1623, max mem: 5794.0, experiment: hmd-ft, epoch: 13, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 2.08, time: 48s 180ms, time_since_start: 01h 03m 18s 521ms, eta: 02h 06m 47s 480ms
[32m2022-10-11T01:26:45 | mmf.trainers.callbacks.logistics: [39mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1599, train/total_loss: 0.0088, train/total_loss/avg: 0.1599, max mem: 5794.0, experiment: hmd-ft, epoch: 13, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 436ms, time_since_start: 01h 04m 05s 958ms, eta: 02h 04m 01s 121ms
[32m2022-10-11T01:27:32 | mmf.trainers.callbacks.logistics: [39mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0078, train/hateful_memes/cross_entropy/avg: 0.1576, train/total_loss: 0.0078, train/total_loss/avg: 0.1576, max mem: 5794.0, experiment: hmd-ft, epoch: 13, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 2.13, time: 47s 390ms, time_since_start: 01h 04m 53s 348ms, eta: 02h 03m 04s 950ms
[32m2022-10-11T01:28:15 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T01:28:15 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T01:28:16 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T01:28:32 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T01:28:32 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1554, train/total_loss: 0.0065, train/total_loss/avg: 0.1554, max mem: 5794.0, experiment: hmd-ft, epoch: 14, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.69, time: 59s 380ms, time_since_start: 01h 05m 52s 729ms, eta: 02h 33m 12s 043ms
[32m2022-10-11T01:28:32 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T01:28:32 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T01:28:32 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T01:28:37 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T01:28:37 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T01:28:37 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, val/hateful_memes/cross_entropy: 1.9557, val/total_loss: 1.9557, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.5455, val/hateful_memes/roc_auc: 0.7204, num_updates: 7000, epoch: 14, iterations: 7000, max_updates: 22000, val_time: 05s 244ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T01:29:27 | mmf.trainers.callbacks.logistics: [39mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0065, train/hateful_memes/cross_entropy/avg: 0.1534, train/total_loss: 0.0065, train/total_loss/avg: 0.1534, max mem: 5794.0, experiment: hmd-ft, epoch: 14, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 2.00, time: 50s 629ms, time_since_start: 01h 06m 48s 608ms, eta: 02h 09m 45s 240ms
[32m2022-10-11T01:30:20 | mmf.trainers.callbacks.logistics: [39mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1513, train/total_loss: 0.0062, train/total_loss/avg: 0.1513, max mem: 5794.0, experiment: hmd-ft, epoch: 14, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 1.92, time: 52s 723ms, time_since_start: 01h 07m 41s 332ms, eta: 02h 14m 12s 750ms
[32m2022-10-11T01:31:16 | mmf.trainers.callbacks.logistics: [39mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1522, train/total_loss: 0.0062, train/total_loss/avg: 0.1522, max mem: 5794.0, experiment: hmd-ft, epoch: 14, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 1.79, time: 56s 074ms, time_since_start: 01h 08m 37s 406ms, eta: 02h 21m 46s 696ms
[32m2022-10-11T01:32:15 | mmf.trainers.callbacks.logistics: [39mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1502, train/total_loss: 0.0043, train/total_loss/avg: 0.1502, max mem: 5794.0, experiment: hmd-ft, epoch: 14, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 1.69, time: 59s 136ms, time_since_start: 01h 09m 36s 543ms, eta: 02h 28m 30s 290ms
[32m2022-10-11T01:33:08 | mmf.trainers.callbacks.logistics: [39mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1483, train/total_loss: 0.0043, train/total_loss/avg: 0.1483, max mem: 5794.0, experiment: hmd-ft, epoch: 15, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 1.92, time: 52s 130ms, time_since_start: 01h 10m 28s 674ms, eta: 02h 10m 849ms
[32m2022-10-11T01:33:52 | mmf.trainers.callbacks.logistics: [39mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1463, train/total_loss: 0.0043, train/total_loss/avg: 0.1463, max mem: 5794.0, experiment: hmd-ft, epoch: 15, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 2.27, time: 44s 487ms, time_since_start: 01h 11m 13s 161ms, eta: 01h 50m 11s 156ms
[32m2022-10-11T01:34:37 | mmf.trainers.callbacks.logistics: [39mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1448, train/total_loss: 0.0043, train/total_loss/avg: 0.1448, max mem: 5794.0, experiment: hmd-ft, epoch: 15, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 2.27, time: 44s 588ms, time_since_start: 01h 11m 57s 749ms, eta: 01h 49m 40s 172ms
[32m2022-10-11T01:35:21 | mmf.trainers.callbacks.logistics: [39mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1435, train/total_loss: 0.0043, train/total_loss/avg: 0.1435, max mem: 5794.0, experiment: hmd-ft, epoch: 15, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 2.27, time: 44s 591ms, time_since_start: 01h 12m 42s 341ms, eta: 01h 48m 54s 611ms
[32m2022-10-11T01:36:06 | mmf.trainers.callbacks.logistics: [39mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1417, train/total_loss: 0.0043, train/total_loss/avg: 0.1417, max mem: 5794.0, experiment: hmd-ft, epoch: 15, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 2.27, time: 44s 573ms, time_since_start: 01h 13m 26s 914ms, eta: 01h 48m 05s 918ms
[32m2022-10-11T01:36:50 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T01:36:50 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T01:36:51 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T01:37:07 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T01:37:07 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0047, train/hateful_memes/cross_entropy/avg: 0.1400, train/total_loss: 0.0047, train/total_loss/avg: 0.1400, max mem: 5794.0, experiment: hmd-ft, epoch: 16, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 622ms, time_since_start: 01h 14m 28s 537ms, eta: 02h 28m 23s 300ms
[32m2022-10-11T01:37:07 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T01:37:07 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T01:37:07 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T01:37:20 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T01:37:20 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T01:37:20 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, val/hateful_memes/cross_entropy: 1.9507, val/total_loss: 1.9507, val/hateful_memes/accuracy: 0.6560, val/hateful_memes/binary_f1: 0.5721, val/hateful_memes/roc_auc: 0.7294, num_updates: 8000, epoch: 16, iterations: 8000, max_updates: 22000, val_time: 12s 211ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T01:38:08 | mmf.trainers.callbacks.logistics: [39mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1383, train/total_loss: 0.0043, train/total_loss/avg: 0.1383, max mem: 5794.0, experiment: hmd-ft, epoch: 16, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 161ms, time_since_start: 01h 15m 28s 916ms, eta: 01h 55m 08s 732ms
[32m2022-10-11T01:38:57 | mmf.trainers.callbacks.logistics: [39mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1367, train/total_loss: 0.0043, train/total_loss/avg: 0.1367, max mem: 5794.0, experiment: hmd-ft, epoch: 16, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 2.04, time: 49s 318ms, time_since_start: 01h 16m 18s 235ms, eta: 01h 57m 03s 740ms
[32m2022-10-11T01:39:49 | mmf.trainers.callbacks.logistics: [39mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1352, train/total_loss: 0.0043, train/total_loss/avg: 0.1352, max mem: 5794.0, experiment: hmd-ft, epoch: 16, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 1.96, time: 51s 857ms, time_since_start: 01h 17m 10s 092ms, eta: 02h 02m 11s 820ms
[32m2022-10-11T01:40:44 | mmf.trainers.callbacks.logistics: [39mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0047, train/hateful_memes/cross_entropy/avg: 0.1337, train/total_loss: 0.0047, train/total_loss/avg: 0.1337, max mem: 5794.0, experiment: hmd-ft, epoch: 16, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 1.82, time: 55s 438ms, time_since_start: 01h 18m 05s 531ms, eta: 02h 09m 40s 898ms
[32m2022-10-11T01:41:43 | mmf.trainers.callbacks.logistics: [39mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1322, train/total_loss: 0.0062, train/total_loss/avg: 0.1322, max mem: 5794.0, experiment: hmd-ft, epoch: 16, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 1.72, time: 58s 424ms, time_since_start: 01h 19m 03s 955ms, eta: 02h 15m 39s 644ms
[32m2022-10-11T01:42:27 | mmf.trainers.callbacks.logistics: [39mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1307, train/total_loss: 0.0062, train/total_loss/avg: 0.1307, max mem: 5794.0, experiment: hmd-ft, epoch: 17, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 2.27, time: 44s 355ms, time_since_start: 01h 19m 48s 310ms, eta: 01h 42m 13s 833ms
[32m2022-10-11T01:43:10 | mmf.trainers.callbacks.logistics: [39mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1292, train/total_loss: 0.0062, train/total_loss/avg: 0.1292, max mem: 5794.0, experiment: hmd-ft, epoch: 17, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 2.33, time: 43s 130ms, time_since_start: 01h 20m 31s 441ms, eta: 01h 38m 39s 989ms
[32m2022-10-11T01:43:53 | mmf.trainers.callbacks.logistics: [39mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1278, train/total_loss: 0.0062, train/total_loss/avg: 0.1278, max mem: 5794.0, experiment: hmd-ft, epoch: 17, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 2.38, time: 42s 799ms, time_since_start: 01h 21m 14s 241ms, eta: 01h 37m 10s 348ms
[32m2022-10-11T01:44:36 | mmf.trainers.callbacks.logistics: [39mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1269, train/total_loss: 0.0062, train/total_loss/avg: 0.1269, max mem: 5794.0, experiment: hmd-ft, epoch: 17, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 2.33, time: 43s 029ms, time_since_start: 01h 21m 57s 270ms, eta: 01h 36m 57s 234ms
[32m2022-10-11T01:45:19 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T01:45:19 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T01:45:21 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T01:45:37 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T01:45:37 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.1256, train/total_loss: 0.0071, train/total_loss/avg: 0.1256, max mem: 5794.0, experiment: hmd-ft, epoch: 17, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 1.67, time: 01m 599ms, time_since_start: 01h 22m 57s 870ms, eta: 02h 15m 30s 036ms
[32m2022-10-11T01:45:37 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T01:45:37 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T01:45:37 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T01:45:42 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T01:45:42 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T01:45:42 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, val/hateful_memes/cross_entropy: 1.7925, val/total_loss: 1.7925, val/hateful_memes/accuracy: 0.6680, val/hateful_memes/binary_f1: 0.6157, val/hateful_memes/roc_auc: 0.7052, num_updates: 9000, epoch: 17, iterations: 9000, max_updates: 22000, val_time: 05s 574ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T01:46:35 | mmf.trainers.callbacks.logistics: [39mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1242, train/total_loss: 0.0062, train/total_loss/avg: 0.1242, max mem: 5794.0, experiment: hmd-ft, epoch: 18, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 1.92, time: 52s 553ms, time_since_start: 01h 23m 56s 004ms, eta: 01h 56m 36s 389ms
[32m2022-10-11T01:47:25 | mmf.trainers.callbacks.logistics: [39mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1229, train/total_loss: 0.0062, train/total_loss/avg: 0.1229, max mem: 5794.0, experiment: hmd-ft, epoch: 18, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 2.00, time: 50s 135ms, time_since_start: 01h 24m 46s 140ms, eta: 01h 50m 22s 717ms
[32m2022-10-11T01:48:16 | mmf.trainers.callbacks.logistics: [39mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0047, train/hateful_memes/cross_entropy/avg: 0.1216, train/total_loss: 0.0047, train/total_loss/avg: 0.1216, max mem: 5794.0, experiment: hmd-ft, epoch: 18, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 2.00, time: 50s 800ms, time_since_start: 01h 25m 36s 940ms, eta: 01h 50m 58s 129ms
[32m2022-10-11T01:49:10 | mmf.trainers.callbacks.logistics: [39mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1205, train/total_loss: 0.0062, train/total_loss/avg: 0.1205, max mem: 5794.0, experiment: hmd-ft, epoch: 18, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 1.85, time: 54s 081ms, time_since_start: 01h 26m 31s 022ms, eta: 01h 57m 12s 295ms
[32m2022-10-11T01:50:05 | mmf.trainers.callbacks.logistics: [39mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1192, train/total_loss: 0.0062, train/total_loss/avg: 0.1192, max mem: 5794.0, experiment: hmd-ft, epoch: 18, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 1.85, time: 54s 632ms, time_since_start: 01h 27m 25s 654ms, eta: 01h 57m 27s 551ms
[32m2022-10-11T01:50:55 | mmf.trainers.callbacks.logistics: [39mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1180, train/total_loss: 0.0062, train/total_loss/avg: 0.1180, max mem: 5794.0, experiment: hmd-ft, epoch: 19, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 2.00, time: 50s 952ms, time_since_start: 01h 28m 16s 606ms, eta: 01h 48m 40s 230ms
[32m2022-10-11T01:51:37 | mmf.trainers.callbacks.logistics: [39mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1171, train/total_loss: 0.0062, train/total_loss/avg: 0.1171, max mem: 5794.0, experiment: hmd-ft, epoch: 19, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 2.44, time: 41s 687ms, time_since_start: 01h 28m 58s 293ms, eta: 01h 28m 11s 672ms
[32m2022-10-11T01:52:19 | mmf.trainers.callbacks.logistics: [39mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1160, train/total_loss: 0.0025, train/total_loss/avg: 0.1160, max mem: 5794.0, experiment: hmd-ft, epoch: 19, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 2.44, time: 41s 891ms, time_since_start: 01h 29m 40s 185ms, eta: 01h 27m 54s 333ms
[32m2022-10-11T01:53:01 | mmf.trainers.callbacks.logistics: [39mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1148, train/total_loss: 0.0025, train/total_loss/avg: 0.1148, max mem: 5794.0, experiment: hmd-ft, epoch: 19, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 2.44, time: 41s 872ms, time_since_start: 01h 30m 22s 058ms, eta: 01h 27m 08s 711ms
[32m2022-10-11T01:53:43 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T01:53:43 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T01:53:44 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T01:53:56 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T01:53:56 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1137, train/total_loss: 0.0017, train/total_loss/avg: 0.1137, max mem: 5794.0, experiment: hmd-ft, epoch: 19, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 1.82, time: 55s 401ms, time_since_start: 01h 31m 17s 459ms, eta: 01h 54m 20s 870ms
[32m2022-10-11T01:53:56 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T01:53:56 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T01:53:56 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T01:54:14 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T01:54:14 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T01:54:14 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.1578, val/total_loss: 2.1578, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5899, val/hateful_memes/roc_auc: 0.7172, num_updates: 10000, epoch: 19, iterations: 10000, max_updates: 22000, val_time: 17s 351ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T01:55:07 | mmf.trainers.callbacks.logistics: [39mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1126, train/total_loss: 0.0025, train/total_loss/avg: 0.1126, max mem: 5794.0, experiment: hmd-ft, epoch: 19, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 1.89, time: 53s 040ms, time_since_start: 01h 32m 27s 853ms, eta: 01h 48m 33s 764ms
[32m2022-10-11T01:55:53 | mmf.trainers.callbacks.logistics: [39mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0025, train/hateful_memes/cross_entropy/avg: 0.1129, train/total_loss: 0.0025, train/total_loss/avg: 0.1129, max mem: 5794.0, experiment: hmd-ft, epoch: 20, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 2.17, time: 46s 135ms, time_since_start: 01h 33m 13s 989ms, eta: 01h 33m 38s 246ms
[32m2022-10-11T01:56:40 | mmf.trainers.callbacks.logistics: [39mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1119, train/total_loss: 0.0017, train/total_loss/avg: 0.1119, max mem: 5794.0, experiment: hmd-ft, epoch: 20, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 178ms, time_since_start: 01h 34m 01s 167ms, eta: 01h 34m 56s 492ms
[32m2022-10-11T01:57:28 | mmf.trainers.callbacks.logistics: [39mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1108, train/total_loss: 0.0015, train/total_loss/avg: 0.1108, max mem: 5794.0, experiment: hmd-ft, epoch: 20, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 196ms, time_since_start: 01h 34m 49s 364ms, eta: 01h 36m 09s 697ms
[32m2022-10-11T01:58:17 | mmf.trainers.callbacks.logistics: [39mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1097, train/total_loss: 0.0009, train/total_loss/avg: 0.1097, max mem: 5794.0, experiment: hmd-ft, epoch: 20, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 896ms, time_since_start: 01h 35m 38s 260ms, eta: 01h 36m 42s 991ms
[32m2022-10-11T01:59:06 | mmf.trainers.callbacks.logistics: [39mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1087, train/total_loss: 0.0008, train/total_loss/avg: 0.1087, max mem: 5794.0, experiment: hmd-ft, epoch: 20, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 973ms, time_since_start: 01h 36m 27s 234ms, eta: 01h 36m 01s 668ms
[32m2022-10-11T01:59:51 | mmf.trainers.callbacks.logistics: [39mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.1077, train/total_loss: 0.0008, train/total_loss/avg: 0.1077, max mem: 5794.0, experiment: hmd-ft, epoch: 21, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 2.27, time: 44s 647ms, time_since_start: 01h 37m 11s 881ms, eta: 01h 26m 46s 653ms
[32m2022-10-11T02:00:33 | mmf.trainers.callbacks.logistics: [39mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1067, train/total_loss: 0.0005, train/total_loss/avg: 0.1067, max mem: 5794.0, experiment: hmd-ft, epoch: 21, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 2.38, time: 42s 118ms, time_since_start: 01h 37m 54s 000ms, eta: 01h 21m 08s 264ms
[32m2022-10-11T02:01:15 | mmf.trainers.callbacks.logistics: [39mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1057, train/total_loss: 0.0004, train/total_loss/avg: 0.1057, max mem: 5794.0, experiment: hmd-ft, epoch: 21, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 2.38, time: 42s 074ms, time_since_start: 01h 38m 36s 075ms, eta: 01h 20m 19s 709ms
[32m2022-10-11T02:01:57 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T02:01:57 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T02:01:59 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T02:02:15 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T02:02:15 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1047, train/total_loss: 0.0003, train/total_loss/avg: 0.1047, max mem: 5794.0, experiment: hmd-ft, epoch: 21, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 1.67, time: 01m 091ms, time_since_start: 01h 39m 36s 166ms, eta: 01h 53m 41s 608ms
[32m2022-10-11T02:02:15 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T02:02:15 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T02:02:15 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T02:02:21 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T02:02:21 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T02:02:21 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.9526, val/total_loss: 2.9526, val/hateful_memes/accuracy: 0.6560, val/hateful_memes/binary_f1: 0.5590, val/hateful_memes/roc_auc: 0.7237, num_updates: 11000, epoch: 21, iterations: 11000, max_updates: 22000, val_time: 06s 028ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T02:03:12 | mmf.trainers.callbacks.logistics: [39mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1041, train/total_loss: 0.0004, train/total_loss/avg: 0.1041, max mem: 5794.0, experiment: hmd-ft, epoch: 21, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 2.00, time: 50s 879ms, time_since_start: 01h 40m 33s 080ms, eta: 01h 35m 23s 299ms
[32m2022-10-11T02:04:01 | mmf.trainers.callbacks.logistics: [39mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1032, train/total_loss: 0.0005, train/total_loss/avg: 0.1032, max mem: 5794.0, experiment: hmd-ft, epoch: 22, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 861ms, time_since_start: 01h 41m 21s 942ms, eta: 01h 30m 45s 946ms
[32m2022-10-11T02:04:47 | mmf.trainers.callbacks.logistics: [39mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1023, train/total_loss: 0.0007, train/total_loss/avg: 0.1023, max mem: 5794.0, experiment: hmd-ft, epoch: 22, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 2.17, time: 46s 437ms, time_since_start: 01h 42m 08s 379ms, eta: 01h 25m 27s 782ms
[32m2022-10-11T02:05:34 | mmf.trainers.callbacks.logistics: [39mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1014, train/total_loss: 0.0005, train/total_loss/avg: 0.1014, max mem: 5794.0, experiment: hmd-ft, epoch: 22, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 2.17, time: 46s 995ms, time_since_start: 01h 42m 55s 374ms, eta: 01h 25m 40s 935ms
[32m2022-10-11T02:06:22 | mmf.trainers.callbacks.logistics: [39mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1005, train/total_loss: 0.0005, train/total_loss/avg: 0.1005, max mem: 5794.0, experiment: hmd-ft, epoch: 22, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 264ms, time_since_start: 01h 43m 42s 639ms, eta: 01h 25m 21s 610ms
[32m2022-10-11T02:07:09 | mmf.trainers.callbacks.logistics: [39mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0997, train/total_loss: 0.0004, train/total_loss/avg: 0.0997, max mem: 5794.0, experiment: hmd-ft, epoch: 22, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 2.13, time: 47s 740ms, time_since_start: 01h 44m 30s 380ms, eta: 01h 25m 23s 932ms
[32m2022-10-11T02:07:59 | mmf.trainers.callbacks.logistics: [39mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0988, train/total_loss: 0.0004, train/total_loss/avg: 0.0988, max mem: 5794.0, experiment: hmd-ft, epoch: 22, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 2.04, time: 49s 412ms, time_since_start: 01h 45m 19s 793ms, eta: 01h 27m 32s 383ms
[32m2022-10-11T02:08:40 | mmf.trainers.callbacks.logistics: [39mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0980, train/total_loss: 0.0004, train/total_loss/avg: 0.0980, max mem: 5794.0, experiment: hmd-ft, epoch: 23, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 2.44, time: 41s 618ms, time_since_start: 01h 46m 01s 412ms, eta: 01h 13m 979ms
[32m2022-10-11T02:09:22 | mmf.trainers.callbacks.logistics: [39mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0972, train/total_loss: 0.0003, train/total_loss/avg: 0.0972, max mem: 5794.0, experiment: hmd-ft, epoch: 23, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 2.44, time: 41s 966ms, time_since_start: 01h 46m 43s 379ms, eta: 01h 12m 54s 275ms
[32m2022-10-11T02:10:04 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T02:10:04 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T02:10:06 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T02:10:20 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T02:10:20 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0964, train/total_loss: 0.0003, train/total_loss/avg: 0.0964, max mem: 5794.0, experiment: hmd-ft, epoch: 23, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 1.75, time: 57s 736ms, time_since_start: 01h 47m 41s 115ms, eta: 01h 39m 18s 442ms
[32m2022-10-11T02:10:20 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T02:10:20 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T02:10:20 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T02:10:38 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T02:10:38 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T02:10:38 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.9767, val/total_loss: 2.9767, val/hateful_memes/accuracy: 0.6220, val/hateful_memes/binary_f1: 0.4933, val/hateful_memes/roc_auc: 0.7072, num_updates: 12000, epoch: 23, iterations: 12000, max_updates: 22000, val_time: 18s 060ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T02:11:32 | mmf.trainers.callbacks.logistics: [39mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0956, train/total_loss: 0.0003, train/total_loss/avg: 0.0956, max mem: 5794.0, experiment: hmd-ft, epoch: 23, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 1.89, time: 53s 918ms, time_since_start: 01h 48m 53s 101ms, eta: 01h 31m 48s 708ms
[32m2022-10-11T02:12:26 | mmf.trainers.callbacks.logistics: [39mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0948, train/total_loss: 0.0003, train/total_loss/avg: 0.0948, max mem: 5794.0, experiment: hmd-ft, epoch: 23, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 1.89, time: 53s 765ms, time_since_start: 01h 49m 46s 866ms, eta: 01h 30m 37s 648ms
[32m2022-10-11T02:13:13 | mmf.trainers.callbacks.logistics: [39mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0941, train/total_loss: 0.0003, train/total_loss/avg: 0.0941, max mem: 5794.0, experiment: hmd-ft, epoch: 24, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 114ms, time_since_start: 01h 50m 33s 981ms, eta: 01h 18m 36s 387ms
[32m2022-10-11T02:13:58 | mmf.trainers.callbacks.logistics: [39mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0933, train/total_loss: 0.0003, train/total_loss/avg: 0.0933, max mem: 5794.0, experiment: hmd-ft, epoch: 24, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 2.27, time: 44s 922ms, time_since_start: 01h 51m 18s 904ms, eta: 01h 14m 10s 561ms
[32m2022-10-11T02:14:43 | mmf.trainers.callbacks.logistics: [39mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0926, train/total_loss: 0.0003, train/total_loss/avg: 0.0926, max mem: 5794.0, experiment: hmd-ft, epoch: 24, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 2.27, time: 44s 958ms, time_since_start: 01h 52m 03s 862ms, eta: 01h 13m 27s 701ms
[32m2022-10-11T02:15:28 | mmf.trainers.callbacks.logistics: [39mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0918, train/total_loss: 0.0002, train/total_loss/avg: 0.0918, max mem: 5794.0, experiment: hmd-ft, epoch: 24, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 2.22, time: 45s 401ms, time_since_start: 01h 52m 49s 263ms, eta: 01h 13m 24s 269ms
[32m2022-10-11T02:16:14 | mmf.trainers.callbacks.logistics: [39mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0911, train/total_loss: 0.0002, train/total_loss/avg: 0.0911, max mem: 5794.0, experiment: hmd-ft, epoch: 24, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 2.22, time: 45s 566ms, time_since_start: 01h 53m 34s 830ms, eta: 01h 12m 53s 317ms
[32m2022-10-11T02:16:58 | mmf.trainers.callbacks.logistics: [39mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0904, train/total_loss: 0.0003, train/total_loss/avg: 0.0904, max mem: 5794.0, experiment: hmd-ft, epoch: 25, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 2.27, time: 44s 533ms, time_since_start: 01h 54m 19s 363ms, eta: 01h 10m 28s 152ms
[32m2022-10-11T02:17:40 | mmf.trainers.callbacks.logistics: [39mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0897, train/total_loss: 0.0003, train/total_loss/avg: 0.0897, max mem: 5794.0, experiment: hmd-ft, epoch: 25, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 2.38, time: 42s 032ms, time_since_start: 01h 55m 01s 396ms, eta: 01h 05m 47s 377ms
[32m2022-10-11T02:18:22 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T02:18:22 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T02:18:24 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T02:18:42 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T02:18:42 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0890, train/total_loss: 0.0004, train/total_loss/avg: 0.0890, max mem: 5794.0, experiment: hmd-ft, epoch: 25, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 1.61, time: 01m 02s 176ms, time_since_start: 01h 56m 03s 572ms, eta: 01h 36m 14s 961ms
[32m2022-10-11T02:18:42 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T02:18:42 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T02:18:42 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T02:18:48 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T02:18:48 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T02:18:48 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.5119, val/total_loss: 2.5119, val/hateful_memes/accuracy: 0.6340, val/hateful_memes/binary_f1: 0.5120, val/hateful_memes/roc_auc: 0.7120, num_updates: 13000, epoch: 25, iterations: 13000, max_updates: 22000, val_time: 05s 604ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T02:19:39 | mmf.trainers.callbacks.logistics: [39mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0884, train/total_loss: 0.0004, train/total_loss/avg: 0.0884, max mem: 5794.0, experiment: hmd-ft, epoch: 25, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 2.00, time: 50s 680ms, time_since_start: 01h 56m 59s 867ms, eta: 01h 17m 34s 933ms
[32m2022-10-11T02:20:29 | mmf.trainers.callbacks.logistics: [39mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0877, train/total_loss: 0.0004, train/total_loss/avg: 0.0877, max mem: 5794.0, experiment: hmd-ft, epoch: 25, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 2.04, time: 49s 945ms, time_since_start: 01h 57m 49s 813ms, eta: 01h 15m 35s 847ms
[32m2022-10-11T02:21:20 | mmf.trainers.callbacks.logistics: [39mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0875, train/total_loss: 0.0004, train/total_loss/avg: 0.0875, max mem: 5794.0, experiment: hmd-ft, epoch: 25, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 2.00, time: 50s 863ms, time_since_start: 01h 58m 40s 676ms, eta: 01h 16m 06s 744ms
[32m2022-10-11T02:22:06 | mmf.trainers.callbacks.logistics: [39mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0869, train/total_loss: 0.0006, train/total_loss/avg: 0.0869, max mem: 5794.0, experiment: hmd-ft, epoch: 26, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 843ms, time_since_start: 01h 59m 27s 520ms, eta: 01h 09m 17s 444ms
[32m2022-10-11T02:22:54 | mmf.trainers.callbacks.logistics: [39mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0863, train/total_loss: 0.0006, train/total_loss/avg: 0.0863, max mem: 5794.0, experiment: hmd-ft, epoch: 26, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 249ms, time_since_start: 02h 14s 769ms, eta: 01h 09m 04s 711ms
[32m2022-10-11T02:23:41 | mmf.trainers.callbacks.logistics: [39mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0856, train/total_loss: 0.0007, train/total_loss/avg: 0.0856, max mem: 5794.0, experiment: hmd-ft, epoch: 26, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 952ms, time_since_start: 02h 01m 01s 722ms, eta: 01h 07m 50s 225ms
[32m2022-10-11T02:24:27 | mmf.trainers.callbacks.logistics: [39mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0850, train/total_loss: 0.0007, train/total_loss/avg: 0.0850, max mem: 5794.0, experiment: hmd-ft, epoch: 26, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 561ms, time_since_start: 02h 01m 48s 283ms, eta: 01h 06m 28s 287ms
[32m2022-10-11T02:25:15 | mmf.trainers.callbacks.logistics: [39mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0844, train/total_loss: 0.0006, train/total_loss/avg: 0.0844, max mem: 5794.0, experiment: hmd-ft, epoch: 26, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 2.13, time: 47s 467ms, time_since_start: 02h 02m 35s 751ms, eta: 01h 06m 56s 923ms
[32m2022-10-11T02:25:58 | mmf.trainers.callbacks.logistics: [39mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0838, train/total_loss: 0.0006, train/total_loss/avg: 0.0838, max mem: 5794.0, experiment: hmd-ft, epoch: 27, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 2.33, time: 43s 728ms, time_since_start: 02h 03m 19s 480ms, eta: 01h 55s 330ms
[32m2022-10-11T02:26:40 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T02:26:40 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T02:26:42 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T02:26:56 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T02:26:56 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0832, train/total_loss: 0.0006, train/total_loss/avg: 0.0832, max mem: 5794.0, experiment: hmd-ft, epoch: 27, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 1.75, time: 57s 252ms, time_since_start: 02h 04m 16s 732ms, eta: 01h 18m 46s 797ms
[32m2022-10-11T02:26:56 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T02:26:56 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T02:26:56 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T02:27:12 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T02:27:12 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T02:27:12 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.5892, val/total_loss: 2.5892, val/hateful_memes/accuracy: 0.6280, val/hateful_memes/binary_f1: 0.4973, val/hateful_memes/roc_auc: 0.7144, num_updates: 14000, epoch: 27, iterations: 14000, max_updates: 22000, val_time: 16s 795ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T02:28:03 | mmf.trainers.callbacks.logistics: [39mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0826, train/total_loss: 0.0006, train/total_loss/avg: 0.0826, max mem: 5794.0, experiment: hmd-ft, epoch: 27, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 2.00, time: 50s 187ms, time_since_start: 02h 05m 23s 721ms, eta: 01h 08m 11s 693ms
[32m2022-10-11T02:28:52 | mmf.trainers.callbacks.logistics: [39mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0820, train/total_loss: 0.0005, train/total_loss/avg: 0.0820, max mem: 5794.0, experiment: hmd-ft, epoch: 27, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 2.04, time: 49s 748ms, time_since_start: 02h 06m 13s 470ms, eta: 01h 06m 44s 594ms
[32m2022-10-11T02:29:43 | mmf.trainers.callbacks.logistics: [39mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0815, train/total_loss: 0.0006, train/total_loss/avg: 0.0815, max mem: 5794.0, experiment: hmd-ft, epoch: 27, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 2.00, time: 50s 491ms, time_since_start: 02h 07m 03s 962ms, eta: 01h 06m 52s 259ms
[32m2022-10-11T02:30:32 | mmf.trainers.callbacks.logistics: [39mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0809, train/total_loss: 0.0006, train/total_loss/avg: 0.0809, max mem: 5794.0, experiment: hmd-ft, epoch: 28, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 2.08, time: 48s 929ms, time_since_start: 02h 07m 52s 892ms, eta: 01h 03m 57s 655ms
[32m2022-10-11T02:31:17 | mmf.trainers.callbacks.logistics: [39mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0804, train/total_loss: 0.0007, train/total_loss/avg: 0.0804, max mem: 5794.0, experiment: hmd-ft, epoch: 28, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 2.22, time: 45s 109ms, time_since_start: 02h 08m 38s 001ms, eta: 58m 11s 473ms
[32m2022-10-11T02:32:02 | mmf.trainers.callbacks.logistics: [39mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0799, train/total_loss: 0.0007, train/total_loss/avg: 0.0799, max mem: 5794.0, experiment: hmd-ft, epoch: 28, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 2.27, time: 44s 828ms, time_since_start: 02h 09m 22s 830ms, eta: 57m 03s 496ms
[32m2022-10-11T02:32:47 | mmf.trainers.callbacks.logistics: [39mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0793, train/total_loss: 0.0007, train/total_loss/avg: 0.0793, max mem: 5794.0, experiment: hmd-ft, epoch: 28, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 2.22, time: 45s 210ms, time_since_start: 02h 10m 08s 040ms, eta: 56m 45s 959ms
[32m2022-10-11T02:33:32 | mmf.trainers.callbacks.logistics: [39mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0788, train/total_loss: 0.0006, train/total_loss/avg: 0.0788, max mem: 5794.0, experiment: hmd-ft, epoch: 28, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 2.22, time: 45s 353ms, time_since_start: 02h 10m 53s 394ms, eta: 56m 09s 973ms
[32m2022-10-11T02:34:17 | mmf.trainers.callbacks.logistics: [39mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0782, train/total_loss: 0.0006, train/total_loss/avg: 0.0782, max mem: 5794.0, experiment: hmd-ft, epoch: 29, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 2.27, time: 44s 520ms, time_since_start: 02h 11m 37s 915ms, eta: 54m 22s 099ms
[32m2022-10-11T02:34:58 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T02:34:58 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T02:35:01 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T02:35:17 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T02:35:17 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0777, train/total_loss: 0.0005, train/total_loss/avg: 0.0777, max mem: 5794.0, experiment: hmd-ft, epoch: 29, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 1.67, time: 01m 382ms, time_since_start: 02h 12m 38s 297ms, eta: 01h 12m 42s 042ms
[32m2022-10-11T02:35:17 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T02:35:17 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T02:35:17 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T02:35:23 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T02:35:23 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T02:35:23 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.3762, val/total_loss: 2.3762, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5586, val/hateful_memes/roc_auc: 0.7253, num_updates: 15000, epoch: 29, iterations: 15000, max_updates: 22000, val_time: 06s 072ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T02:36:14 | mmf.trainers.callbacks.logistics: [39mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0772, train/total_loss: 0.0004, train/total_loss/avg: 0.0772, max mem: 5794.0, experiment: hmd-ft, epoch: 29, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 2.00, time: 50s 767ms, time_since_start: 02h 13m 35s 143ms, eta: 01h 15s 036ms
[32m2022-10-11T02:37:04 | mmf.trainers.callbacks.logistics: [39mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0767, train/total_loss: 0.0004, train/total_loss/avg: 0.0767, max mem: 5794.0, experiment: hmd-ft, epoch: 29, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 2.00, time: 50s 301ms, time_since_start: 02h 14m 25s 444ms, eta: 58m 49s 976ms
[32m2022-10-11T02:37:55 | mmf.trainers.callbacks.logistics: [39mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0762, train/total_loss: 0.0004, train/total_loss/avg: 0.0762, max mem: 5794.0, experiment: hmd-ft, epoch: 29, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 2.00, time: 50s 314ms, time_since_start: 02h 15m 15s 759ms, eta: 57m 58s 966ms
[32m2022-10-11T02:38:47 | mmf.trainers.callbacks.logistics: [39mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0757, train/total_loss: 0.0003, train/total_loss/avg: 0.0757, max mem: 5794.0, experiment: hmd-ft, epoch: 29, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 1.92, time: 52s 271ms, time_since_start: 02h 16m 08s 031ms, eta: 59m 20s 309ms
[32m2022-10-11T02:39:33 | mmf.trainers.callbacks.logistics: [39mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0752, train/total_loss: 0.0003, train/total_loss/avg: 0.0752, max mem: 5794.0, experiment: hmd-ft, epoch: 30, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 2.17, time: 46s 255ms, time_since_start: 02h 16m 54s 286ms, eta: 51m 42s 838ms
[32m2022-10-11T02:40:17 | mmf.trainers.callbacks.logistics: [39mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0748, train/total_loss: 0.0003, train/total_loss/avg: 0.0748, max mem: 5794.0, experiment: hmd-ft, epoch: 30, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 2.27, time: 44s 321ms, time_since_start: 02h 17m 38s 608ms, eta: 48m 47s 366ms
[32m2022-10-11T02:41:02 | mmf.trainers.callbacks.logistics: [39mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0743, train/total_loss: 0.0003, train/total_loss/avg: 0.0743, max mem: 5794.0, experiment: hmd-ft, epoch: 30, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 2.27, time: 44s 620ms, time_since_start: 02h 18m 23s 228ms, eta: 48m 21s 015ms
[32m2022-10-11T02:41:46 | mmf.trainers.callbacks.logistics: [39mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0738, train/total_loss: 0.0003, train/total_loss/avg: 0.0738, max mem: 5794.0, experiment: hmd-ft, epoch: 30, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 2.27, time: 44s 025ms, time_since_start: 02h 19m 07s 254ms, eta: 46m 56s 934ms
[32m2022-10-11T02:42:30 | mmf.trainers.callbacks.logistics: [39mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0734, train/total_loss: 0.0003, train/total_loss/avg: 0.0734, max mem: 5794.0, experiment: hmd-ft, epoch: 30, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 2.27, time: 44s 242ms, time_since_start: 02h 19m 51s 497ms, eta: 46m 25s 163ms
[32m2022-10-11T02:43:14 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T02:43:14 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T02:43:15 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T02:43:32 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T02:43:32 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0729, train/total_loss: 0.0003, train/total_loss/avg: 0.0729, max mem: 5794.0, experiment: hmd-ft, epoch: 31, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 697ms, time_since_start: 02h 20m 53s 194ms, eta: 01h 03m 40s 330ms
[32m2022-10-11T02:43:32 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T02:43:32 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T02:43:32 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T02:43:44 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T02:43:44 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T02:43:44 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.8955, val/total_loss: 2.8955, val/hateful_memes/accuracy: 0.6340, val/hateful_memes/binary_f1: 0.5041, val/hateful_memes/roc_auc: 0.7167, num_updates: 16000, epoch: 31, iterations: 16000, max_updates: 22000, val_time: 12s 144ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T02:44:32 | mmf.trainers.callbacks.logistics: [39mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0724, train/total_loss: 0.0004, train/total_loss/avg: 0.0724, max mem: 5794.0, experiment: hmd-ft, epoch: 31, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 496ms, time_since_start: 02h 21m 52s 841ms, eta: 48m 11s 944ms
[32m2022-10-11T02:45:19 | mmf.trainers.callbacks.logistics: [39mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0720, train/total_loss: 0.0004, train/total_loss/avg: 0.0720, max mem: 5794.0, experiment: hmd-ft, epoch: 31, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 247ms, time_since_start: 02h 22m 40s 089ms, eta: 47m 08s 067ms
[32m2022-10-11T02:46:07 | mmf.trainers.callbacks.logistics: [39mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0715, train/total_loss: 0.0003, train/total_loss/avg: 0.0715, max mem: 5794.0, experiment: hmd-ft, epoch: 31, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 123ms, time_since_start: 02h 23m 28s 213ms, eta: 47m 10s 820ms
[32m2022-10-11T02:47:00 | mmf.trainers.callbacks.logistics: [39mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0711, train/total_loss: 0.0003, train/total_loss/avg: 0.0711, max mem: 5794.0, experiment: hmd-ft, epoch: 31, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 1.92, time: 52s 492ms, time_since_start: 02h 24m 20s 705ms, eta: 50m 33s 630ms
[32m2022-10-11T02:47:51 | mmf.trainers.callbacks.logistics: [39mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0707, train/total_loss: 0.0003, train/total_loss/avg: 0.0707, max mem: 5794.0, experiment: hmd-ft, epoch: 32, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 1.96, time: 51s 636ms, time_since_start: 02h 25m 12s 342ms, eta: 48m 50s 910ms
[32m2022-10-11T02:48:34 | mmf.trainers.callbacks.logistics: [39mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0703, train/total_loss: 0.0003, train/total_loss/avg: 0.0703, max mem: 5794.0, experiment: hmd-ft, epoch: 32, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 2.38, time: 42s 653ms, time_since_start: 02h 25m 54s 995ms, eta: 39m 36s 981ms
[32m2022-10-11T02:49:17 | mmf.trainers.callbacks.logistics: [39mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0698, train/total_loss: 0.0002, train/total_loss/avg: 0.0698, max mem: 5794.0, experiment: hmd-ft, epoch: 32, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 2.38, time: 42s 942ms, time_since_start: 02h 26m 37s 937ms, eta: 39m 08s 760ms
[32m2022-10-11T02:50:00 | mmf.trainers.callbacks.logistics: [39mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0694, train/total_loss: 0.0002, train/total_loss/avg: 0.0694, max mem: 5794.0, experiment: hmd-ft, epoch: 32, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 2.38, time: 42s 697ms, time_since_start: 02h 27m 20s 635ms, eta: 38m 11s 307ms
[32m2022-10-11T02:50:42 | mmf.trainers.callbacks.logistics: [39mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0690, train/total_loss: 0.0002, train/total_loss/avg: 0.0690, max mem: 5794.0, experiment: hmd-ft, epoch: 32, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 2.38, time: 42s 610ms, time_since_start: 02h 28m 03s 245ms, eta: 37m 22s 656ms
[32m2022-10-11T02:51:25 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T02:51:25 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T02:51:27 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T02:51:43 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T02:51:43 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0686, train/total_loss: 0.0002, train/total_loss/avg: 0.0686, max mem: 5794.0, experiment: hmd-ft, epoch: 32, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 1.67, time: 01m 667ms, time_since_start: 02h 29m 03s 912ms, eta: 52m 10s 431ms
[32m2022-10-11T02:51:43 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T02:51:43 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T02:51:43 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T02:51:49 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T02:51:49 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T02:51:49 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, val/hateful_memes/cross_entropy: 3.7369, val/total_loss: 3.7369, val/hateful_memes/accuracy: 0.6180, val/hateful_memes/binary_f1: 0.4366, val/hateful_memes/roc_auc: 0.6868, num_updates: 17000, epoch: 32, iterations: 17000, max_updates: 22000, val_time: 05s 983ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T02:52:39 | mmf.trainers.callbacks.logistics: [39mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0682, train/total_loss: 0.0002, train/total_loss/avg: 0.0682, max mem: 5794.0, experiment: hmd-ft, epoch: 33, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 2.00, time: 50s 359ms, time_since_start: 02h 30m 258ms, eta: 42m 26s 601ms
[32m2022-10-11T02:53:28 | mmf.trainers.callbacks.logistics: [39mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0678, train/total_loss: 0.0002, train/total_loss/avg: 0.0678, max mem: 5794.0, experiment: hmd-ft, epoch: 33, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 2.04, time: 49s 254ms, time_since_start: 02h 30m 49s 512ms, eta: 40m 39s 856ms
[32m2022-10-11T02:54:18 | mmf.trainers.callbacks.logistics: [39mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0674, train/total_loss: 0.0001, train/total_loss/avg: 0.0674, max mem: 5794.0, experiment: hmd-ft, epoch: 33, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 2.04, time: 49s 652ms, time_since_start: 02h 31m 39s 165ms, eta: 40m 08s 350ms
[32m2022-10-11T02:55:09 | mmf.trainers.callbacks.logistics: [39mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0670, train/total_loss: 0.0001, train/total_loss/avg: 0.0670, max mem: 5794.0, experiment: hmd-ft, epoch: 33, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 1.96, time: 51s 225ms, time_since_start: 02h 32m 30s 390ms, eta: 40m 31s 786ms
[32m2022-10-11T02:56:04 | mmf.trainers.callbacks.logistics: [39mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0666, train/total_loss: 0.0001, train/total_loss/avg: 0.0666, max mem: 5794.0, experiment: hmd-ft, epoch: 33, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 1.85, time: 54s 245ms, time_since_start: 02h 33m 24s 636ms, eta: 41m 59s 173ms
[32m2022-10-11T02:56:52 | mmf.trainers.callbacks.logistics: [39mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0662, train/total_loss: 0.0000, train/total_loss/avg: 0.0662, max mem: 5794.0, experiment: hmd-ft, epoch: 34, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 189ms, time_since_start: 02h 34m 12s 826ms, eta: 36m 28s 195ms
[32m2022-10-11T02:57:34 | mmf.trainers.callbacks.logistics: [39mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0659, train/total_loss: 0.0000, train/total_loss/avg: 0.0659, max mem: 5794.0, experiment: hmd-ft, epoch: 34, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 2.38, time: 42s 113ms, time_since_start: 02h 34m 54s 939ms, eta: 31m 08s 828ms
[32m2022-10-11T02:58:16 | mmf.trainers.callbacks.logistics: [39mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0655, train/total_loss: 0.0000, train/total_loss/avg: 0.0655, max mem: 5794.0, experiment: hmd-ft, epoch: 34, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 2.38, time: 42s 174ms, time_since_start: 02h 35m 37s 114ms, eta: 30m 28s 022ms
[32m2022-10-11T02:58:58 | mmf.trainers.callbacks.logistics: [39mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0651, train/total_loss: 0.0000, train/total_loss/avg: 0.0651, max mem: 5794.0, experiment: hmd-ft, epoch: 34, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 2.38, time: 42s 166ms, time_since_start: 02h 36m 19s 281ms, eta: 29m 44s 156ms
[32m2022-10-11T02:59:40 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T02:59:40 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T02:59:42 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T02:59:54 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T02:59:54 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0648, train/total_loss: 0.0000, train/total_loss/avg: 0.0648, max mem: 5794.0, experiment: hmd-ft, epoch: 34, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 1.79, time: 56s 106ms, time_since_start: 02h 37m 15s 388ms, eta: 38m 36s 091ms
[32m2022-10-11T02:59:54 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T02:59:54 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T02:59:54 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T03:00:10 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T03:00:10 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T03:00:10 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, val/hateful_memes/cross_entropy: 3.6850, val/total_loss: 3.6850, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5229, val/hateful_memes/roc_auc: 0.6949, num_updates: 18000, epoch: 34, iterations: 18000, max_updates: 22000, val_time: 15s 948ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T03:01:03 | mmf.trainers.callbacks.logistics: [39mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0644, train/total_loss: 0.0000, train/total_loss/avg: 0.0644, max mem: 5794.0, experiment: hmd-ft, epoch: 35, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 1.92, time: 52s 683ms, time_since_start: 02h 38m 24s 026ms, eta: 35m 20s 419ms
[32m2022-10-11T03:01:49 | mmf.trainers.callbacks.logistics: [39mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0641, train/total_loss: 0.0000, train/total_loss/avg: 0.0641, max mem: 5794.0, experiment: hmd-ft, epoch: 35, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 2.22, time: 45s 985ms, time_since_start: 02h 39m 10s 011ms, eta: 30m 03s 348ms
[32m2022-10-11T03:02:36 | mmf.trainers.callbacks.logistics: [39mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0637, train/total_loss: 0.0000, train/total_loss/avg: 0.0637, max mem: 5794.0, experiment: hmd-ft, epoch: 35, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 295ms, time_since_start: 02h 39m 57s 306ms, eta: 30m 05s 912ms
[32m2022-10-11T03:03:25 | mmf.trainers.callbacks.logistics: [39mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0634, train/total_loss: 0.0000, train/total_loss/avg: 0.0634, max mem: 5794.0, experiment: hmd-ft, epoch: 35, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 501ms, time_since_start: 02h 40m 45s 807ms, eta: 30m 01s 919ms
[32m2022-10-11T03:04:14 | mmf.trainers.callbacks.logistics: [39mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0630, train/total_loss: 0.0000, train/total_loss/avg: 0.0630, max mem: 5794.0, experiment: hmd-ft, epoch: 35, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 907ms, time_since_start: 02h 41m 34s 715ms, eta: 29m 26s 542ms
[32m2022-10-11T03:05:02 | mmf.trainers.callbacks.logistics: [39mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0627, train/total_loss: 0.0000, train/total_loss/avg: 0.0627, max mem: 5794.0, experiment: hmd-ft, epoch: 35, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 561ms, time_since_start: 02h 42m 23s 276ms, eta: 28m 23s 920ms
[32m2022-10-11T03:05:45 | mmf.trainers.callbacks.logistics: [39mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0623, train/total_loss: 0.0000, train/total_loss/avg: 0.0623, max mem: 5794.0, experiment: hmd-ft, epoch: 36, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 2.33, time: 43s 221ms, time_since_start: 02h 43m 06s 498ms, eta: 24m 31s 954ms
[32m2022-10-11T03:06:28 | mmf.trainers.callbacks.logistics: [39mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0620, train/total_loss: 0.0000, train/total_loss/avg: 0.0620, max mem: 5794.0, experiment: hmd-ft, epoch: 36, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 2.38, time: 42s 140ms, time_since_start: 02h 43m 48s 638ms, eta: 23m 11s 643ms
[32m2022-10-11T03:07:10 | mmf.trainers.callbacks.logistics: [39mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0617, train/total_loss: 0.0000, train/total_loss/avg: 0.0617, max mem: 5794.0, experiment: hmd-ft, epoch: 36, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 2.38, time: 42s 090ms, time_since_start: 02h 44m 30s 729ms, eta: 22m 26s 555ms
[32m2022-10-11T03:07:52 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T03:07:52 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T03:07:54 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T03:08:09 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T03:08:09 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0614, train/total_loss: 0.0000, train/total_loss/avg: 0.0614, max mem: 5794.0, experiment: hmd-ft, epoch: 36, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 1.69, time: 59s 740ms, time_since_start: 02h 45m 30s 469ms, eta: 30m 49s 555ms
[32m2022-10-11T03:08:09 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T03:08:09 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T03:08:09 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T03:08:16 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T03:08:16 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T03:08:16 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, val/hateful_memes/cross_entropy: 2.8818, val/total_loss: 2.8818, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5393, val/hateful_memes/roc_auc: 0.7165, num_updates: 19000, epoch: 36, iterations: 19000, max_updates: 22000, val_time: 06s 265ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T03:09:06 | mmf.trainers.callbacks.logistics: [39mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0610, train/total_loss: 0.0000, train/total_loss/avg: 0.0610, max mem: 5794.0, experiment: hmd-ft, epoch: 36, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 2.00, time: 50s 838ms, time_since_start: 02h 46m 27s 576ms, eta: 25m 21s 509ms
[32m2022-10-11T03:09:55 | mmf.trainers.callbacks.logistics: [39mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0607, train/total_loss: 0.0000, train/total_loss/avg: 0.0607, max mem: 5794.0, experiment: hmd-ft, epoch: 37, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 2.08, time: 48s 366ms, time_since_start: 02h 47m 15s 942ms, eta: 23m 17s 600ms
[32m2022-10-11T03:10:42 | mmf.trainers.callbacks.logistics: [39mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0604, train/total_loss: 0.0000, train/total_loss/avg: 0.0604, max mem: 5794.0, experiment: hmd-ft, epoch: 37, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 476ms, time_since_start: 02h 48m 03s 419ms, eta: 22m 02s 881ms
[32m2022-10-11T03:11:30 | mmf.trainers.callbacks.logistics: [39mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0601, train/total_loss: 0.0000, train/total_loss/avg: 0.0601, max mem: 5794.0, experiment: hmd-ft, epoch: 37, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 899ms, time_since_start: 02h 48m 51s 318ms, eta: 21m 25s 234ms
[32m2022-10-11T03:12:20 | mmf.trainers.callbacks.logistics: [39mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0598, train/total_loss: 0.0000, train/total_loss/avg: 0.0598, max mem: 5794.0, experiment: hmd-ft, epoch: 37, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 2.04, time: 49s 398ms, time_since_start: 02h 49m 40s 716ms, eta: 21m 14s 469ms
[32m2022-10-11T03:13:09 | mmf.trainers.callbacks.logistics: [39mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0595, train/total_loss: 0.0000, train/total_loss/avg: 0.0595, max mem: 5794.0, experiment: hmd-ft, epoch: 37, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 2.04, time: 49s 892ms, time_since_start: 02h 50m 30s 609ms, eta: 20m 35s 745ms
[32m2022-10-11T03:13:57 | mmf.trainers.callbacks.logistics: [39mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0592, train/total_loss: 0.0000, train/total_loss/avg: 0.0592, max mem: 5794.0, experiment: hmd-ft, epoch: 38, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 2.13, time: 47s 914ms, time_since_start: 02h 51m 18s 523ms, eta: 18m 57s 297ms
[32m2022-10-11T03:14:39 | mmf.trainers.callbacks.logistics: [39mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0589, train/total_loss: 0.0000, train/total_loss/avg: 0.0589, max mem: 5794.0, experiment: hmd-ft, epoch: 38, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 2.44, time: 41s 743ms, time_since_start: 02h 52m 267ms, eta: 15m 47s 745ms
[32m2022-10-11T03:15:21 | mmf.trainers.callbacks.logistics: [39mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0586, train/total_loss: 0.0000, train/total_loss/avg: 0.0586, max mem: 5794.0, experiment: hmd-ft, epoch: 38, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 2.38, time: 42s 055ms, time_since_start: 02h 52m 42s 323ms, eta: 15m 11s 432ms
[32m2022-10-11T03:16:03 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T03:16:03 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T03:16:05 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T03:16:21 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T03:16:21 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0583, train/total_loss: 0.0000, train/total_loss/avg: 0.0583, max mem: 5831.0, experiment: hmd-ft, epoch: 38, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 1.69, time: 59s 565ms, time_since_start: 02h 53m 41s 888ms, eta: 20m 29s 432ms
[32m2022-10-11T03:16:21 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T03:16:21 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T03:16:21 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T03:16:33 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T03:16:33 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T03:16:33 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.2991, val/total_loss: 3.2991, val/hateful_memes/accuracy: 0.6320, val/hateful_memes/binary_f1: 0.5000, val/hateful_memes/roc_auc: 0.7177, num_updates: 20000, epoch: 38, iterations: 20000, max_updates: 22000, val_time: 11s 776ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T03:17:23 | mmf.trainers.callbacks.logistics: [39mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0580, train/total_loss: 0.0000, train/total_loss/avg: 0.0580, max mem: 5831.0, experiment: hmd-ft, epoch: 38, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 2.00, time: 50s 447ms, time_since_start: 02h 54m 44s 118ms, eta: 16m 29s 167ms
[32m2022-10-11T03:18:13 | mmf.trainers.callbacks.logistics: [39mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0577, train/total_loss: 0.0000, train/total_loss/avg: 0.0577, max mem: 5831.0, experiment: hmd-ft, epoch: 38, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 2.00, time: 50s 283ms, time_since_start: 02h 55m 34s 401ms, eta: 15m 34s 068ms
[32m2022-10-11T03:18:59 | mmf.trainers.callbacks.logistics: [39mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0574, train/total_loss: 0.0000, train/total_loss/avg: 0.0574, max mem: 5831.0, experiment: hmd-ft, epoch: 39, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 2.22, time: 45s 891ms, time_since_start: 02h 56m 20s 293ms, eta: 13m 25s 114ms
[32m2022-10-11T03:19:46 | mmf.trainers.callbacks.logistics: [39mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0571, train/total_loss: 0.0000, train/total_loss/avg: 0.0571, max mem: 5831.0, experiment: hmd-ft, epoch: 39, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 493ms, time_since_start: 02h 57m 06s 786ms, eta: 12m 47s 694ms
[32m2022-10-11T03:20:32 | mmf.trainers.callbacks.logistics: [39mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0568, train/total_loss: 0.0000, train/total_loss/avg: 0.0568, max mem: 5831.0, experiment: hmd-ft, epoch: 39, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 198ms, time_since_start: 02h 57m 52s 984ms, eta: 11m 55s 155ms
[32m2022-10-11T03:21:18 | mmf.trainers.callbacks.logistics: [39mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0566, train/total_loss: 0.0000, train/total_loss/avg: 0.0566, max mem: 5831.0, experiment: hmd-ft, epoch: 39, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 428ms, time_since_start: 02h 58m 39s 413ms, eta: 11m 10s 799ms
[32m2022-10-11T03:22:05 | mmf.trainers.callbacks.logistics: [39mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0563, train/total_loss: 0.0000, train/total_loss/avg: 0.0563, max mem: 5831.0, experiment: hmd-ft, epoch: 39, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 302ms, time_since_start: 02h 59m 25s 716ms, eta: 10m 21s 201ms
[32m2022-10-11T03:22:49 | mmf.trainers.callbacks.logistics: [39mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0560, train/total_loss: 0.0000, train/total_loss/avg: 0.0560, max mem: 5831.0, experiment: hmd-ft, epoch: 40, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 2.33, time: 43s 930ms, time_since_start: 03h 09s 646ms, eta: 09m 04s 029ms
[32m2022-10-11T03:23:31 | mmf.trainers.callbacks.logistics: [39mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0558, train/total_loss: 0.0000, train/total_loss/avg: 0.0558, max mem: 5831.0, experiment: hmd-ft, epoch: 40, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 2.38, time: 42s 078ms, time_since_start: 03h 51s 725ms, eta: 07m 57s 680ms
[32m2022-10-11T03:24:13 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T03:24:13 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T03:24:14 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T03:24:29 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T03:24:29 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0555, train/total_loss: 0.0000, train/total_loss/avg: 0.0555, max mem: 5831.0, experiment: hmd-ft, epoch: 40, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 1.72, time: 58s 772ms, time_since_start: 03h 01m 50s 498ms, eta: 10m 06s 532ms
[32m2022-10-11T03:24:29 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T03:24:29 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T03:24:29 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T03:24:36 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T03:24:36 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T03:24:36 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.4209, val/total_loss: 3.4209, val/hateful_memes/accuracy: 0.6320, val/hateful_memes/binary_f1: 0.5106, val/hateful_memes/roc_auc: 0.7180, num_updates: 21000, epoch: 40, iterations: 21000, max_updates: 22000, val_time: 06s 356ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T03:25:27 | mmf.trainers.callbacks.logistics: [39mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0552, train/total_loss: 0.0000, train/total_loss/avg: 0.0552, max mem: 5831.0, experiment: hmd-ft, epoch: 40, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 2.00, time: 50s 847ms, time_since_start: 03h 02m 47s 708ms, eta: 07m 52s 270ms
[32m2022-10-11T03:26:17 | mmf.trainers.callbacks.logistics: [39mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0550, train/total_loss: 0.0000, train/total_loss/avg: 0.0550, max mem: 5831.0, experiment: hmd-ft, epoch: 40, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 2.00, time: 50s 103ms, time_since_start: 03h 03m 37s 811ms, eta: 06m 53s 654ms
[32m2022-10-11T03:27:05 | mmf.trainers.callbacks.logistics: [39mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0547, train/total_loss: 0.0000, train/total_loss/avg: 0.0547, max mem: 5831.0, experiment: hmd-ft, epoch: 41, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 2.08, time: 48s 636ms, time_since_start: 03h 04m 26s 448ms, eta: 05m 51s 353ms
[32m2022-10-11T03:27:50 | mmf.trainers.callbacks.logistics: [39mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0545, train/total_loss: 0.0000, train/total_loss/avg: 0.0545, max mem: 5831.0, experiment: hmd-ft, epoch: 41, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 2.22, time: 45s 142ms, time_since_start: 03h 05m 11s 590ms, eta: 04m 39s 520ms
[32m2022-10-11T03:28:36 | mmf.trainers.callbacks.logistics: [39mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0542, train/total_loss: 0.0000, train/total_loss/avg: 0.0542, max mem: 5831.0, experiment: hmd-ft, epoch: 41, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 2.22, time: 45s 369ms, time_since_start: 03h 05m 56s 960ms, eta: 03m 54s 106ms
[32m2022-10-11T03:29:22 | mmf.trainers.callbacks.logistics: [39mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0539, train/total_loss: 0.0000, train/total_loss/avg: 0.0539, max mem: 5831.0, experiment: hmd-ft, epoch: 41, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 2.17, time: 46s 441ms, time_since_start: 03h 06m 43s 401ms, eta: 03m 11s 711ms
[32m2022-10-11T03:30:10 | mmf.trainers.callbacks.logistics: [39mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0537, train/total_loss: 0.0000, train/total_loss/avg: 0.0537, max mem: 5831.0, experiment: hmd-ft, epoch: 41, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 317ms, time_since_start: 03h 07m 30s 719ms, eta: 02m 26s 496ms
[32m2022-10-11T03:30:57 | mmf.trainers.callbacks.logistics: [39mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0534, train/total_loss: 0.0000, train/total_loss/avg: 0.0534, max mem: 5831.0, experiment: hmd-ft, epoch: 41, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 2.13, time: 47s 669ms, time_since_start: 03h 08m 18s 389ms, eta: 01m 38s 390ms
[32m2022-10-11T03:31:39 | mmf.trainers.callbacks.logistics: [39mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0532, train/total_loss: 0.0000, train/total_loss/avg: 0.0532, max mem: 5831.0, experiment: hmd-ft, epoch: 42, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 2.44, time: 41s 831ms, time_since_start: 03h 09m 220ms, eta: 43s 169ms
[32m2022-10-11T03:32:21 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-11T03:32:21 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-11T03:32:22 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-11T03:32:34 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-11T03:32:34 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0530, train/total_loss: 0.0000, train/total_loss/avg: 0.0530, max mem: 5831.0, experiment: hmd-ft, epoch: 42, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 1.85, time: 54s 786ms, time_since_start: 03h 09m 55s 007ms, eta: 0ms
[32m2022-10-11T03:32:34 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-11T03:32:34 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T03:32:34 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-11T03:32:50 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T03:32:50 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T03:32:50 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, val/hateful_memes/cross_entropy: 3.3308, val/total_loss: 3.3308, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.5524, val/hateful_memes/roc_auc: 0.7207, num_updates: 22000, epoch: 42, iterations: 22000, max_updates: 22000, val_time: 16s 339ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.743899
[32m2022-10-11T03:32:50 | mmf.trainers.core.training_loop: [39mStepping into final validation check
[32m2022-10-11T03:32:50 | mmf.utils.checkpoint: [39mRestoring checkpoint
[32m2022-10-11T03:32:50 | mmf.utils.checkpoint: [39mLoading checkpoint
[32m2022-10-11T03:32:58 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-11T03:32:58 | mmf.utils.checkpoint: [39mCurrent num updates: 2000
[32m2022-10-11T03:32:58 | mmf.utils.checkpoint: [39mCurrent iteration: 2000
[32m2022-10-11T03:32:58 | mmf.utils.checkpoint: [39mCurrent epoch: 4
[32m2022-10-11T03:32:58 | mmf.utils.checkpoint: [39mSalvando o modelo final..
[32m2022-10-11T03:32:58 | mmf.trainers.mmf_trainer: [39mStarting inference on val set
[32m2022-10-11T03:32:58 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-11T03:32:58 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False


100% 32/32 [00:04<00:00,  7.39it/s]
[32m2022-10-11T03:33:03 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-11T03:33:03 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-11T03:33:03 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.9627, val/total_loss: 0.9627, val/hateful_memes/accuracy: 0.6740, val/hateful_memes/binary_f1: 0.6510, val/hateful_memes/roc_auc: 0.7439
[32m2022-10-11T03:33:03 | mmf.trainers.callbacks.logistics: [39mFinished run in 03h 10m 23s 737ms
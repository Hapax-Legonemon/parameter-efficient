
[32m2022-10-18T06:11:56 | mmf.utils.checkpoint: [39mLoading checkpoint
[31m[5mWARNING[39m[25m [32m2022-10-18T06:11:59 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-18T06:11:59 | mmf: [39mKey distributed is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-18T06:11:59 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-18T06:11:59 | mmf: [39mKey distributed is not present in registry, returning default value of None
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mPretrained model loaded
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCurrent num updates: 0
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCurrent iteration: 0
[32m2022-10-18T06:11:59 | mmf.utils.checkpoint: [39mCurrent epoch: 0
[32m2022-10-18T06:11:59 | mmf.trainers.mmf_trainer: [39m===== Model x=====
[32m2022-10-18T06:11:59 | mmf.trainers.mmf_trainer: [39mVisualBERT(
  (model): VisualBERTForClassification(
    (bert): VisualBERTBase(
      (embeddings): BertVisioLinguisticEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (token_type_embeddings_visual): Embedding(2, 768)
        (position_embeddings_visual): Embedding(512, 768)
        (projection): Linear(in_features=2048, out_features=768, bias=True)
      )
      (encoder): BertEncoderJit(
        (layer): ModuleList(
          (0): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (1): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (2): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (3): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (4): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (5): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (6): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (7): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (8): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (9): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (10): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (11): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Sequential(
      (0): BertPredictionHeadTransform(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (1): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-10-18T06:11:59 | mmf.utils.general: [39mTotal Parameters: 113455250. Trained Parameters: 2004626
[32m2022-10-18T06:11:59 | mmf.trainers.core.training_loop: [39mStarting training...
[31m[5mWARNING[39m[25m [32m2022-10-18T06:12:00 | py.warnings: [39m/home/smart/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[31m[5mWARNING[39m[25m [32m2022-10-18T06:12:00 | py.warnings: [39m/home/smart/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[32m2022-10-18T06:13:11 | mmf.trainers.callbacks.logistics: [39mprogress: 100/22000, train/mmh/cross_entropy: 0.6349, train/mmh/cross_entropy/avg: 0.6349, train/total_loss: 0.6349, train/total_loss/avg: 0.6349, max mem: 3135.0, experiment: mmh_a10, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.41, time: 01m 11s 369ms, time_since_start: 01m 24s 643ms, eta: 05h 42m 02s 132ms
[32m2022-10-18T06:14:25 | mmf.trainers.callbacks.logistics: [39mprogress: 200/22000, train/mmh/cross_entropy: 0.6349, train/mmh/cross_entropy/avg: 0.6660, train/total_loss: 0.6349, train/total_loss/avg: 0.6660, max mem: 3135.0, experiment: mmh_a10, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.35, time: 01m 14s 199ms, time_since_start: 02m 38s 843ms, eta: 05h 53m 58s 359ms
[32m2022-10-18T06:15:40 | mmf.trainers.callbacks.logistics: [39mprogress: 300/22000, train/mmh/cross_entropy: 0.6651, train/mmh/cross_entropy/avg: 0.6657, train/total_loss: 0.6651, train/total_loss/avg: 0.6657, max mem: 3135.0, experiment: mmh_a10, epoch: 1, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.35, time: 01m 14s 855ms, time_since_start: 03m 53s 698ms, eta: 05h 55m 27s 806ms
[32m2022-10-18T06:16:51 | mmf.trainers.callbacks.logistics: [39mprogress: 400/22000, train/mmh/cross_entropy: 0.6651, train/mmh/cross_entropy/avg: 0.6735, train/total_loss: 0.6651, train/total_loss/avg: 0.6735, max mem: 3135.0, experiment: mmh_a10, epoch: 1, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.41, time: 01m 11s 474ms, time_since_start: 05m 05s 173ms, eta: 05h 37m 50s 880ms
[32m2022-10-18T06:18:02 | mmf.trainers.callbacks.logistics: [39mprogress: 500/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6764, train/total_loss: 0.6883, train/total_loss/avg: 0.6764, max mem: 3135.0, experiment: mmh_a10, epoch: 1, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 697ms, time_since_start: 06m 15s 870ms, eta: 05h 32m 37s 588ms
[32m2022-10-18T06:19:13 | mmf.trainers.callbacks.logistics: [39mprogress: 600/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6855, train/total_loss: 0.6883, train/total_loss/avg: 0.6855, max mem: 3135.0, experiment: mmh_a10, epoch: 1, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 1.41, time: 01m 11s 092ms, time_since_start: 07m 26s 963ms, eta: 05h 32m 55s 673ms
[32m2022-10-18T06:20:24 | mmf.trainers.callbacks.logistics: [39mprogress: 700/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6764, train/total_loss: 0.6883, train/total_loss/avg: 0.6764, max mem: 3135.0, experiment: mmh_a10, epoch: 1, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 731ms, time_since_start: 08m 37s 694ms, eta: 05h 29m 41s 450ms
[32m2022-10-18T06:21:34 | mmf.trainers.callbacks.logistics: [39mprogress: 800/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6913, train/total_loss: 0.6883, train/total_loss/avg: 0.6913, max mem: 3135.0, experiment: mmh_a10, epoch: 1, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 674ms, time_since_start: 09m 48s 369ms, eta: 05h 27m 52s 596ms
[32m2022-10-18T06:22:45 | mmf.trainers.callbacks.logistics: [39mprogress: 900/22000, train/mmh/cross_entropy: 0.6968, train/mmh/cross_entropy/avg: 0.6970, train/total_loss: 0.6968, train/total_loss/avg: 0.6970, max mem: 3135.0, experiment: mmh_a10, epoch: 1, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 892ms, time_since_start: 10m 59s 261ms, eta: 05h 27m 20s 240ms
[32m2022-10-18T06:23:56 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T06:23:56 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T06:23:57 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T06:23:57 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T06:23:57 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6886, train/total_loss: 0.6883, train/total_loss/avg: 0.6886, max mem: 3135.0, experiment: mmh_a10, epoch: 1, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.39, time: 01m 12s 265ms, time_since_start: 12m 11s 526ms, eta: 05h 32m 05s 758ms
[32m2022-10-18T06:23:57 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T06:23:57 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T06:23:57 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T06:26:57 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T06:26:57 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T06:26:57 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T06:27:00 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T06:27:00 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T06:27:03 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T06:27:03 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, val/mmh/cross_entropy: 0.6241, val/total_loss: 0.6241, val/mmh/accuracy: 0.7408, val/mmh/binary_f1: 0.8497, val/mmh/roc_auc: 0.5922, num_updates: 1000, epoch: 1, iterations: 1000, max_updates: 22000, val_time: 03m 05s 641ms, best_update: 1000, best_iteration: 1000, best_val/mmh/roc_auc: 0.592158
[32m2022-10-18T06:28:18 | mmf.trainers.callbacks.logistics: [39mprogress: 1100/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6846, train/total_loss: 0.6883, train/total_loss/avg: 0.6846, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 041ms, time_since_start: 16m 32s 211ms, eta: 05h 43m 12s 660ms
[32m2022-10-18T06:29:29 | mmf.trainers.callbacks.logistics: [39mprogress: 1200/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6855, train/total_loss: 0.6883, train/total_loss/avg: 0.6855, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 1.41, time: 01m 11s 322ms, time_since_start: 17m 43s 534ms, eta: 05h 24m 38s 495ms
[32m2022-10-18T06:30:41 | mmf.trainers.callbacks.logistics: [39mprogress: 1300/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6845, train/total_loss: 0.6883, train/total_loss/avg: 0.6845, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 1.41, time: 01m 11s 174ms, time_since_start: 18m 54s 709ms, eta: 05h 22m 24s 669ms
[32m2022-10-18T06:31:52 | mmf.trainers.callbacks.logistics: [39mprogress: 1400/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6862, train/total_loss: 0.6883, train/total_loss/avg: 0.6862, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 1.41, time: 01m 11s 351ms, time_since_start: 20m 06s 060ms, eta: 05h 21m 39s 002ms
[32m2022-10-18T06:33:03 | mmf.trainers.callbacks.logistics: [39mprogress: 1500/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6825, train/total_loss: 0.6883, train/total_loss/avg: 0.6825, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 904ms, time_since_start: 21m 16s 965ms, eta: 05h 18m 05s 117ms
[32m2022-10-18T06:34:13 | mmf.trainers.callbacks.logistics: [39mprogress: 1600/22000, train/mmh/cross_entropy: 0.6726, train/mmh/cross_entropy/avg: 0.6801, train/total_loss: 0.6726, train/total_loss/avg: 0.6801, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 055ms, time_since_start: 22m 27s 020ms, eta: 05h 12m 44s 431ms
[32m2022-10-18T06:35:24 | mmf.trainers.callbacks.logistics: [39mprogress: 1700/22000, train/mmh/cross_entropy: 0.6726, train/mmh/cross_entropy/avg: 0.6773, train/total_loss: 0.6726, train/total_loss/avg: 0.6773, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 993ms, time_since_start: 23m 38s 014ms, eta: 05h 15m 22s 501ms
[32m2022-10-18T06:36:34 | mmf.trainers.callbacks.logistics: [39mprogress: 1800/22000, train/mmh/cross_entropy: 0.6651, train/mmh/cross_entropy/avg: 0.6693, train/total_loss: 0.6651, train/total_loss/avg: 0.6693, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 1.43, time: 01m 10s 468ms, time_since_start: 24m 48s 483ms, eta: 05h 11m 30s 208ms
[32m2022-10-18T06:37:46 | mmf.trainers.callbacks.logistics: [39mprogress: 1900/22000, train/mmh/cross_entropy: 0.6726, train/mmh/cross_entropy/avg: 0.6732, train/total_loss: 0.6726, train/total_loss/avg: 0.6732, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 144ms, time_since_start: 25m 59s 627ms, eta: 05h 12m 56s 032ms
[32m2022-10-18T06:38:57 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T06:38:57 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T06:38:57 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T06:39:00 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T06:39:00 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, train/mmh/cross_entropy: 0.6651, train/mmh/cross_entropy/avg: 0.6654, train/total_loss: 0.6651, train/total_loss/avg: 0.6654, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 866ms, time_since_start: 27m 14s 494ms, eta: 05h 27m 39s 895ms
[32m2022-10-18T06:39:00 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T06:39:00 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T06:39:00 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T06:42:01 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T06:42:01 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T06:42:01 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T06:42:04 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T06:42:07 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T06:42:10 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T06:42:10 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, val/mmh/cross_entropy: 0.5381, val/total_loss: 0.5381, val/mmh/accuracy: 0.7070, val/mmh/binary_f1: 0.8082, val/mmh/roc_auc: 0.6899, num_updates: 2000, epoch: 1, iterations: 2000, max_updates: 22000, val_time: 03m 09s 792ms, best_update: 2000, best_iteration: 2000, best_val/mmh/roc_auc: 0.689921
[32m2022-10-18T06:43:26 | mmf.trainers.callbacks.logistics: [39mprogress: 2100/22000, train/mmh/cross_entropy: 0.6678, train/mmh/cross_entropy/avg: 0.6655, train/total_loss: 0.6678, train/total_loss/avg: 0.6655, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 1.33, time: 01m 15s 415ms, time_since_start: 31m 39s 708ms, eta: 05h 28m 25s 102ms
[32m2022-10-18T06:44:37 | mmf.trainers.callbacks.logistics: [39mprogress: 2200/22000, train/mmh/cross_entropy: 0.6651, train/mmh/cross_entropy/avg: 0.6611, train/total_loss: 0.6651, train/total_loss/avg: 0.6611, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 537ms, time_since_start: 32m 51s 245ms, eta: 05h 09m 57s 810ms
[32m2022-10-18T06:45:49 | mmf.trainers.callbacks.logistics: [39mprogress: 2300/22000, train/mmh/cross_entropy: 0.6678, train/mmh/cross_entropy/avg: 0.6670, train/total_loss: 0.6678, train/total_loss/avg: 0.6670, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 963ms, time_since_start: 34m 03s 208ms, eta: 05h 10m 14s 088ms
[32m2022-10-18T06:47:00 | mmf.trainers.callbacks.logistics: [39mprogress: 2400/22000, train/mmh/cross_entropy: 0.6441, train/mmh/cross_entropy/avg: 0.6612, train/total_loss: 0.6441, train/total_loss/avg: 0.6612, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 045ms, time_since_start: 35m 14s 253ms, eta: 05h 04m 43s 367ms
[32m2022-10-18T06:48:11 | mmf.trainers.callbacks.logistics: [39mprogress: 2500/22000, train/mmh/cross_entropy: 0.6437, train/mmh/cross_entropy/avg: 0.6603, train/total_loss: 0.6437, train/total_loss/avg: 0.6603, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 229ms, time_since_start: 36m 25s 483ms, eta: 05h 03m 57s 224ms
[32m2022-10-18T06:49:22 | mmf.trainers.callbacks.logistics: [39mprogress: 2600/22000, train/mmh/cross_entropy: 0.6367, train/mmh/cross_entropy/avg: 0.6525, train/total_loss: 0.6367, train/total_loss/avg: 0.6525, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 1.43, time: 01m 10s 372ms, time_since_start: 37m 35s 855ms, eta: 04h 58m 45s 447ms
[32m2022-10-18T06:50:32 | mmf.trainers.callbacks.logistics: [39mprogress: 2700/22000, train/mmh/cross_entropy: 0.6367, train/mmh/cross_entropy/avg: 0.6509, train/total_loss: 0.6367, train/total_loss/avg: 0.6509, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 1.43, time: 01m 10s 402ms, time_since_start: 38m 46s 258ms, eta: 04h 57m 20s 683ms
[32m2022-10-18T06:51:43 | mmf.trainers.callbacks.logistics: [39mprogress: 2800/22000, train/mmh/cross_entropy: 0.6336, train/mmh/cross_entropy/avg: 0.6486, train/total_loss: 0.6336, train/total_loss/avg: 0.6486, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 1.43, time: 01m 10s 987ms, time_since_start: 39m 57s 245ms, eta: 04h 58m 15s 559ms
[32m2022-10-18T06:52:54 | mmf.trainers.callbacks.logistics: [39mprogress: 2900/22000, train/mmh/cross_entropy: 0.6336, train/mmh/cross_entropy/avg: 0.6482, train/total_loss: 0.6336, train/total_loss/avg: 0.6482, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 1.43, time: 01m 10s 583ms, time_since_start: 41m 07s 829ms, eta: 04h 55m 01s 144ms
[32m2022-10-18T06:54:04 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T06:54:04 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T06:54:05 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T06:54:08 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T06:54:08 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, train/mmh/cross_entropy: 0.6336, train/mmh/cross_entropy/avg: 0.6457, train/total_loss: 0.6336, train/total_loss/avg: 0.6457, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.37, time: 01m 13s 987ms, time_since_start: 42m 21s 816ms, eta: 05h 07m 37s 697ms
[32m2022-10-18T06:54:08 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T06:54:08 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T06:54:08 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T06:57:03 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T06:57:03 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T06:57:03 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T06:57:06 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T06:57:09 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T06:57:12 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T06:57:12 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, val/mmh/cross_entropy: 0.5295, val/total_loss: 0.5295, val/mmh/accuracy: 0.7438, val/mmh/binary_f1: 0.8531, val/mmh/roc_auc: 0.7014, num_updates: 3000, epoch: 1, iterations: 3000, max_updates: 22000, val_time: 03m 03s 939ms, best_update: 3000, best_iteration: 3000, best_val/mmh/roc_auc: 0.701371
[32m2022-10-18T06:58:27 | mmf.trainers.callbacks.logistics: [39mprogress: 3100/22000, train/mmh/cross_entropy: 0.6308, train/mmh/cross_entropy/avg: 0.6436, train/total_loss: 0.6308, train/total_loss/avg: 0.6436, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 942ms, time_since_start: 46m 40s 700ms, eta: 05h 09m 57s 583ms
[32m2022-10-18T06:59:39 | mmf.trainers.callbacks.logistics: [39mprogress: 3200/22000, train/mmh/cross_entropy: 0.6308, train/mmh/cross_entropy/avg: 0.6454, train/total_loss: 0.6308, train/total_loss/avg: 0.6454, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 1.39, time: 01m 12s 603ms, time_since_start: 47m 53s 304ms, eta: 04h 58m 41s 697ms
[32m2022-10-18T07:00:51 | mmf.trainers.callbacks.logistics: [39mprogress: 3300/22000, train/mmh/cross_entropy: 0.6308, train/mmh/cross_entropy/avg: 0.6460, train/total_loss: 0.6308, train/total_loss/avg: 0.6460, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 760ms, time_since_start: 49m 05s 064ms, eta: 04h 53m 39s 317ms
[32m2022-10-18T07:02:03 | mmf.trainers.callbacks.logistics: [39mprogress: 3400/22000, train/mmh/cross_entropy: 0.6093, train/mmh/cross_entropy/avg: 0.6433, train/total_loss: 0.6093, train/total_loss/avg: 0.6433, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 827ms, time_since_start: 50m 16s 892ms, eta: 04h 52m 21s 632ms
[32m2022-10-18T07:03:14 | mmf.trainers.callbacks.logistics: [39mprogress: 3500/22000, train/mmh/cross_entropy: 0.5910, train/mmh/cross_entropy/avg: 0.6418, train/total_loss: 0.5910, train/total_loss/avg: 0.6418, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 202ms, time_since_start: 51m 28s 094ms, eta: 04h 48m 15s 503ms
[32m2022-10-18T07:04:25 | mmf.trainers.callbacks.logistics: [39mprogress: 3600/22000, train/mmh/cross_entropy: 0.5910, train/mmh/cross_entropy/avg: 0.6449, train/total_loss: 0.5910, train/total_loss/avg: 0.6449, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 425ms, time_since_start: 52m 39s 520ms, eta: 04h 47m 35s 904ms
[32m2022-10-18T07:05:37 | mmf.trainers.callbacks.logistics: [39mprogress: 3700/22000, train/mmh/cross_entropy: 0.5889, train/mmh/cross_entropy/avg: 0.6434, train/total_loss: 0.5889, train/total_loss/avg: 0.6434, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 729ms, time_since_start: 53m 51s 250ms, eta: 04h 47m 15s 207ms
[32m2022-10-18T07:06:49 | mmf.trainers.callbacks.logistics: [39mprogress: 3800/22000, train/mmh/cross_entropy: 0.5889, train/mmh/cross_entropy/avg: 0.6396, train/total_loss: 0.5889, train/total_loss/avg: 0.6396, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 947ms, time_since_start: 55m 03s 198ms, eta: 04h 46m 33s 097ms
[32m2022-10-18T07:08:00 | mmf.trainers.callbacks.logistics: [39mprogress: 3900/22000, train/mmh/cross_entropy: 0.5889, train/mmh/cross_entropy/avg: 0.6394, train/total_loss: 0.5889, train/total_loss/avg: 0.6394, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 1.41, time: 01m 11s 188ms, time_since_start: 56m 14s 386ms, eta: 04h 41m 58s 123ms
[32m2022-10-18T07:09:11 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T07:09:11 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T07:09:12 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T07:09:15 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T07:09:15 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, train/mmh/cross_entropy: 0.5889, train/mmh/cross_entropy/avg: 0.6374, train/total_loss: 0.5889, train/total_loss/avg: 0.6374, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 638ms, time_since_start: 57m 29s 025ms, eta: 04h 54m 176ms
[32m2022-10-18T07:09:15 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T07:09:15 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T07:09:15 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T07:12:13 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T07:12:13 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T07:12:13 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T07:12:16 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T07:12:19 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T07:12:22 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T07:12:22 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, val/mmh/cross_entropy: 0.5315, val/total_loss: 0.5315, val/mmh/accuracy: 0.6708, val/mmh/binary_f1: 0.7571, val/mmh/roc_auc: 0.7109, num_updates: 4000, epoch: 1, iterations: 4000, max_updates: 22000, val_time: 03m 06s 869ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.710891
[32m2022-10-18T07:13:33 | mmf.trainers.callbacks.logistics: [39mprogress: 4100/22000, train/mmh/cross_entropy: 0.5889, train/mmh/cross_entropy/avg: 0.6373, train/total_loss: 0.5889, train/total_loss/avg: 0.6373, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 653ms, time_since_start: 01h 01m 47s 556ms, eta: 04h 40m 40s 586ms
[32m2022-10-18T07:14:45 | mmf.trainers.callbacks.logistics: [39mprogress: 4200/22000, train/mmh/cross_entropy: 0.5910, train/mmh/cross_entropy/avg: 0.6371, train/total_loss: 0.5910, train/total_loss/avg: 0.6371, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 172ms, time_since_start: 01h 02m 58s 728ms, eta: 04h 37m 13s 953ms
[32m2022-10-18T07:15:56 | mmf.trainers.callbacks.logistics: [39mprogress: 4300/22000, train/mmh/cross_entropy: 0.5910, train/mmh/cross_entropy/avg: 0.6370, train/total_loss: 0.5910, train/total_loss/avg: 0.6370, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 427ms, time_since_start: 01h 04m 10s 156ms, eta: 04h 36m 39s 867ms
[32m2022-10-18T07:17:07 | mmf.trainers.callbacks.logistics: [39mprogress: 4400/22000, train/mmh/cross_entropy: 0.6093, train/mmh/cross_entropy/avg: 0.6368, train/total_loss: 0.6093, train/total_loss/avg: 0.6368, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 805ms, time_since_start: 01h 05m 20s 961ms, eta: 04h 32m 42s 257ms
[32m2022-10-18T07:18:18 | mmf.trainers.callbacks.logistics: [39mprogress: 4500/22000, train/mmh/cross_entropy: 0.5910, train/mmh/cross_entropy/avg: 0.6357, train/total_loss: 0.5910, train/total_loss/avg: 0.6357, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 860ms, time_since_start: 01h 06m 31s 821ms, eta: 04h 31m 21s 903ms
[32m2022-10-18T07:19:28 | mmf.trainers.callbacks.logistics: [39mprogress: 4600/22000, train/mmh/cross_entropy: 0.6093, train/mmh/cross_entropy/avg: 0.6367, train/total_loss: 0.6093, train/total_loss/avg: 0.6367, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 562ms, time_since_start: 01h 07m 42s 383ms, eta: 04h 28m 40s 786ms
[32m2022-10-18T07:20:39 | mmf.trainers.callbacks.logistics: [39mprogress: 4700/22000, train/mmh/cross_entropy: 0.6267, train/mmh/cross_entropy/avg: 0.6371, train/total_loss: 0.6267, train/total_loss/avg: 0.6371, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 637ms, time_since_start: 01h 08m 53s 021ms, eta: 04h 27m 25s 158ms
[32m2022-10-18T07:21:49 | mmf.trainers.callbacks.logistics: [39mprogress: 4800/22000, train/mmh/cross_entropy: 0.6267, train/mmh/cross_entropy/avg: 0.6344, train/total_loss: 0.6267, train/total_loss/avg: 0.6344, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 304ms, time_since_start: 01h 10m 03s 325ms, eta: 04h 24m 37s 336ms
[32m2022-10-18T07:23:00 | mmf.trainers.callbacks.logistics: [39mprogress: 4900/22000, train/mmh/cross_entropy: 0.6267, train/mmh/cross_entropy/avg: 0.6347, train/total_loss: 0.6267, train/total_loss/avg: 0.6347, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 024ms, time_since_start: 01h 11m 14s 350ms, eta: 04h 25m 46s 728ms
[32m2022-10-18T07:24:12 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T07:24:12 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T07:24:12 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T07:24:15 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T07:24:15 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, train/mmh/cross_entropy: 0.6267, train/mmh/cross_entropy/avg: 0.6329, train/total_loss: 0.6267, train/total_loss/avg: 0.6329, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 812ms, time_since_start: 01h 12m 29s 163ms, eta: 04h 38m 18s 884ms
[32m2022-10-18T07:24:15 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T07:24:15 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T07:24:15 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T07:27:13 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T07:27:13 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T07:27:13 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T07:27:17 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T07:27:19 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T07:27:22 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T07:27:22 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, val/mmh/cross_entropy: 0.5626, val/total_loss: 0.5626, val/mmh/accuracy: 0.6542, val/mmh/binary_f1: 0.7309, val/mmh/roc_auc: 0.7189, num_updates: 5000, epoch: 1, iterations: 5000, max_updates: 22000, val_time: 03m 07s 294ms, best_update: 5000, best_iteration: 5000, best_val/mmh/roc_auc: 0.718934
[32m2022-10-18T07:28:34 | mmf.trainers.callbacks.logistics: [39mprogress: 5100/22000, train/mmh/cross_entropy: 0.6292, train/mmh/cross_entropy/avg: 0.6352, train/total_loss: 0.6292, train/total_loss/avg: 0.6352, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 251ms, time_since_start: 01h 16m 47s 715ms, eta: 04h 23m 30s 465ms
[32m2022-10-18T07:29:46 | mmf.trainers.callbacks.logistics: [39mprogress: 5200/22000, train/mmh/cross_entropy: 0.6267, train/mmh/cross_entropy/avg: 0.6350, train/total_loss: 0.6267, train/total_loss/avg: 0.6350, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 939ms, time_since_start: 01h 17m 59s 655ms, eta: 04h 24m 28s 794ms
[32m2022-10-18T07:30:59 | mmf.trainers.callbacks.logistics: [39mprogress: 5300/22000, train/mmh/cross_entropy: 0.6243, train/mmh/cross_entropy/avg: 0.6337, train/total_loss: 0.6243, train/total_loss/avg: 0.6337, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 1.37, time: 01m 13s 811ms, time_since_start: 01h 19m 13s 467ms, eta: 04h 29m 44s 690ms
[32m2022-10-18T07:32:13 | mmf.trainers.callbacks.logistics: [39mprogress: 5400/22000, train/mmh/cross_entropy: 0.6243, train/mmh/cross_entropy/avg: 0.6325, train/total_loss: 0.6243, train/total_loss/avg: 0.6325, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 1.37, time: 01m 13s 423ms, time_since_start: 01h 20m 26s 891ms, eta: 04h 26m 43s 331ms
[32m2022-10-18T07:33:24 | mmf.trainers.callbacks.logistics: [39mprogress: 5500/22000, train/mmh/cross_entropy: 0.6267, train/mmh/cross_entropy/avg: 0.6338, train/total_loss: 0.6267, train/total_loss/avg: 0.6338, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 074ms, time_since_start: 01h 21m 37s 965ms, eta: 04h 16m 37s 985ms
[32m2022-10-18T07:34:35 | mmf.trainers.callbacks.logistics: [39mprogress: 5600/22000, train/mmh/cross_entropy: 0.6243, train/mmh/cross_entropy/avg: 0.6328, train/total_loss: 0.6243, train/total_loss/avg: 0.6328, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 787ms, time_since_start: 01h 22m 48s 753ms, eta: 04h 14m 02s 893ms
[32m2022-10-18T07:35:46 | mmf.trainers.callbacks.logistics: [39mprogress: 5700/22000, train/mmh/cross_entropy: 0.6267, train/mmh/cross_entropy/avg: 0.6342, train/total_loss: 0.6267, train/total_loss/avg: 0.6342, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 334ms, time_since_start: 01h 24m 088ms, eta: 04h 14m 26s 950ms
[32m2022-10-18T07:36:57 | mmf.trainers.callbacks.logistics: [39mprogress: 5800/22000, train/mmh/cross_entropy: 0.6267, train/mmh/cross_entropy/avg: 0.6335, train/total_loss: 0.6267, train/total_loss/avg: 0.6335, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 827ms, time_since_start: 01h 25m 10s 916ms, eta: 04h 11m 05s 516ms
[32m2022-10-18T07:38:08 | mmf.trainers.callbacks.logistics: [39mprogress: 5900/22000, train/mmh/cross_entropy: 0.6243, train/mmh/cross_entropy/avg: 0.6322, train/total_loss: 0.6243, train/total_loss/avg: 0.6322, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 912ms, time_since_start: 01h 26m 21s 829ms, eta: 04h 09m 50s 486ms
[32m2022-10-18T07:39:19 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T07:39:19 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T07:39:20 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T07:39:23 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T07:39:23 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, train/mmh/cross_entropy: 0.6267, train/mmh/cross_entropy/avg: 0.6330, train/total_loss: 0.6267, train/total_loss/avg: 0.6330, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 943ms, time_since_start: 01h 27m 36s 772ms, eta: 04h 22m 24s 150ms
[32m2022-10-18T07:39:23 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T07:39:23 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T07:39:23 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T07:42:18 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T07:42:18 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T07:42:18 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T07:42:21 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T07:42:24 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T07:42:27 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T07:42:27 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, val/mmh/cross_entropy: 0.5584, val/total_loss: 0.5584, val/mmh/accuracy: 0.6628, val/mmh/binary_f1: 0.7416, val/mmh/roc_auc: 0.7191, num_updates: 6000, epoch: 1, iterations: 6000, max_updates: 22000, val_time: 03m 04s 004ms, best_update: 6000, best_iteration: 6000, best_val/mmh/roc_auc: 0.719058
[32m2022-10-18T07:43:39 | mmf.trainers.callbacks.logistics: [39mprogress: 6100/22000, train/mmh/cross_entropy: 0.6267, train/mmh/cross_entropy/avg: 0.6341, train/total_loss: 0.6267, train/total_loss/avg: 0.6341, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 1.39, time: 01m 12s 557ms, time_since_start: 01h 31m 53s 341ms, eta: 04h 12m 27s 559ms
[32m2022-10-18T07:44:51 | mmf.trainers.callbacks.logistics: [39mprogress: 6200/22000, train/mmh/cross_entropy: 0.6243, train/mmh/cross_entropy/avg: 0.6327, train/total_loss: 0.6243, train/total_loss/avg: 0.6327, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 890ms, time_since_start: 01h 33m 05s 231ms, eta: 04h 08m 34s 049ms
[32m2022-10-18T07:46:02 | mmf.trainers.callbacks.logistics: [39mprogress: 6300/22000, train/mmh/cross_entropy: 0.6054, train/mmh/cross_entropy/avg: 0.6323, train/total_loss: 0.6054, train/total_loss/avg: 0.6323, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 220ms, time_since_start: 01h 34m 16s 452ms, eta: 04h 04m 41s 548ms
[32m2022-10-18T07:47:13 | mmf.trainers.callbacks.logistics: [39mprogress: 6400/22000, train/mmh/cross_entropy: 0.5909, train/mmh/cross_entropy/avg: 0.6305, train/total_loss: 0.5909, train/total_loss/avg: 0.6305, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 161ms, time_since_start: 01h 35m 27s 614ms, eta: 04h 02m 55s 957ms
[32m2022-10-18T07:48:25 | mmf.trainers.callbacks.logistics: [39mprogress: 6500/22000, train/mmh/cross_entropy: 0.6054, train/mmh/cross_entropy/avg: 0.6309, train/total_loss: 0.6054, train/total_loss/avg: 0.6309, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 618ms, time_since_start: 01h 36m 39s 233ms, eta: 04h 02m 55s 474ms
[32m2022-10-18T07:49:36 | mmf.trainers.callbacks.logistics: [39mprogress: 6600/22000, train/mmh/cross_entropy: 0.6054, train/mmh/cross_entropy/avg: 0.6320, train/total_loss: 0.6054, train/total_loss/avg: 0.6320, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 870ms, time_since_start: 01h 37m 50s 104ms, eta: 03h 58m 50s 174ms
[32m2022-10-18T07:50:47 | mmf.trainers.callbacks.logistics: [39mprogress: 6700/22000, train/mmh/cross_entropy: 0.5909, train/mmh/cross_entropy/avg: 0.6311, train/total_loss: 0.5909, train/total_loss/avg: 0.6311, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 768ms, time_since_start: 01h 39m 872ms, eta: 03h 56m 56s 592ms
[32m2022-10-18T07:51:58 | mmf.trainers.callbacks.logistics: [39mprogress: 6800/22000, train/mmh/cross_entropy: 0.6054, train/mmh/cross_entropy/avg: 0.6323, train/total_loss: 0.6054, train/total_loss/avg: 0.6323, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 005ms, time_since_start: 01h 40m 11s 877ms, eta: 03h 56m 10s 920ms
[32m2022-10-18T07:53:08 | mmf.trainers.callbacks.logistics: [39mprogress: 6900/22000, train/mmh/cross_entropy: 0.6054, train/mmh/cross_entropy/avg: 0.6326, train/total_loss: 0.6054, train/total_loss/avg: 0.6326, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 525ms, time_since_start: 01h 41m 22s 402ms, eta: 03h 53m 02s 561ms
[32m2022-10-18T07:54:19 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T07:54:19 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T07:54:20 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T07:54:23 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T07:54:23 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, train/mmh/cross_entropy: 0.6054, train/mmh/cross_entropy/avg: 0.6319, train/total_loss: 0.6054, train/total_loss/avg: 0.6319, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 703ms, time_since_start: 01h 42m 37s 106ms, eta: 04h 05m 12s 950ms
[32m2022-10-18T07:54:23 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T07:54:23 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T07:54:23 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T07:57:23 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T07:57:23 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T07:57:23 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T07:57:26 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T07:57:29 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T07:57:32 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T07:57:32 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, val/mmh/cross_entropy: 0.5323, val/total_loss: 0.5323, val/mmh/accuracy: 0.7374, val/mmh/binary_f1: 0.8379, val/mmh/roc_auc: 0.7236, num_updates: 7000, epoch: 1, iterations: 7000, max_updates: 22000, val_time: 03m 08s 701ms, best_update: 7000, best_iteration: 7000, best_val/mmh/roc_auc: 0.723587
[32m2022-10-18T07:58:43 | mmf.trainers.callbacks.logistics: [39mprogress: 7100/22000, train/mmh/cross_entropy: 0.6054, train/mmh/cross_entropy/avg: 0.6320, train/total_loss: 0.6054, train/total_loss/avg: 0.6320, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 243ms, time_since_start: 01h 46m 57s 058ms, eta: 03h 52m 17s 892ms
[32m2022-10-18T07:59:54 | mmf.trainers.callbacks.logistics: [39mprogress: 7200/22000, train/mmh/cross_entropy: 0.5909, train/mmh/cross_entropy/avg: 0.6312, train/total_loss: 0.5909, train/total_loss/avg: 0.6312, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 123ms, time_since_start: 01h 48m 08s 181ms, eta: 03h 50m 20s 933ms
[32m2022-10-18T08:01:05 | mmf.trainers.callbacks.logistics: [39mprogress: 7300/22000, train/mmh/cross_entropy: 0.5909, train/mmh/cross_entropy/avg: 0.6301, train/total_loss: 0.5909, train/total_loss/avg: 0.6301, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 132ms, time_since_start: 01h 49m 19s 314ms, eta: 03h 48m 49s 405ms
[32m2022-10-18T08:02:16 | mmf.trainers.callbacks.logistics: [39mprogress: 7400/22000, train/mmh/cross_entropy: 0.6054, train/mmh/cross_entropy/avg: 0.6322, train/total_loss: 0.6054, train/total_loss/avg: 0.6322, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 963ms, time_since_start: 01h 50m 30s 277ms, eta: 03h 46m 43s 576ms
[32m2022-10-18T08:03:28 | mmf.trainers.callbacks.logistics: [39mprogress: 7500/22000, train/mmh/cross_entropy: 0.6054, train/mmh/cross_entropy/avg: 0.6337, train/total_loss: 0.6054, train/total_loss/avg: 0.6337, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 350ms, time_since_start: 01h 51m 41s 628ms, eta: 03h 46m 24s 079ms
[32m2022-10-18T08:04:38 | mmf.trainers.callbacks.logistics: [39mprogress: 7600/22000, train/mmh/cross_entropy: 0.6380, train/mmh/cross_entropy/avg: 0.6355, train/total_loss: 0.6380, train/total_loss/avg: 0.6355, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 684ms, time_since_start: 01h 52m 52s 312ms, eta: 03h 42m 44s 501ms
[32m2022-10-18T08:05:49 | mmf.trainers.callbacks.logistics: [39mprogress: 7700/22000, train/mmh/cross_entropy: 0.6380, train/mmh/cross_entropy/avg: 0.6362, train/total_loss: 0.6380, train/total_loss/avg: 0.6362, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 077ms, time_since_start: 01h 54m 03s 390ms, eta: 03h 42m 25s 487ms
[32m2022-10-18T08:07:00 | mmf.trainers.callbacks.logistics: [39mprogress: 7800/22000, train/mmh/cross_entropy: 0.6380, train/mmh/cross_entropy/avg: 0.6362, train/total_loss: 0.6380, train/total_loss/avg: 0.6362, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 1.43, time: 01m 10s 672ms, time_since_start: 01h 55m 14s 062ms, eta: 03h 39m 36s 558ms
[32m2022-10-18T08:08:11 | mmf.trainers.callbacks.logistics: [39mprogress: 7900/22000, train/mmh/cross_entropy: 0.6380, train/mmh/cross_entropy/avg: 0.6359, train/total_loss: 0.6380, train/total_loss/avg: 0.6359, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 1.41, time: 01m 11s 020ms, time_since_start: 01h 56m 25s 083ms, eta: 03h 39m 08s 263ms
[32m2022-10-18T08:09:22 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T08:09:22 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T08:09:22 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T08:09:25 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T08:09:25 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, train/mmh/cross_entropy: 0.6380, train/mmh/cross_entropy/avg: 0.6364, train/total_loss: 0.6380, train/total_loss/avg: 0.6364, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.35, time: 01m 14s 048ms, time_since_start: 01h 57m 39s 132ms, eta: 03h 46m 51s 672ms
[32m2022-10-18T08:09:25 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T08:09:25 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T08:09:25 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T08:12:26 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T08:12:26 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T08:12:26 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T08:12:30 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T08:12:33 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T08:12:33 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, val/mmh/cross_entropy: 0.5671, val/total_loss: 0.5671, val/mmh/accuracy: 0.6512, val/mmh/binary_f1: 0.7248, val/mmh/roc_auc: 0.7230, num_updates: 8000, epoch: 1, iterations: 8000, max_updates: 22000, val_time: 03m 08s 397ms, best_update: 7000, best_iteration: 7000, best_val/mmh/roc_auc: 0.723587
[32m2022-10-18T08:13:44 | mmf.trainers.callbacks.logistics: [39mprogress: 8100/22000, train/mmh/cross_entropy: 0.6319, train/mmh/cross_entropy/avg: 0.6359, train/total_loss: 0.6319, train/total_loss/avg: 0.6359, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 976ms, time_since_start: 02h 01m 58s 509ms, eta: 03h 35m 53s 669ms
[32m2022-10-18T08:14:56 | mmf.trainers.callbacks.logistics: [39mprogress: 8200/22000, train/mmh/cross_entropy: 0.6319, train/mmh/cross_entropy/avg: 0.6355, train/total_loss: 0.6319, train/total_loss/avg: 0.6355, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 1.41, time: 01m 11s 221ms, time_since_start: 02h 03m 09s 730ms, eta: 03h 35m 04s 889ms
[32m2022-10-18T08:16:06 | mmf.trainers.callbacks.logistics: [39mprogress: 8300/22000, train/mmh/cross_entropy: 0.6319, train/mmh/cross_entropy/avg: 0.6338, train/total_loss: 0.6319, train/total_loss/avg: 0.6338, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 546ms, time_since_start: 02h 04m 20s 277ms, eta: 03h 31m 30s 044ms
[32m2022-10-18T08:17:17 | mmf.trainers.callbacks.logistics: [39mprogress: 8400/22000, train/mmh/cross_entropy: 0.6380, train/mmh/cross_entropy/avg: 0.6347, train/total_loss: 0.6380, train/total_loss/avg: 0.6347, max mem: 3161.0, experiment: mmh_a10, epoch: 1, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 665ms, time_since_start: 02h 05m 30s 943ms, eta: 03h 30m 18s 584ms
[32m2022-10-18T08:18:25 | mmf.trainers.callbacks.logistics: [39mprogress: 8500/22000, train/mmh/cross_entropy: 0.6319, train/mmh/cross_entropy/avg: 0.6344, train/total_loss: 0.6319, train/total_loss/avg: 0.6344, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 1.47, time: 01m 08s 443ms, time_since_start: 02h 06m 39s 386ms, eta: 03h 22m 11s 902ms
[32m2022-10-18T08:19:34 | mmf.trainers.callbacks.logistics: [39mprogress: 8600/22000, train/mmh/cross_entropy: 0.6144, train/mmh/cross_entropy/avg: 0.6326, train/total_loss: 0.6144, train/total_loss/avg: 0.6326, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 1.47, time: 01m 08s 655ms, time_since_start: 02h 07m 48s 041ms, eta: 03h 21m 19s 392ms
[32m2022-10-18T08:20:43 | mmf.trainers.callbacks.logistics: [39mprogress: 8700/22000, train/mmh/cross_entropy: 0.6319, train/mmh/cross_entropy/avg: 0.6342, train/total_loss: 0.6319, train/total_loss/avg: 0.6342, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 1.45, time: 01m 09s 340ms, time_since_start: 02h 08m 57s 382ms, eta: 03h 21m 48s 938ms
[32m2022-10-18T08:21:53 | mmf.trainers.callbacks.logistics: [39mprogress: 8800/22000, train/mmh/cross_entropy: 0.6144, train/mmh/cross_entropy/avg: 0.6331, train/total_loss: 0.6144, train/total_loss/avg: 0.6331, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 190ms, time_since_start: 02h 10m 07s 573ms, eta: 03h 22m 45s 201ms
[32m2022-10-18T08:23:04 | mmf.trainers.callbacks.logistics: [39mprogress: 8900/22000, train/mmh/cross_entropy: 0.6144, train/mmh/cross_entropy/avg: 0.6330, train/total_loss: 0.6144, train/total_loss/avg: 0.6330, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 299ms, time_since_start: 02h 11m 17s 873ms, eta: 03h 21m 31s 725ms
[32m2022-10-18T08:24:15 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T08:24:15 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T08:24:15 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T08:24:19 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T08:24:19 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, train/mmh/cross_entropy: 0.6159, train/mmh/cross_entropy/avg: 0.6328, train/total_loss: 0.6159, train/total_loss/avg: 0.6328, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 268ms, time_since_start: 02h 12m 33s 141ms, eta: 03h 34m 07s 580ms
[32m2022-10-18T08:24:19 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T08:24:19 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T08:24:19 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T08:27:19 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T08:27:19 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T08:27:19 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T08:27:23 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T08:27:26 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T08:27:30 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T08:27:30 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, val/mmh/cross_entropy: 0.5403, val/total_loss: 0.5403, val/mmh/accuracy: 0.6950, val/mmh/binary_f1: 0.7845, val/mmh/roc_auc: 0.7238, num_updates: 9000, epoch: 2, iterations: 9000, max_updates: 22000, val_time: 03m 11s 330ms, best_update: 9000, best_iteration: 9000, best_val/mmh/roc_auc: 0.723753
[32m2022-10-18T08:28:46 | mmf.trainers.callbacks.logistics: [39mprogress: 9100/22000, train/mmh/cross_entropy: 0.6159, train/mmh/cross_entropy/avg: 0.6331, train/total_loss: 0.6159, train/total_loss/avg: 0.6331, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 980ms, time_since_start: 02h 17m 454ms, eta: 03h 34m 29s 377ms
[32m2022-10-18T08:30:00 | mmf.trainers.callbacks.logistics: [39mprogress: 9200/22000, train/mmh/cross_entropy: 0.6159, train/mmh/cross_entropy/avg: 0.6319, train/total_loss: 0.6159, train/total_loss/avg: 0.6319, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 1.37, time: 01m 13s 382ms, time_since_start: 02h 18m 13s 836ms, eta: 03h 25m 32s 878ms
[32m2022-10-18T08:31:11 | mmf.trainers.callbacks.logistics: [39mprogress: 9300/22000, train/mmh/cross_entropy: 0.6159, train/mmh/cross_entropy/avg: 0.6315, train/total_loss: 0.6159, train/total_loss/avg: 0.6315, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 1.41, time: 01m 11s 404ms, time_since_start: 02h 19m 25s 241ms, eta: 03h 18m 26s 807ms
[32m2022-10-18T08:32:24 | mmf.trainers.callbacks.logistics: [39mprogress: 9400/22000, train/mmh/cross_entropy: 0.6159, train/mmh/cross_entropy/avg: 0.6313, train/total_loss: 0.6159, train/total_loss/avg: 0.6313, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 1.37, time: 01m 13s 118ms, time_since_start: 02h 20m 38s 359ms, eta: 03h 21m 36s 559ms
[32m2022-10-18T08:33:39 | mmf.trainers.callbacks.logistics: [39mprogress: 9500/22000, train/mmh/cross_entropy: 0.6149, train/mmh/cross_entropy/avg: 0.6312, train/total_loss: 0.6149, train/total_loss/avg: 0.6312, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 1.35, time: 01m 14s 968ms, time_since_start: 02h 21m 53s 328ms, eta: 03h 25m 04s 245ms
[32m2022-10-18T08:35:00 | mmf.trainers.callbacks.logistics: [39mprogress: 9600/22000, train/mmh/cross_entropy: 0.6144, train/mmh/cross_entropy/avg: 0.6295, train/total_loss: 0.6144, train/total_loss/avg: 0.6295, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 738ms, time_since_start: 02h 23m 14s 067ms, eta: 03h 39m 05s 199ms
[32m2022-10-18T08:36:21 | mmf.trainers.callbacks.logistics: [39mprogress: 9700/22000, train/mmh/cross_entropy: 0.6144, train/mmh/cross_entropy/avg: 0.6314, train/total_loss: 0.6144, train/total_loss/avg: 0.6314, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 892ms, time_since_start: 02h 24m 34s 959ms, eta: 03h 37m 44s 067ms
[32m2022-10-18T08:37:38 | mmf.trainers.callbacks.logistics: [39mprogress: 9800/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6285, train/total_loss: 0.6080, train/total_loss/avg: 0.6285, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 1.30, time: 01m 17s 087ms, time_since_start: 02h 25m 52s 046ms, eta: 03h 25m 48s 302ms
[32m2022-10-18T08:38:51 | mmf.trainers.callbacks.logistics: [39mprogress: 9900/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6291, train/total_loss: 0.6080, train/total_loss/avg: 0.6291, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 1.37, time: 01m 13s 527ms, time_since_start: 02h 27m 05s 574ms, eta: 03h 14m 41s 551ms
[32m2022-10-18T08:40:03 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T08:40:03 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T08:40:04 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T08:40:07 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T08:40:07 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, train/mmh/cross_entropy: 0.6042, train/mmh/cross_entropy/avg: 0.6285, train/total_loss: 0.6042, train/total_loss/avg: 0.6285, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 411ms, time_since_start: 02h 28m 20s 985ms, eta: 03h 18m 01s 829ms
[32m2022-10-18T08:40:07 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T08:40:07 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T08:40:07 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T08:43:06 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T08:43:06 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T08:43:06 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T08:43:09 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T08:43:12 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T08:43:12 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, val/mmh/cross_entropy: 0.5354, val/total_loss: 0.5354, val/mmh/accuracy: 0.6676, val/mmh/binary_f1: 0.7471, val/mmh/roc_auc: 0.7235, num_updates: 10000, epoch: 2, iterations: 10000, max_updates: 22000, val_time: 03m 05s 475ms, best_update: 9000, best_iteration: 9000, best_val/mmh/roc_auc: 0.723753
[32m2022-10-18T08:44:25 | mmf.trainers.callbacks.logistics: [39mprogress: 10100/22000, train/mmh/cross_entropy: 0.6042, train/mmh/cross_entropy/avg: 0.6277, train/total_loss: 0.6042, train/total_loss/avg: 0.6277, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 1.39, time: 01m 12s 388ms, time_since_start: 02h 32m 38s 854ms, eta: 03h 08m 30s 455ms
[32m2022-10-18T08:45:35 | mmf.trainers.callbacks.logistics: [39mprogress: 10200/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6288, train/total_loss: 0.6080, train/total_loss/avg: 0.6288, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 321ms, time_since_start: 02h 33m 49s 176ms, eta: 03h 01m 35s 252ms
[32m2022-10-18T08:46:46 | mmf.trainers.callbacks.logistics: [39mprogress: 10300/22000, train/mmh/cross_entropy: 0.6149, train/mmh/cross_entropy/avg: 0.6299, train/total_loss: 0.6149, train/total_loss/avg: 0.6299, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 790ms, time_since_start: 02h 34m 59s 967ms, eta: 03h 01m 14s 898ms
[32m2022-10-18T08:47:56 | mmf.trainers.callbacks.logistics: [39mprogress: 10400/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6287, train/total_loss: 0.6080, train/total_loss/avg: 0.6287, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 469ms, time_since_start: 02h 36m 10s 437ms, eta: 02h 58m 53s 141ms
[32m2022-10-18T08:49:07 | mmf.trainers.callbacks.logistics: [39mprogress: 10500/22000, train/mmh/cross_entropy: 0.6149, train/mmh/cross_entropy/avg: 0.6294, train/total_loss: 0.6149, train/total_loss/avg: 0.6294, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 670ms, time_since_start: 02h 37m 21s 107ms, eta: 02h 57m 50s 923ms
[32m2022-10-18T08:50:17 | mmf.trainers.callbacks.logistics: [39mprogress: 10600/22000, train/mmh/cross_entropy: 0.6159, train/mmh/cross_entropy/avg: 0.6300, train/total_loss: 0.6159, train/total_loss/avg: 0.6300, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 1.45, time: 01m 09s 848ms, time_since_start: 02h 38m 30s 956ms, eta: 02h 54m 15s 089ms
[32m2022-10-18T08:51:28 | mmf.trainers.callbacks.logistics: [39mprogress: 10700/22000, train/mmh/cross_entropy: 0.6149, train/mmh/cross_entropy/avg: 0.6288, train/total_loss: 0.6149, train/total_loss/avg: 0.6288, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 813ms, time_since_start: 02h 39m 41s 769ms, eta: 02h 55m 06s 498ms
[32m2022-10-18T08:52:38 | mmf.trainers.callbacks.logistics: [39mprogress: 10800/22000, train/mmh/cross_entropy: 0.6159, train/mmh/cross_entropy/avg: 0.6293, train/total_loss: 0.6159, train/total_loss/avg: 0.6293, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 079ms, time_since_start: 02h 40m 51s 849ms, eta: 02h 51m 45s 647ms
[32m2022-10-18T08:53:48 | mmf.trainers.callbacks.logistics: [39mprogress: 10900/22000, train/mmh/cross_entropy: 0.6149, train/mmh/cross_entropy/avg: 0.6290, train/total_loss: 0.6149, train/total_loss/avg: 0.6290, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 1.45, time: 01m 09s 923ms, time_since_start: 02h 42m 01s 773ms, eta: 02h 49m 50s 888ms
[32m2022-10-18T08:54:57 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T08:54:57 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T08:54:58 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T08:55:01 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T08:55:01 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, train/mmh/cross_entropy: 0.6149, train/mmh/cross_entropy/avg: 0.6291, train/total_loss: 0.6149, train/total_loss/avg: 0.6291, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 1.37, time: 01m 13s 302ms, time_since_start: 02h 43m 15s 076ms, eta: 02h 56m 27s 140ms
[32m2022-10-18T08:55:01 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T08:55:01 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T08:55:01 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T08:58:01 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T08:58:01 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T08:58:01 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T08:58:04 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T08:58:07 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T08:58:10 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T08:58:10 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, val/mmh/cross_entropy: 0.5328, val/total_loss: 0.5328, val/mmh/accuracy: 0.6912, val/mmh/binary_f1: 0.7773, val/mmh/roc_auc: 0.7252, num_updates: 11000, epoch: 2, iterations: 11000, max_updates: 22000, val_time: 03m 08s 702ms, best_update: 11000, best_iteration: 11000, best_val/mmh/roc_auc: 0.725223
[32m2022-10-18T08:59:20 | mmf.trainers.callbacks.logistics: [39mprogress: 11100/22000, train/mmh/cross_entropy: 0.5909, train/mmh/cross_entropy/avg: 0.6279, train/total_loss: 0.5909, train/total_loss/avg: 0.6279, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 475ms, time_since_start: 02h 47m 34s 256ms, eta: 02h 48m 06s 217ms
[32m2022-10-18T09:00:30 | mmf.trainers.callbacks.logistics: [39mprogress: 11200/22000, train/mmh/cross_entropy: 0.6149, train/mmh/cross_entropy/avg: 0.6281, train/total_loss: 0.6149, train/total_loss/avg: 0.6281, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 226ms, time_since_start: 02h 48m 44s 482ms, eta: 02h 45m 58s 338ms
[32m2022-10-18T09:01:41 | mmf.trainers.callbacks.logistics: [39mprogress: 11300/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6289, train/total_loss: 0.6175, train/total_loss/avg: 0.6289, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 687ms, time_since_start: 02h 49m 55s 170ms, eta: 02h 45m 30s 957ms
[32m2022-10-18T09:02:51 | mmf.trainers.callbacks.logistics: [39mprogress: 11400/22000, train/mmh/cross_entropy: 0.6373, train/mmh/cross_entropy/avg: 0.6301, train/total_loss: 0.6373, train/total_loss/avg: 0.6301, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 1.45, time: 01m 09s 851ms, time_since_start: 02h 51m 05s 022ms, eta: 02h 42m 01s 840ms
[32m2022-10-18T09:04:00 | mmf.trainers.callbacks.logistics: [39mprogress: 11500/22000, train/mmh/cross_entropy: 0.6373, train/mmh/cross_entropy/avg: 0.6292, train/total_loss: 0.6373, train/total_loss/avg: 0.6292, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 1.45, time: 01m 09s 548ms, time_since_start: 02h 52m 14s 570ms, eta: 02h 39m 48s 310ms
[32m2022-10-18T09:05:10 | mmf.trainers.callbacks.logistics: [39mprogress: 11600/22000, train/mmh/cross_entropy: 0.6373, train/mmh/cross_entropy/avg: 0.6279, train/total_loss: 0.6373, train/total_loss/avg: 0.6279, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 1.45, time: 01m 09s 654ms, time_since_start: 02h 53m 24s 225ms, eta: 02h 38m 31s 490ms
[32m2022-10-18T09:06:20 | mmf.trainers.callbacks.logistics: [39mprogress: 11700/22000, train/mmh/cross_entropy: 0.6373, train/mmh/cross_entropy/avg: 0.6282, train/total_loss: 0.6373, train/total_loss/avg: 0.6282, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 1.45, time: 01m 09s 850ms, time_since_start: 02h 54m 34s 075ms, eta: 02h 37m 26s 476ms
[32m2022-10-18T09:07:30 | mmf.trainers.callbacks.logistics: [39mprogress: 11800/22000, train/mmh/cross_entropy: 0.6443, train/mmh/cross_entropy/avg: 0.6293, train/total_loss: 0.6443, train/total_loss/avg: 0.6293, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 1.45, time: 01m 09s 560ms, time_since_start: 02h 55m 43s 636ms, eta: 02h 35m 16s 025ms
[32m2022-10-18T09:08:39 | mmf.trainers.callbacks.logistics: [39mprogress: 11900/22000, train/mmh/cross_entropy: 0.6401, train/mmh/cross_entropy/avg: 0.6294, train/total_loss: 0.6401, train/total_loss/avg: 0.6294, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 1.45, time: 01m 09s 389ms, time_since_start: 02h 56m 53s 026ms, eta: 02h 33m 21s 964ms
[32m2022-10-18T09:09:48 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T09:09:48 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T09:09:49 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T09:09:51 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T09:09:51 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, train/mmh/cross_entropy: 0.6443, train/mmh/cross_entropy/avg: 0.6301, train/total_loss: 0.6443, train/total_loss/avg: 0.6301, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 1.39, time: 01m 12s 561ms, time_since_start: 02h 58m 05s 587ms, eta: 02h 38m 47s 327ms
[32m2022-10-18T09:09:51 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T09:09:51 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T09:09:51 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T09:12:52 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T09:12:52 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T09:12:52 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T09:12:55 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T09:12:58 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T09:13:01 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T09:13:01 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, val/mmh/cross_entropy: 0.5282, val/total_loss: 0.5282, val/mmh/accuracy: 0.6758, val/mmh/binary_f1: 0.7581, val/mmh/roc_auc: 0.7262, num_updates: 12000, epoch: 2, iterations: 12000, max_updates: 22000, val_time: 03m 09s 574ms, best_update: 12000, best_iteration: 12000, best_val/mmh/roc_auc: 0.726188
[32m2022-10-18T09:14:12 | mmf.trainers.callbacks.logistics: [39mprogress: 12100/22000, train/mmh/cross_entropy: 0.6624, train/mmh/cross_entropy/avg: 0.6309, train/total_loss: 0.6624, train/total_loss/avg: 0.6309, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 833ms, time_since_start: 03h 02m 26s 002ms, eta: 02h 33m 27s 455ms
[32m2022-10-18T09:15:22 | mmf.trainers.callbacks.logistics: [39mprogress: 12200/22000, train/mmh/cross_entropy: 0.6443, train/mmh/cross_entropy/avg: 0.6308, train/total_loss: 0.6443, train/total_loss/avg: 0.6308, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 503ms, time_since_start: 03h 03m 36s 506ms, eta: 02h 31m 12s 024ms
[32m2022-10-18T09:16:32 | mmf.trainers.callbacks.logistics: [39mprogress: 12300/22000, train/mmh/cross_entropy: 0.6401, train/mmh/cross_entropy/avg: 0.6292, train/total_loss: 0.6401, train/total_loss/avg: 0.6292, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 034ms, time_since_start: 03h 04m 46s 540ms, eta: 02h 28m 39s 681ms
[32m2022-10-18T09:17:43 | mmf.trainers.callbacks.logistics: [39mprogress: 12400/22000, train/mmh/cross_entropy: 0.6401, train/mmh/cross_entropy/avg: 0.6289, train/total_loss: 0.6401, train/total_loss/avg: 0.6289, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 093ms, time_since_start: 03h 05m 56s 634ms, eta: 02h 27m 15s 148ms
[32m2022-10-18T09:18:53 | mmf.trainers.callbacks.logistics: [39mprogress: 12500/22000, train/mmh/cross_entropy: 0.6373, train/mmh/cross_entropy/avg: 0.6281, train/total_loss: 0.6373, train/total_loss/avg: 0.6281, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 566ms, time_since_start: 03h 07m 07s 200ms, eta: 02h 26m 42s 104ms
[32m2022-10-18T09:20:03 | mmf.trainers.callbacks.logistics: [39mprogress: 12600/22000, train/mmh/cross_entropy: 0.6198, train/mmh/cross_entropy/avg: 0.6277, train/total_loss: 0.6198, train/total_loss/avg: 0.6277, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 709ms, time_since_start: 03h 08m 16s 910ms, eta: 02h 23m 23s 740ms
[32m2022-10-18T09:21:13 | mmf.trainers.callbacks.logistics: [39mprogress: 12700/22000, train/mmh/cross_entropy: 0.6373, train/mmh/cross_entropy/avg: 0.6284, train/total_loss: 0.6373, train/total_loss/avg: 0.6284, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 906ms, time_since_start: 03h 09m 26s 817ms, eta: 02h 22m 16s 258ms
[32m2022-10-18T09:22:23 | mmf.trainers.callbacks.logistics: [39mprogress: 12800/22000, train/mmh/cross_entropy: 0.6373, train/mmh/cross_entropy/avg: 0.6291, train/total_loss: 0.6373, train/total_loss/avg: 0.6291, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 013ms, time_since_start: 03h 10m 36s 830ms, eta: 02h 20m 57s 321ms
[32m2022-10-18T09:23:32 | mmf.trainers.callbacks.logistics: [39mprogress: 12900/22000, train/mmh/cross_entropy: 0.6373, train/mmh/cross_entropy/avg: 0.6287, train/total_loss: 0.6373, train/total_loss/avg: 0.6287, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 614ms, time_since_start: 03h 11m 46s 445ms, eta: 02h 18m 37s 805ms
[32m2022-10-18T09:24:43 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T09:24:43 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T09:24:43 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T09:24:46 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T09:24:46 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, train/mmh/cross_entropy: 0.6401, train/mmh/cross_entropy/avg: 0.6299, train/total_loss: 0.6401, train/total_loss/avg: 0.6299, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 1.37, time: 01m 13s 795ms, time_since_start: 03h 13m 241ms, eta: 02h 25m 20s 373ms
[32m2022-10-18T09:24:46 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T09:24:46 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T09:24:46 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T09:27:47 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T09:27:47 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T09:27:47 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T09:27:49 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T09:27:52 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T09:27:55 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T09:27:55 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, val/mmh/cross_entropy: 0.5137, val/total_loss: 0.5137, val/mmh/accuracy: 0.7410, val/mmh/binary_f1: 0.8421, val/mmh/roc_auc: 0.7268, num_updates: 13000, epoch: 2, iterations: 13000, max_updates: 22000, val_time: 03m 09s 274ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.726792
[32m2022-10-18T09:29:06 | mmf.trainers.callbacks.logistics: [39mprogress: 13100/22000, train/mmh/cross_entropy: 0.6443, train/mmh/cross_entropy/avg: 0.6307, train/total_loss: 0.6443, train/total_loss/avg: 0.6307, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 707ms, time_since_start: 03h 17m 20s 229ms, eta: 02h 17m 42s 622ms
[32m2022-10-18T09:30:17 | mmf.trainers.callbacks.logistics: [39mprogress: 13200/22000, train/mmh/cross_entropy: 0.6401, train/mmh/cross_entropy/avg: 0.6300, train/total_loss: 0.6401, train/total_loss/avg: 0.6300, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 453ms, time_since_start: 03h 18m 30s 682ms, eta: 02h 15m 40s 473ms
[32m2022-10-18T09:31:27 | mmf.trainers.callbacks.logistics: [39mprogress: 13300/22000, train/mmh/cross_entropy: 0.6401, train/mmh/cross_entropy/avg: 0.6304, train/total_loss: 0.6401, train/total_loss/avg: 0.6304, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 074ms, time_since_start: 03h 19m 40s 756ms, eta: 02h 13m 24s 659ms
[32m2022-10-18T09:32:37 | mmf.trainers.callbacks.logistics: [39mprogress: 13400/22000, train/mmh/cross_entropy: 0.6401, train/mmh/cross_entropy/avg: 0.6310, train/total_loss: 0.6401, train/total_loss/avg: 0.6310, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 598ms, time_since_start: 03h 20m 51s 355ms, eta: 02h 12m 51s 844ms
[32m2022-10-18T09:33:47 | mmf.trainers.callbacks.logistics: [39mprogress: 13500/22000, train/mmh/cross_entropy: 0.6401, train/mmh/cross_entropy/avg: 0.6306, train/total_loss: 0.6401, train/total_loss/avg: 0.6306, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 925ms, time_since_start: 03h 22m 01s 280ms, eta: 02h 10m 04s 015ms
[32m2022-10-18T09:34:58 | mmf.trainers.callbacks.logistics: [39mprogress: 13600/22000, train/mmh/cross_entropy: 0.6624, train/mmh/cross_entropy/avg: 0.6312, train/total_loss: 0.6624, train/total_loss/avg: 0.6312, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 878ms, time_since_start: 03h 23m 12s 159ms, eta: 02h 10m 17s 314ms
[32m2022-10-18T09:36:08 | mmf.trainers.callbacks.logistics: [39mprogress: 13700/22000, train/mmh/cross_entropy: 0.6492, train/mmh/cross_entropy/avg: 0.6313, train/total_loss: 0.6492, train/total_loss/avg: 0.6313, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 091ms, time_since_start: 03h 24m 22s 250ms, eta: 02h 07m 18s 459ms
[32m2022-10-18T09:37:18 | mmf.trainers.callbacks.logistics: [39mprogress: 13800/22000, train/mmh/cross_entropy: 0.6401, train/mmh/cross_entropy/avg: 0.6302, train/total_loss: 0.6401, train/total_loss/avg: 0.6302, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 271ms, time_since_start: 03h 25m 32s 521ms, eta: 02h 06m 05s 829ms
[32m2022-10-18T09:38:28 | mmf.trainers.callbacks.logistics: [39mprogress: 13900/22000, train/mmh/cross_entropy: 0.6492, train/mmh/cross_entropy/avg: 0.6303, train/total_loss: 0.6492, train/total_loss/avg: 0.6303, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 481ms, time_since_start: 03h 26m 42s 002ms, eta: 02h 03m 09s 552ms
[32m2022-10-18T09:39:37 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T09:39:37 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T09:39:38 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T09:39:41 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T09:39:41 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, train/mmh/cross_entropy: 0.6198, train/mmh/cross_entropy/avg: 0.6294, train/total_loss: 0.6198, train/total_loss/avg: 0.6294, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 1.37, time: 01m 13s 154ms, time_since_start: 03h 27m 55s 157ms, eta: 02h 08m 04s 136ms
[32m2022-10-18T09:39:41 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T09:39:41 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T09:39:41 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T09:42:42 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T09:42:42 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T09:42:42 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T09:42:45 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T09:42:48 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T09:42:48 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, val/mmh/cross_entropy: 0.5321, val/total_loss: 0.5321, val/mmh/accuracy: 0.6874, val/mmh/binary_f1: 0.7731, val/mmh/roc_auc: 0.7244, num_updates: 14000, epoch: 2, iterations: 14000, max_updates: 22000, val_time: 03m 06s 863ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.726792
[32m2022-10-18T09:43:59 | mmf.trainers.callbacks.logistics: [39mprogress: 14100/22000, train/mmh/cross_entropy: 0.6198, train/mmh/cross_entropy/avg: 0.6297, train/total_loss: 0.6198, train/total_loss/avg: 0.6297, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 698ms, time_since_start: 03h 32m 12s 724ms, eta: 02h 02m 13s 306ms
[32m2022-10-18T09:45:09 | mmf.trainers.callbacks.logistics: [39mprogress: 14200/22000, train/mmh/cross_entropy: 0.6438, train/mmh/cross_entropy/avg: 0.6298, train/total_loss: 0.6438, train/total_loss/avg: 0.6298, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 355ms, time_since_start: 03h 33m 23s 079ms, eta: 02h 05s 350ms
[32m2022-10-18T09:46:19 | mmf.trainers.callbacks.logistics: [39mprogress: 14300/22000, train/mmh/cross_entropy: 0.6438, train/mmh/cross_entropy/avg: 0.6298, train/total_loss: 0.6438, train/total_loss/avg: 0.6298, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 470ms, time_since_start: 03h 34m 33s 549ms, eta: 01h 58m 44s 593ms
[32m2022-10-18T09:47:30 | mmf.trainers.callbacks.logistics: [39mprogress: 14400/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6299, train/total_loss: 0.6471, train/total_loss/avg: 0.6299, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 647ms, time_since_start: 03h 35m 44s 197ms, eta: 01h 57m 29s 742ms
[32m2022-10-18T09:48:40 | mmf.trainers.callbacks.logistics: [39mprogress: 14500/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6298, train/total_loss: 0.6471, train/total_loss/avg: 0.6298, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 311ms, time_since_start: 03h 36m 54s 508ms, eta: 01h 55m 23s 877ms
[32m2022-10-18T09:49:50 | mmf.trainers.callbacks.logistics: [39mprogress: 14600/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6296, train/total_loss: 0.6471, train/total_loss/avg: 0.6296, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 409ms, time_since_start: 03h 38m 03s 917ms, eta: 01h 52m 23s 973ms
[32m2022-10-18T09:50:59 | mmf.trainers.callbacks.logistics: [39mprogress: 14700/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6301, train/total_loss: 0.6471, train/total_loss/avg: 0.6301, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 262ms, time_since_start: 03h 39m 13s 180ms, eta: 01h 50m 38s 782ms
[32m2022-10-18T09:52:08 | mmf.trainers.callbacks.logistics: [39mprogress: 14800/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6311, train/total_loss: 0.6471, train/total_loss/avg: 0.6311, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 152ms, time_since_start: 03h 40m 22s 332ms, eta: 01h 48m 57s 371ms
[32m2022-10-18T09:53:18 | mmf.trainers.callbacks.logistics: [39mprogress: 14900/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6308, train/total_loss: 0.6471, train/total_loss/avg: 0.6308, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 680ms, time_since_start: 03h 41m 32s 013ms, eta: 01h 48m 15s 832ms
[32m2022-10-18T09:54:27 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T09:54:27 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T09:54:28 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T09:54:31 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T09:54:31 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, train/mmh/cross_entropy: 0.6438, train/mmh/cross_entropy/avg: 0.6306, train/total_loss: 0.6438, train/total_loss/avg: 0.6306, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 1.37, time: 01m 13s 189ms, time_since_start: 03h 42m 45s 203ms, eta: 01h 52m 06s 857ms
[32m2022-10-18T09:54:31 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T09:54:31 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T09:54:31 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T09:57:33 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T09:57:33 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T09:57:33 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T09:57:37 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T09:57:41 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T09:57:41 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, val/mmh/cross_entropy: 0.5565, val/total_loss: 0.5565, val/mmh/accuracy: 0.6578, val/mmh/binary_f1: 0.7317, val/mmh/roc_auc: 0.7239, num_updates: 15000, epoch: 2, iterations: 15000, max_updates: 22000, val_time: 03m 09s 604ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.726792
[32m2022-10-18T09:58:52 | mmf.trainers.callbacks.logistics: [39mprogress: 15100/22000, train/mmh/cross_entropy: 0.6250, train/mmh/cross_entropy/avg: 0.6299, train/total_loss: 0.6250, train/total_loss/avg: 0.6299, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 1.41, time: 01m 11s 216ms, time_since_start: 03h 47m 06s 027ms, eta: 01h 47m 32s 042ms
[32m2022-10-18T10:00:02 | mmf.trainers.callbacks.logistics: [39mprogress: 15200/22000, train/mmh/cross_entropy: 0.6438, train/mmh/cross_entropy/avg: 0.6307, train/total_loss: 0.6438, train/total_loss/avg: 0.6307, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 074ms, time_since_start: 03h 48m 16s 102ms, eta: 01h 44m 16s 577ms
[32m2022-10-18T10:01:13 | mmf.trainers.callbacks.logistics: [39mprogress: 15300/22000, train/mmh/cross_entropy: 0.6250, train/mmh/cross_entropy/avg: 0.6301, train/total_loss: 0.6250, train/total_loss/avg: 0.6301, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 1.41, time: 01m 11s 000ms, time_since_start: 03h 49m 27s 103ms, eta: 01h 44m 06s 001ms
[32m2022-10-18T10:02:23 | mmf.trainers.callbacks.logistics: [39mprogress: 15400/22000, train/mmh/cross_entropy: 0.6086, train/mmh/cross_entropy/avg: 0.6298, train/total_loss: 0.6086, train/total_loss/avg: 0.6298, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 790ms, time_since_start: 03h 50m 36s 894ms, eta: 01h 40m 47s 930ms
[32m2022-10-18T10:03:34 | mmf.trainers.callbacks.logistics: [39mprogress: 15500/22000, train/mmh/cross_entropy: 0.6250, train/mmh/cross_entropy/avg: 0.6302, train/total_loss: 0.6250, train/total_loss/avg: 0.6302, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 1.43, time: 01m 10s 873ms, time_since_start: 03h 51m 47s 767ms, eta: 01h 40m 48s 729ms
[32m2022-10-18T10:04:43 | mmf.trainers.callbacks.logistics: [39mprogress: 15600/22000, train/mmh/cross_entropy: 0.6086, train/mmh/cross_entropy/avg: 0.6291, train/total_loss: 0.6086, train/total_loss/avg: 0.6291, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 506ms, time_since_start: 03h 52m 57s 274ms, eta: 01h 37m 20s 761ms
[32m2022-10-18T10:05:53 | mmf.trainers.callbacks.logistics: [39mprogress: 15700/22000, train/mmh/cross_entropy: 0.6086, train/mmh/cross_entropy/avg: 0.6296, train/total_loss: 0.6086, train/total_loss/avg: 0.6296, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 403ms, time_since_start: 03h 54m 06s 677ms, eta: 01h 35m 40s 975ms
[32m2022-10-18T10:07:02 | mmf.trainers.callbacks.logistics: [39mprogress: 15800/22000, train/mmh/cross_entropy: 0.6250, train/mmh/cross_entropy/avg: 0.6302, train/total_loss: 0.6250, train/total_loss/avg: 0.6302, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 936ms, time_since_start: 03h 55m 16s 614ms, eta: 01h 34m 53s 279ms
[32m2022-10-18T10:08:12 | mmf.trainers.callbacks.logistics: [39mprogress: 15900/22000, train/mmh/cross_entropy: 0.6250, train/mmh/cross_entropy/avg: 0.6305, train/total_loss: 0.6250, train/total_loss/avg: 0.6305, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 1.45, time: 01m 09s 896ms, time_since_start: 03h 56m 26s 510ms, eta: 01h 33m 18s 187ms
[32m2022-10-18T10:09:22 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T10:09:22 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T10:09:23 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T10:09:26 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T10:09:26 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, train/mmh/cross_entropy: 0.6250, train/mmh/cross_entropy/avg: 0.6301, train/total_loss: 0.6250, train/total_loss/avg: 0.6301, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 1.37, time: 01m 13s 429ms, time_since_start: 03h 57m 39s 940ms, eta: 01h 36m 24s 810ms
[32m2022-10-18T10:09:26 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T10:09:26 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T10:09:26 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T10:12:26 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T10:12:26 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T10:12:26 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T10:12:29 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T10:12:33 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T10:12:33 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, val/mmh/cross_entropy: 0.5367, val/total_loss: 0.5367, val/mmh/accuracy: 0.6764, val/mmh/binary_f1: 0.7580, val/mmh/roc_auc: 0.7238, num_updates: 16000, epoch: 2, iterations: 16000, max_updates: 22000, val_time: 03m 06s 864ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.726792
[32m2022-10-18T10:13:44 | mmf.trainers.callbacks.logistics: [39mprogress: 16100/22000, train/mmh/cross_entropy: 0.6086, train/mmh/cross_entropy/avg: 0.6298, train/total_loss: 0.6086, train/total_loss/avg: 0.6298, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 897ms, time_since_start: 04h 01m 57s 704ms, eta: 01h 31m 32s 242ms
[32m2022-10-18T10:14:55 | mmf.trainers.callbacks.logistics: [39mprogress: 16200/22000, train/mmh/cross_entropy: 0.6086, train/mmh/cross_entropy/avg: 0.6302, train/total_loss: 0.6086, train/total_loss/avg: 0.6302, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 980ms, time_since_start: 04h 03m 08s 685ms, eta: 01h 30m 05s 446ms
[32m2022-10-18T10:16:05 | mmf.trainers.callbacks.logistics: [39mprogress: 16300/22000, train/mmh/cross_entropy: 0.6086, train/mmh/cross_entropy/avg: 0.6309, train/total_loss: 0.6086, train/total_loss/avg: 0.6309, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 588ms, time_since_start: 04h 04m 19s 273ms, eta: 01h 28m 02s 924ms
[32m2022-10-18T10:17:16 | mmf.trainers.callbacks.logistics: [39mprogress: 16400/22000, train/mmh/cross_entropy: 0.6086, train/mmh/cross_entropy/avg: 0.6317, train/total_loss: 0.6086, train/total_loss/avg: 0.6317, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 751ms, time_since_start: 04h 05m 30s 025ms, eta: 01h 26m 42s 240ms
[32m2022-10-18T10:18:27 | mmf.trainers.callbacks.logistics: [39mprogress: 16500/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6316, train/total_loss: 0.6080, train/total_loss/avg: 0.6316, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 759ms, time_since_start: 04h 06m 40s 785ms, eta: 01h 25m 09s 926ms
[32m2022-10-18T10:19:37 | mmf.trainers.callbacks.logistics: [39mprogress: 16600/22000, train/mmh/cross_entropy: 0.6753, train/mmh/cross_entropy/avg: 0.6321, train/total_loss: 0.6753, train/total_loss/avg: 0.6321, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 249ms, time_since_start: 04h 07m 51s 035ms, eta: 01h 23m 844ms
[32m2022-10-18T10:20:47 | mmf.trainers.callbacks.logistics: [39mprogress: 16700/22000, train/mmh/cross_entropy: 0.6753, train/mmh/cross_entropy/avg: 0.6329, train/total_loss: 0.6753, train/total_loss/avg: 0.6329, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 490ms, time_since_start: 04h 09m 01s 526ms, eta: 01h 21m 45s 394ms
[32m2022-10-18T10:21:57 | mmf.trainers.callbacks.logistics: [39mprogress: 16800/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6327, train/total_loss: 0.6080, train/total_loss/avg: 0.6327, max mem: 3161.0, experiment: mmh_a10, epoch: 2, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 1.45, time: 01m 09s 652ms, time_since_start: 04h 10m 11s 178ms, eta: 01h 19m 15s 583ms
[32m2022-10-18T10:23:07 | mmf.trainers.callbacks.logistics: [39mprogress: 16900/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6321, train/total_loss: 0.6080, train/total_loss/avg: 0.6321, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 1.45, time: 01m 09s 531ms, time_since_start: 04h 11m 20s 709ms, eta: 01h 17m 36s 026ms
[32m2022-10-18T10:24:17 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T10:24:17 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T10:24:17 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T10:24:20 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T10:24:20 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6317, train/total_loss: 0.6080, train/total_loss/avg: 0.6317, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 1.37, time: 01m 13s 605ms, time_since_start: 04h 12m 34s 315ms, eta: 01h 20m 32s 205ms
[32m2022-10-18T10:24:20 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T10:24:20 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T10:24:20 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T10:27:19 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T10:27:19 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T10:27:19 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T10:27:23 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T10:27:26 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T10:27:26 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, val/mmh/cross_entropy: 0.5378, val/total_loss: 0.5378, val/mmh/accuracy: 0.6662, val/mmh/binary_f1: 0.7444, val/mmh/roc_auc: 0.7240, num_updates: 17000, epoch: 3, iterations: 17000, max_updates: 22000, val_time: 03m 05s 599ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.726792
[32m2022-10-18T10:28:38 | mmf.trainers.callbacks.logistics: [39mprogress: 17100/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6311, train/total_loss: 0.6080, train/total_loss/avg: 0.6311, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 585ms, time_since_start: 04h 16m 52s 507ms, eta: 01h 17m 49s 932ms
[32m2022-10-18T10:29:51 | mmf.trainers.callbacks.logistics: [39mprogress: 17200/22000, train/mmh/cross_entropy: 0.6009, train/mmh/cross_entropy/avg: 0.6304, train/total_loss: 0.6009, train/total_loss/avg: 0.6304, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 386ms, time_since_start: 04h 18m 04s 894ms, eta: 01h 16m 02s 114ms
[32m2022-10-18T10:31:03 | mmf.trainers.callbacks.logistics: [39mprogress: 17300/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6306, train/total_loss: 0.6080, train/total_loss/avg: 0.6306, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 688ms, time_since_start: 04h 19m 17s 582ms, eta: 01h 14m 45s 679ms
[32m2022-10-18T10:32:16 | mmf.trainers.callbacks.logistics: [39mprogress: 17400/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6293, train/total_loss: 0.6080, train/total_loss/avg: 0.6293, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 749ms, time_since_start: 04h 20m 30s 332ms, eta: 01h 13m 13s 935ms
[32m2022-10-18T10:33:29 | mmf.trainers.callbacks.logistics: [39mprogress: 17500/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6296, train/total_loss: 0.6080, train/total_loss/avg: 0.6296, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 952ms, time_since_start: 04h 21m 43s 285ms, eta: 01h 11m 50s 418ms
[32m2022-10-18T10:34:42 | mmf.trainers.callbacks.logistics: [39mprogress: 17600/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6289, train/total_loss: 0.6080, train/total_loss/avg: 0.6289, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 670ms, time_since_start: 04h 22m 55s 955ms, eta: 01h 09m 58s 303ms
[32m2022-10-18T10:35:54 | mmf.trainers.callbacks.logistics: [39mprogress: 17700/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6288, train/total_loss: 0.6080, train/total_loss/avg: 0.6288, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 393ms, time_since_start: 04h 24m 08s 348ms, eta: 01h 08m 07s 257ms
[32m2022-10-18T10:37:06 | mmf.trainers.callbacks.logistics: [39mprogress: 17800/22000, train/mmh/cross_entropy: 0.6009, train/mmh/cross_entropy/avg: 0.6285, train/total_loss: 0.6009, train/total_loss/avg: 0.6285, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 252ms, time_since_start: 04h 25m 20s 601ms, eta: 01h 06m 24s 435ms
[32m2022-10-18T10:38:19 | mmf.trainers.callbacks.logistics: [39mprogress: 17900/22000, train/mmh/cross_entropy: 0.6009, train/mmh/cross_entropy/avg: 0.6287, train/total_loss: 0.6009, train/total_loss/avg: 0.6287, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 582ms, time_since_start: 04h 26m 33s 183ms, eta: 01h 05m 07s 323ms
[32m2022-10-18T10:39:32 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T10:39:32 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T10:39:33 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T10:39:36 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T10:39:36 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, train/mmh/cross_entropy: 0.6060, train/mmh/cross_entropy/avg: 0.6286, train/total_loss: 0.6060, train/total_loss/avg: 0.6286, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 1.30, time: 01m 17s 207ms, time_since_start: 04h 27m 50s 391ms, eta: 01h 07m 34s 948ms
[32m2022-10-18T10:39:36 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T10:39:36 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T10:39:36 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T10:42:36 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T10:42:36 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T10:42:36 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, val/mmh/cross_entropy: 0.5466, val/total_loss: 0.5466, val/mmh/accuracy: 0.6688, val/mmh/binary_f1: 0.7489, val/mmh/roc_auc: 0.7240, num_updates: 18000, epoch: 3, iterations: 18000, max_updates: 22000, val_time: 03m 191ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.726792
[32m2022-10-18T10:43:52 | mmf.trainers.callbacks.logistics: [39mprogress: 18100/22000, train/mmh/cross_entropy: 0.6060, train/mmh/cross_entropy/avg: 0.6282, train/total_loss: 0.6060, train/total_loss/avg: 0.6282, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 612ms, time_since_start: 04h 32m 06s 201ms, eta: 01h 04m 31s 884ms
[32m2022-10-18T10:45:06 | mmf.trainers.callbacks.logistics: [39mprogress: 18200/22000, train/mmh/cross_entropy: 0.6009, train/mmh/cross_entropy/avg: 0.6280, train/total_loss: 0.6009, train/total_loss/avg: 0.6280, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 1.35, time: 01m 14s 395ms, time_since_start: 04h 33m 20s 597ms, eta: 01h 01m 51s 907ms
[32m2022-10-18T10:46:20 | mmf.trainers.callbacks.logistics: [39mprogress: 18300/22000, train/mmh/cross_entropy: 0.5953, train/mmh/cross_entropy/avg: 0.6278, train/total_loss: 0.5953, train/total_loss/avg: 0.6278, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 1.37, time: 01m 13s 091ms, time_since_start: 04h 34m 33s 689ms, eta: 59m 10s 881ms
[32m2022-10-18T10:47:32 | mmf.trainers.callbacks.logistics: [39mprogress: 18400/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6274, train/total_loss: 0.5915, train/total_loss/avg: 0.6274, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 023ms, time_since_start: 04h 35m 45s 713ms, eta: 56m 44s 425ms
[32m2022-10-18T10:48:43 | mmf.trainers.callbacks.logistics: [39mprogress: 18500/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6278, train/total_loss: 0.5915, train/total_loss/avg: 0.6278, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 1.41, time: 01m 11s 830ms, time_since_start: 04h 36m 57s 543ms, eta: 55m 953ms
[32m2022-10-18T10:49:55 | mmf.trainers.callbacks.logistics: [39mprogress: 18600/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6278, train/total_loss: 0.5915, train/total_loss/avg: 0.6278, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 1.41, time: 01m 11s 745ms, time_since_start: 04h 38m 09s 289ms, eta: 53m 22s 877ms
[32m2022-10-18T10:51:07 | mmf.trainers.callbacks.logistics: [39mprogress: 18700/22000, train/mmh/cross_entropy: 0.5816, train/mmh/cross_entropy/avg: 0.6269, train/total_loss: 0.5816, train/total_loss/avg: 0.6269, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 1.41, time: 01m 11s 382ms, time_since_start: 04h 39m 20s 671ms, eta: 51m 32s 919ms
[32m2022-10-18T10:52:18 | mmf.trainers.callbacks.logistics: [39mprogress: 18800/22000, train/mmh/cross_entropy: 0.5816, train/mmh/cross_entropy/avg: 0.6279, train/total_loss: 0.5816, train/total_loss/avg: 0.6279, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 1.41, time: 01m 11s 457ms, time_since_start: 04h 40m 32s 128ms, eta: 50m 02s 359ms
[32m2022-10-18T10:53:28 | mmf.trainers.callbacks.logistics: [39mprogress: 18900/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6280, train/total_loss: 0.5915, train/total_loss/avg: 0.6280, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 102ms, time_since_start: 04h 41m 42s 231ms, eta: 47m 33s 392ms
[32m2022-10-18T10:54:39 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T10:54:39 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T10:54:40 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T10:54:43 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T10:54:43 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, train/mmh/cross_entropy: 0.5953, train/mmh/cross_entropy/avg: 0.6280, train/total_loss: 0.5953, train/total_loss/avg: 0.6280, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 1.35, time: 01m 14s 648ms, time_since_start: 04h 42m 56s 879ms, eta: 49m 401ms
[32m2022-10-18T10:54:43 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T10:54:43 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T10:54:43 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T10:57:43 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T10:57:43 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T10:57:43 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, val/mmh/cross_entropy: 0.5322, val/total_loss: 0.5322, val/mmh/accuracy: 0.6942, val/mmh/binary_f1: 0.7828, val/mmh/roc_auc: 0.7241, num_updates: 19000, epoch: 3, iterations: 19000, max_updates: 22000, val_time: 03m 528ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.726792
[32m2022-10-18T10:58:54 | mmf.trainers.callbacks.logistics: [39mprogress: 19100/22000, train/mmh/cross_entropy: 0.5953, train/mmh/cross_entropy/avg: 0.6274, train/total_loss: 0.5953, train/total_loss/avg: 0.6274, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 902ms, time_since_start: 04h 47m 08s 317ms, eta: 44m 59s 768ms
[32m2022-10-18T11:00:05 | mmf.trainers.callbacks.logistics: [39mprogress: 19200/22000, train/mmh/cross_entropy: 0.5953, train/mmh/cross_entropy/avg: 0.6270, train/total_loss: 0.5953, train/total_loss/avg: 0.6270, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 381ms, time_since_start: 04h 48m 18s 699ms, eta: 43m 07s 511ms
[32m2022-10-18T11:01:15 | mmf.trainers.callbacks.logistics: [39mprogress: 19300/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6266, train/total_loss: 0.5915, train/total_loss/avg: 0.6266, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 280ms, time_since_start: 04h 49m 28s 979ms, eta: 41m 31s 497ms
[32m2022-10-18T11:02:25 | mmf.trainers.callbacks.logistics: [39mprogress: 19400/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6262, train/total_loss: 0.5915, train/total_loss/avg: 0.6262, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 154ms, time_since_start: 04h 50m 39s 134ms, eta: 39m 54s 946ms
[32m2022-10-18T11:03:35 | mmf.trainers.callbacks.logistics: [39mprogress: 19500/22000, train/mmh/cross_entropy: 0.5816, train/mmh/cross_entropy/avg: 0.6256, train/total_loss: 0.5816, train/total_loss/avg: 0.6256, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 1.45, time: 01m 09s 607ms, time_since_start: 04h 51m 48s 741ms, eta: 38m 04s 860ms
[32m2022-10-18T11:04:45 | mmf.trainers.callbacks.logistics: [39mprogress: 19600/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6262, train/total_loss: 0.5915, train/total_loss/avg: 0.6262, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 569ms, time_since_start: 04h 52m 59s 310ms, eta: 37m 03s 772ms
[32m2022-10-18T11:05:55 | mmf.trainers.callbacks.logistics: [39mprogress: 19700/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6263, train/total_loss: 0.5915, train/total_loss/avg: 0.6263, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 1.45, time: 01m 09s 829ms, time_since_start: 04h 54m 09s 139ms, eta: 35m 08s 775ms
[32m2022-10-18T11:07:05 | mmf.trainers.callbacks.logistics: [39mprogress: 19800/22000, train/mmh/cross_entropy: 0.5953, train/mmh/cross_entropy/avg: 0.6270, train/total_loss: 0.5953, train/total_loss/avg: 0.6270, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 1.45, time: 01m 09s 957ms, time_since_start: 04h 55m 19s 097ms, eta: 33m 40s 783ms
[32m2022-10-18T11:08:15 | mmf.trainers.callbacks.logistics: [39mprogress: 19900/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6266, train/total_loss: 0.5915, train/total_loss/avg: 0.6266, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 1.43, time: 01m 10s 082ms, time_since_start: 04h 56m 29s 179ms, eta: 32m 12s 372ms
[32m2022-10-18T11:09:25 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T11:09:25 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T11:09:25 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T11:09:28 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T11:09:28 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6266, train/total_loss: 0.5915, train/total_loss/avg: 0.6266, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 1.39, time: 01m 12s 978ms, time_since_start: 04h 57m 42s 158ms, eta: 31m 56s 424ms
[32m2022-10-18T11:09:28 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T11:09:28 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T11:09:28 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T11:12:28 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T11:12:28 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T11:12:28 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, val/mmh/cross_entropy: 0.5308, val/total_loss: 0.5308, val/mmh/accuracy: 0.6912, val/mmh/binary_f1: 0.7771, val/mmh/roc_auc: 0.7243, num_updates: 20000, epoch: 3, iterations: 20000, max_updates: 22000, val_time: 02m 59s 955ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.726792
[32m2022-10-18T11:13:38 | mmf.trainers.callbacks.logistics: [39mprogress: 20100/22000, train/mmh/cross_entropy: 0.5915, train/mmh/cross_entropy/avg: 0.6261, train/total_loss: 0.5915, train/total_loss/avg: 0.6261, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.43, time: 01m 10s 084ms, time_since_start: 05h 01m 52s 200ms, eta: 29m 08s 391ms
[32m2022-10-18T11:14:48 | mmf.trainers.callbacks.logistics: [39mprogress: 20200/22000, train/mmh/cross_entropy: 0.5561, train/mmh/cross_entropy/avg: 0.6252, train/total_loss: 0.5561, train/total_loss/avg: 0.6252, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 977ms, time_since_start: 05h 03m 02s 177ms, eta: 27m 33s 849ms
[32m2022-10-18T11:15:58 | mmf.trainers.callbacks.logistics: [39mprogress: 20300/22000, train/mmh/cross_entropy: 0.5561, train/mmh/cross_entropy/avg: 0.6251, train/total_loss: 0.5561, train/total_loss/avg: 0.6251, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 767ms, time_since_start: 05h 04m 11s 945ms, eta: 25m 57s 287ms
[32m2022-10-18T11:17:08 | mmf.trainers.callbacks.logistics: [39mprogress: 20400/22000, train/mmh/cross_entropy: 0.5561, train/mmh/cross_entropy/avg: 0.6247, train/total_loss: 0.5561, train/total_loss/avg: 0.6247, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 778ms, time_since_start: 05h 05m 21s 723ms, eta: 24m 25s 901ms
[32m2022-10-18T11:18:18 | mmf.trainers.callbacks.logistics: [39mprogress: 20500/22000, train/mmh/cross_entropy: 0.5561, train/mmh/cross_entropy/avg: 0.6256, train/total_loss: 0.5561, train/total_loss/avg: 0.6256, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.43, time: 01m 10s 238ms, time_since_start: 05h 06m 31s 962ms, eta: 23m 03s 352ms
[32m2022-10-18T11:19:27 | mmf.trainers.callbacks.logistics: [39mprogress: 20600/22000, train/mmh/cross_entropy: 0.5561, train/mmh/cross_entropy/avg: 0.6256, train/total_loss: 0.5561, train/total_loss/avg: 0.6256, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 647ms, time_since_start: 05h 07m 41s 609ms, eta: 21m 20s 252ms
[32m2022-10-18T11:20:37 | mmf.trainers.callbacks.logistics: [39mprogress: 20700/22000, train/mmh/cross_entropy: 0.6096, train/mmh/cross_entropy/avg: 0.6257, train/total_loss: 0.6096, train/total_loss/avg: 0.6257, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 887ms, time_since_start: 05h 08m 51s 497ms, eta: 19m 52s 913ms
[32m2022-10-18T11:21:47 | mmf.trainers.callbacks.logistics: [39mprogress: 20800/22000, train/mmh/cross_entropy: 0.6096, train/mmh/cross_entropy/avg: 0.6258, train/total_loss: 0.6096, train/total_loss/avg: 0.6258, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 674ms, time_since_start: 05h 10m 01s 172ms, eta: 18m 17s 792ms
[32m2022-10-18T11:22:57 | mmf.trainers.callbacks.logistics: [39mprogress: 20900/22000, train/mmh/cross_entropy: 0.5561, train/mmh/cross_entropy/avg: 0.6253, train/total_loss: 0.5561, train/total_loss/avg: 0.6253, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 570ms, time_since_start: 05h 11m 10s 742ms, eta: 16m 44s 799ms
[32m2022-10-18T11:24:06 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T11:24:06 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T11:24:07 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T11:24:10 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T11:24:10 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, train/mmh/cross_entropy: 0.5533, train/mmh/cross_entropy/avg: 0.6248, train/total_loss: 0.5533, train/total_loss/avg: 0.6248, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 1.37, time: 01m 13s 266ms, time_since_start: 05h 12m 24s 008ms, eta: 16m 01s 988ms
[32m2022-10-18T11:24:10 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T11:24:10 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T11:24:10 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T11:27:10 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T11:27:10 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T11:27:10 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, val/mmh/cross_entropy: 0.5294, val/total_loss: 0.5294, val/mmh/accuracy: 0.6942, val/mmh/binary_f1: 0.7811, val/mmh/roc_auc: 0.7249, num_updates: 21000, epoch: 3, iterations: 21000, max_updates: 22000, val_time: 02m 59s 999ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.726792
[32m2022-10-18T11:28:20 | mmf.trainers.callbacks.logistics: [39mprogress: 21100/22000, train/mmh/cross_entropy: 0.5561, train/mmh/cross_entropy/avg: 0.6248, train/total_loss: 0.5561, train/total_loss/avg: 0.6248, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.43, time: 01m 10s 117ms, time_since_start: 05h 16m 34s 127ms, eta: 13m 48s 574ms
[32m2022-10-18T11:29:30 | mmf.trainers.callbacks.logistics: [39mprogress: 21200/22000, train/mmh/cross_entropy: 0.6096, train/mmh/cross_entropy/avg: 0.6255, train/total_loss: 0.6096, train/total_loss/avg: 0.6255, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 701ms, time_since_start: 05h 17m 43s 828ms, eta: 12m 12s 139ms
[32m2022-10-18T11:30:40 | mmf.trainers.callbacks.logistics: [39mprogress: 21300/22000, train/mmh/cross_entropy: 0.6220, train/mmh/cross_entropy/avg: 0.6256, train/total_loss: 0.6220, train/total_loss/avg: 0.6256, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 801ms, time_since_start: 05h 18m 53s 629ms, eta: 10m 41s 541ms
[32m2022-10-18T11:31:49 | mmf.trainers.callbacks.logistics: [39mprogress: 21400/22000, train/mmh/cross_entropy: 0.6315, train/mmh/cross_entropy/avg: 0.6258, train/total_loss: 0.6315, train/total_loss/avg: 0.6258, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 740ms, time_since_start: 05h 20m 03s 370ms, eta: 09m 09s 415ms
[32m2022-10-18T11:32:59 | mmf.trainers.callbacks.logistics: [39mprogress: 21500/22000, train/mmh/cross_entropy: 0.6315, train/mmh/cross_entropy/avg: 0.6254, train/total_loss: 0.6315, train/total_loss/avg: 0.6254, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 494ms, time_since_start: 05h 21m 12s 864ms, eta: 07m 36s 228ms
[32m2022-10-18T11:34:09 | mmf.trainers.callbacks.logistics: [39mprogress: 21600/22000, train/mmh/cross_entropy: 0.6220, train/mmh/cross_entropy/avg: 0.6248, train/total_loss: 0.6220, train/total_loss/avg: 0.6248, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.43, time: 01m 10s 270ms, time_since_start: 05h 22m 23s 134ms, eta: 06m 09s 058ms
[32m2022-10-18T11:35:19 | mmf.trainers.callbacks.logistics: [39mprogress: 21700/22000, train/mmh/cross_entropy: 0.6096, train/mmh/cross_entropy/avg: 0.6241, train/total_loss: 0.6096, train/total_loss/avg: 0.6241, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 778ms, time_since_start: 05h 23m 32s 912ms, eta: 04m 34s 855ms
[32m2022-10-18T11:36:28 | mmf.trainers.callbacks.logistics: [39mprogress: 21800/22000, train/mmh/cross_entropy: 0.5711, train/mmh/cross_entropy/avg: 0.6238, train/total_loss: 0.5711, train/total_loss/avg: 0.6238, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 189ms, time_since_start: 05h 24m 42s 101ms, eta: 03m 01s 690ms
[32m2022-10-18T11:37:37 | mmf.trainers.callbacks.logistics: [39mprogress: 21900/22000, train/mmh/cross_entropy: 0.5711, train/mmh/cross_entropy/avg: 0.6228, train/total_loss: 0.5711, train/total_loss/avg: 0.6228, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.45, time: 01m 09s 392ms, time_since_start: 05h 25m 51s 493ms, eta: 01m 31s 112ms
[32m2022-10-18T11:38:47 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T11:38:47 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T11:38:48 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T11:38:51 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T11:38:51 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, train/mmh/cross_entropy: 0.5351, train/mmh/cross_entropy/avg: 0.6223, train/total_loss: 0.5351, train/total_loss/avg: 0.6223, max mem: 3161.0, experiment: mmh_a10, epoch: 3, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 1.37, time: 01m 13s 383ms, time_since_start: 05h 27m 04s 877ms, eta: 0ms
[32m2022-10-18T11:38:51 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T11:38:51 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T11:38:51 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T11:41:51 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T11:41:51 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T11:41:51 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, val/mmh/cross_entropy: 0.5311, val/total_loss: 0.5311, val/mmh/accuracy: 0.6938, val/mmh/binary_f1: 0.7803, val/mmh/roc_auc: 0.7250, num_updates: 22000, epoch: 3, iterations: 22000, max_updates: 22000, val_time: 03m 486ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.726792
[32m2022-10-18T11:41:51 | mmf.trainers.core.training_loop: [39mStepping into final validation check
[32m2022-10-18T11:41:51 | mmf.utils.checkpoint: [39mRestoring checkpoint
[32m2022-10-18T11:41:51 | mmf.utils.checkpoint: [39mLoading checkpoint
[32m2022-10-18T11:41:54 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-18T11:41:54 | mmf.utils.checkpoint: [39mCurrent num updates: 13000
[32m2022-10-18T11:41:54 | mmf.utils.checkpoint: [39mCurrent iteration: 13000
[32m2022-10-18T11:41:54 | mmf.utils.checkpoint: [39mCurrent epoch: 2
[32m2022-10-18T11:41:55 | mmf.utils.checkpoint: [39mSalvando o modelo final..
[32m2022-10-18T11:41:55 | mmf.trainers.mmf_trainer: [39mStarting inference on val set
[32m2022-10-18T11:41:55 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-18T11:41:55 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False






















100% 313/313 [00:45<00:00,  6.90it/s]
[32m2022-10-18T11:42:40 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-18T11:42:40 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T11:42:40 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, val/mmh/cross_entropy: 0.5137, val/total_loss: 0.5137, val/mmh/accuracy: 0.7410, val/mmh/binary_f1: 0.8421, val/mmh/roc_auc: 0.7268
[32m2022-10-18T11:42:40 | mmf.trainers.callbacks.logistics: [39mFinished run in 05h 30m 54s 250ms
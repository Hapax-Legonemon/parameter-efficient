
[32m2022-10-17T11:40:27 | mmf.utils.checkpoint: [39mLoading checkpoint
[31m[5mWARNING[39m[25m [32m2022-10-17T11:40:31 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-17T11:40:31 | mmf: [39mKey distributed is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-17T11:40:31 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-17T11:40:31 | mmf: [39mKey distributed is not present in registry, returning default value of None
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mPretrained model loaded
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCurrent num updates: 0
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCurrent iteration: 0
[32m2022-10-17T11:40:31 | mmf.utils.checkpoint: [39mCurrent epoch: 0
[32m2022-10-17T11:40:31 | mmf.trainers.mmf_trainer: [39m===== Model x=====
[32m2022-10-17T11:40:31 | mmf.trainers.mmf_trainer: [39mVisualBERT(
  (model): VisualBERTForClassification(
    (bert): VisualBERTBase(
      (embeddings): BertVisioLinguisticEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (token_type_embeddings_visual): Embedding(2, 768)
        (position_embeddings_visual): Embedding(512, 768)
        (projection): Linear(in_features=2048, out_features=768, bias=True)
      )
      (encoder): BertEncoderJit(
        (layer): ModuleList(
          (0): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Sequential(
      (0): BertPredictionHeadTransform(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (1): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-10-17T11:40:31 | mmf.utils.general: [39mTotal Parameters: 112044290. Trained Parameters: 593666
[32m2022-10-17T11:40:31 | mmf.trainers.core.training_loop: [39mStarting training...
[31m[5mWARNING[39m[25m [32m2022-10-17T11:40:33 | py.warnings: [39m/home/smart/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[31m[5mWARNING[39m[25m [32m2022-10-17T11:40:33 | py.warnings: [39m/home/smart/anaconda3/envs/mmf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[32m2022-10-17T11:41:35 | mmf.trainers.callbacks.logistics: [39mprogress: 100/22000, train/mmh/cross_entropy: 0.7436, train/mmh/cross_entropy/avg: 0.7436, train/total_loss: 0.7436, train/total_loss/avg: 0.7436, max mem: 675.0, experiment: mmh, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.59, time: 01m 03s 079ms, time_since_start: 01m 14s 947ms, eta: 05h 02m 18s 427ms
[32m2022-10-17T11:42:37 | mmf.trainers.callbacks.logistics: [39mprogress: 200/22000, train/mmh/cross_entropy: 0.7436, train/mmh/cross_entropy/avg: 0.7624, train/total_loss: 0.7436, train/total_loss/avg: 0.7624, max mem: 675.0, experiment: mmh, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 633ms, time_since_start: 02m 17s 581ms, eta: 04h 58m 47s 841ms
[32m2022-10-17T11:43:40 | mmf.trainers.callbacks.logistics: [39mprogress: 300/22000, train/mmh/cross_entropy: 0.7436, train/mmh/cross_entropy/avg: 0.7139, train/total_loss: 0.7436, train/total_loss/avg: 0.7139, max mem: 675.0, experiment: mmh, epoch: 1, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 878ms, time_since_start: 03m 20s 460ms, eta: 04h 58m 35s 417ms
[32m2022-10-17T11:44:44 | mmf.trainers.callbacks.logistics: [39mprogress: 400/22000, train/mmh/cross_entropy: 0.6743, train/mmh/cross_entropy/avg: 0.7040, train/total_loss: 0.6743, train/total_loss/avg: 0.7040, max mem: 675.0, experiment: mmh, epoch: 1, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 846ms, time_since_start: 04m 24s 306ms, eta: 05h 01m 47s 338ms
[32m2022-10-17T11:45:47 | mmf.trainers.callbacks.logistics: [39mprogress: 500/22000, train/mmh/cross_entropy: 0.6923, train/mmh/cross_entropy/avg: 0.7016, train/total_loss: 0.6923, train/total_loss/avg: 0.7016, max mem: 675.0, experiment: mmh, epoch: 1, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 074ms, time_since_start: 05m 27s 380ms, eta: 04h 56m 45s 607ms
[32m2022-10-17T11:46:50 | mmf.trainers.callbacks.logistics: [39mprogress: 600/22000, train/mmh/cross_entropy: 0.6749, train/mmh/cross_entropy/avg: 0.6972, train/total_loss: 0.6749, train/total_loss/avg: 0.6972, max mem: 675.0, experiment: mmh, epoch: 1, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 1.59, time: 01m 03s 012ms, time_since_start: 06m 30s 393ms, eta: 04h 55m 05s 439ms
[32m2022-10-17T11:47:56 | mmf.trainers.callbacks.logistics: [39mprogress: 700/22000, train/mmh/cross_entropy: 0.6923, train/mmh/cross_entropy/avg: 0.7106, train/total_loss: 0.6923, train/total_loss/avg: 0.7106, max mem: 675.0, experiment: mmh, epoch: 1, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 1.54, time: 01m 05s 978ms, time_since_start: 07m 36s 371ms, eta: 05h 07m 32s 035ms
[32m2022-10-17T11:49:01 | mmf.trainers.callbacks.logistics: [39mprogress: 800/22000, train/mmh/cross_entropy: 0.6923, train/mmh/cross_entropy/avg: 0.7118, train/total_loss: 0.6923, train/total_loss/avg: 0.7118, max mem: 675.0, experiment: mmh, epoch: 1, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 1.56, time: 01m 04s 680ms, time_since_start: 08m 41s 051ms, eta: 05h 04s 072ms
[32m2022-10-17T11:50:05 | mmf.trainers.callbacks.logistics: [39mprogress: 900/22000, train/mmh/cross_entropy: 0.6923, train/mmh/cross_entropy/avg: 0.7033, train/total_loss: 0.6923, train/total_loss/avg: 0.7033, max mem: 675.0, experiment: mmh, epoch: 1, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 1.59, time: 01m 03s 958ms, time_since_start: 09m 45s 009ms, eta: 04h 55m 19s 145ms
[32m2022-10-17T11:51:08 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T11:51:08 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T11:51:09 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T11:51:09 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T11:51:09 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, train/mmh/cross_entropy: 0.6749, train/mmh/cross_entropy/avg: 0.6941, train/total_loss: 0.6749, train/total_loss/avg: 0.6941, max mem: 675.0, experiment: mmh, epoch: 1, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.56, time: 01m 04s 865ms, time_since_start: 10m 49s 875ms, eta: 04h 58m 05s 355ms
[32m2022-10-17T11:51:09 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T11:51:09 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T11:51:09 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T11:54:13 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T11:54:13 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T11:54:13 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T11:54:16 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T11:54:16 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T11:54:20 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T11:54:20 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, val/mmh/cross_entropy: 0.6255, val/total_loss: 0.6255, val/mmh/accuracy: 0.7404, val/mmh/binary_f1: 0.8504, val/mmh/roc_auc: 0.5699, num_updates: 1000, epoch: 1, iterations: 1000, max_updates: 22000, val_time: 03m 10s 162ms, best_update: 1000, best_iteration: 1000, best_val/mmh/roc_auc: 0.569887
[32m2022-10-17T11:55:30 | mmf.trainers.callbacks.logistics: [39mprogress: 1100/22000, train/mmh/cross_entropy: 0.6749, train/mmh/cross_entropy/avg: 0.6865, train/total_loss: 0.6749, train/total_loss/avg: 0.6865, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 1.43, time: 01m 10s 219ms, time_since_start: 15m 10s 259ms, eta: 05h 21m 09s 551ms
[32m2022-10-17T11:56:33 | mmf.trainers.callbacks.logistics: [39mprogress: 1200/22000, train/mmh/cross_entropy: 0.6743, train/mmh/cross_entropy/avg: 0.6848, train/total_loss: 0.6743, train/total_loss/avg: 0.6848, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 1.59, time: 01m 03s 586ms, time_since_start: 16m 13s 845ms, eta: 04h 49m 25s 653ms
[32m2022-10-17T11:57:36 | mmf.trainers.callbacks.logistics: [39mprogress: 1300/22000, train/mmh/cross_entropy: 0.6743, train/mmh/cross_entropy/avg: 0.6736, train/total_loss: 0.6743, train/total_loss/avg: 0.6736, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 980ms, time_since_start: 17m 16s 826ms, eta: 04h 45m 17s 616ms
[32m2022-10-17T11:58:39 | mmf.trainers.callbacks.logistics: [39mprogress: 1400/22000, train/mmh/cross_entropy: 0.6743, train/mmh/cross_entropy/avg: 0.6761, train/total_loss: 0.6743, train/total_loss/avg: 0.6761, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 512ms, time_since_start: 18m 19s 339ms, eta: 04h 41m 48s 371ms
[32m2022-10-17T11:59:41 | mmf.trainers.callbacks.logistics: [39mprogress: 1500/22000, train/mmh/cross_entropy: 0.6743, train/mmh/cross_entropy/avg: 0.6738, train/total_loss: 0.6743, train/total_loss/avg: 0.6738, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 550ms, time_since_start: 19m 21s 890ms, eta: 04h 40m 36s 487ms
[32m2022-10-17T12:00:44 | mmf.trainers.callbacks.logistics: [39mprogress: 1600/22000, train/mmh/cross_entropy: 0.6743, train/mmh/cross_entropy/avg: 0.6740, train/total_loss: 0.6743, train/total_loss/avg: 0.6740, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 351ms, time_since_start: 20m 24s 241ms, eta: 04h 38m 20s 987ms
[32m2022-10-17T12:01:46 | mmf.trainers.callbacks.logistics: [39mprogress: 1700/22000, train/mmh/cross_entropy: 0.6749, train/mmh/cross_entropy/avg: 0.6786, train/total_loss: 0.6749, train/total_loss/avg: 0.6786, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 132ms, time_since_start: 21m 26s 374ms, eta: 04h 36m 867ms
[32m2022-10-17T12:02:48 | mmf.trainers.callbacks.logistics: [39mprogress: 1800/22000, train/mmh/cross_entropy: 0.6743, train/mmh/cross_entropy/avg: 0.6747, train/total_loss: 0.6743, train/total_loss/avg: 0.6747, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 1.61, time: 01m 02s 300ms, time_since_start: 22m 28s 675ms, eta: 04h 35m 23s 671ms
[32m2022-10-17T12:03:50 | mmf.trainers.callbacks.logistics: [39mprogress: 1900/22000, train/mmh/cross_entropy: 0.6743, train/mmh/cross_entropy/avg: 0.6709, train/total_loss: 0.6743, train/total_loss/avg: 0.6709, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 1.61, time: 01m 02s 049ms, time_since_start: 23m 30s 724ms, eta: 04h 32m 55s 648ms
[32m2022-10-17T12:04:53 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T12:04:53 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T12:04:54 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T12:04:57 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T12:04:57 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, train/mmh/cross_entropy: 0.6743, train/mmh/cross_entropy/avg: 0.6728, train/total_loss: 0.6743, train/total_loss/avg: 0.6728, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.52, time: 01m 06s 238ms, time_since_start: 24m 36s 963ms, eta: 04h 49m 54s 264ms
[32m2022-10-17T12:04:57 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T12:04:57 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T12:04:57 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T12:07:57 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T12:07:57 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T12:07:57 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T12:08:00 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T12:08:03 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T12:08:06 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T12:08:06 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, val/mmh/cross_entropy: 0.5733, val/total_loss: 0.5733, val/mmh/accuracy: 0.7438, val/mmh/binary_f1: 0.8531, val/mmh/roc_auc: 0.5705, num_updates: 2000, epoch: 1, iterations: 2000, max_updates: 22000, val_time: 03m 09s 104ms, best_update: 2000, best_iteration: 2000, best_val/mmh/roc_auc: 0.570494
[32m2022-10-17T12:09:11 | mmf.trainers.callbacks.logistics: [39mprogress: 2100/22000, train/mmh/cross_entropy: 0.6661, train/mmh/cross_entropy/avg: 0.6721, train/total_loss: 0.6661, train/total_loss/avg: 0.6721, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 1.56, time: 01m 04s 866ms, time_since_start: 28m 50s 935ms, eta: 04h 42m 28s 752ms
[32m2022-10-17T12:10:14 | mmf.trainers.callbacks.logistics: [39mprogress: 2200/22000, train/mmh/cross_entropy: 0.6569, train/mmh/cross_entropy/avg: 0.6703, train/total_loss: 0.6569, train/total_loss/avg: 0.6703, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 1.59, time: 01m 03s 545ms, time_since_start: 29m 54s 480ms, eta: 04h 35m 20s 109ms
[32m2022-10-17T12:11:17 | mmf.trainers.callbacks.logistics: [39mprogress: 2300/22000, train/mmh/cross_entropy: 0.6569, train/mmh/cross_entropy/avg: 0.6665, train/total_loss: 0.6569, train/total_loss/avg: 0.6665, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 1.61, time: 01m 02s 469ms, time_since_start: 30m 56s 950ms, eta: 04h 29m 18s 524ms
[32m2022-10-17T12:12:19 | mmf.trainers.callbacks.logistics: [39mprogress: 2400/22000, train/mmh/cross_entropy: 0.6569, train/mmh/cross_entropy/avg: 0.6671, train/total_loss: 0.6569, train/total_loss/avg: 0.6671, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 1.61, time: 01m 02s 093ms, time_since_start: 31m 59s 043ms, eta: 04h 26m 19s 529ms
[32m2022-10-17T12:13:22 | mmf.trainers.callbacks.logistics: [39mprogress: 2500/22000, train/mmh/cross_entropy: 0.6569, train/mmh/cross_entropy/avg: 0.6723, train/total_loss: 0.6569, train/total_loss/avg: 0.6723, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 1.59, time: 01m 03s 111ms, time_since_start: 33m 02s 155ms, eta: 04h 29m 18s 754ms
[32m2022-10-17T12:14:26 | mmf.trainers.callbacks.logistics: [39mprogress: 2600/22000, train/mmh/cross_entropy: 0.6415, train/mmh/cross_entropy/avg: 0.6703, train/total_loss: 0.6415, train/total_loss/avg: 0.6703, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 1.56, time: 01m 04s 184ms, time_since_start: 34m 06s 340ms, eta: 04h 32m 29s 219ms
[32m2022-10-17T12:15:30 | mmf.trainers.callbacks.logistics: [39mprogress: 2700/22000, train/mmh/cross_entropy: 0.6415, train/mmh/cross_entropy/avg: 0.6715, train/total_loss: 0.6415, train/total_loss/avg: 0.6715, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 1.59, time: 01m 03s 574ms, time_since_start: 35m 09s 914ms, eta: 04h 28m 30s 466ms
[32m2022-10-17T12:16:32 | mmf.trainers.callbacks.logistics: [39mprogress: 2800/22000, train/mmh/cross_entropy: 0.6415, train/mmh/cross_entropy/avg: 0.6718, train/total_loss: 0.6415, train/total_loss/avg: 0.6718, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 1.61, time: 01m 02s 688ms, time_since_start: 36m 12s 603ms, eta: 04h 23m 23s 409ms
[32m2022-10-17T12:17:35 | mmf.trainers.callbacks.logistics: [39mprogress: 2900/22000, train/mmh/cross_entropy: 0.6421, train/mmh/cross_entropy/avg: 0.6708, train/total_loss: 0.6421, train/total_loss/avg: 0.6708, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 1.61, time: 01m 02s 820ms, time_since_start: 37m 15s 423ms, eta: 04h 22m 34s 291ms
[32m2022-10-17T12:18:38 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T12:18:38 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T12:18:39 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T12:18:42 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T12:18:42 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, train/mmh/cross_entropy: 0.6569, train/mmh/cross_entropy/avg: 0.6713, train/total_loss: 0.6569, train/total_loss/avg: 0.6713, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.52, time: 01m 06s 530ms, time_since_start: 38m 21s 954ms, eta: 04h 36m 37s 504ms
[32m2022-10-17T12:18:42 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T12:18:42 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T12:18:42 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T12:21:35 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T12:21:35 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T12:21:35 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T12:21:38 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T12:21:40 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T12:21:43 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T12:21:43 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, val/mmh/cross_entropy: 0.6165, val/total_loss: 0.6165, val/mmh/accuracy: 0.7440, val/mmh/binary_f1: 0.8530, val/mmh/roc_auc: 0.6029, num_updates: 3000, epoch: 1, iterations: 3000, max_updates: 22000, val_time: 03m 01s 599ms, best_update: 3000, best_iteration: 3000, best_val/mmh/roc_auc: 0.602912
[32m2022-10-17T12:22:52 | mmf.trainers.callbacks.logistics: [39mprogress: 3100/22000, train/mmh/cross_entropy: 0.6661, train/mmh/cross_entropy/avg: 0.6741, train/total_loss: 0.6661, train/total_loss/avg: 0.6741, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 1.45, time: 01m 09s 155ms, time_since_start: 42m 32s 716ms, eta: 04h 46m 01s 527ms
[32m2022-10-17T12:23:59 | mmf.trainers.callbacks.logistics: [39mprogress: 3200/22000, train/mmh/cross_entropy: 0.6569, train/mmh/cross_entropy/avg: 0.6735, train/total_loss: 0.6569, train/total_loss/avg: 0.6735, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 1.49, time: 01m 07s 061ms, time_since_start: 43m 39s 778ms, eta: 04h 35m 53s 853ms
[32m2022-10-17T12:25:04 | mmf.trainers.callbacks.logistics: [39mprogress: 3300/22000, train/mmh/cross_entropy: 0.6646, train/mmh/cross_entropy/avg: 0.6733, train/total_loss: 0.6646, train/total_loss/avg: 0.6733, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 1.56, time: 01m 04s 749ms, time_since_start: 44m 44s 527ms, eta: 04h 24m 58s 022ms
[32m2022-10-17T12:26:07 | mmf.trainers.callbacks.logistics: [39mprogress: 3400/22000, train/mmh/cross_entropy: 0.6646, train/mmh/cross_entropy/avg: 0.6743, train/total_loss: 0.6646, train/total_loss/avg: 0.6743, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 1.61, time: 01m 02s 874ms, time_since_start: 45m 47s 402ms, eta: 04h 15m 55s 186ms
[32m2022-10-17T12:27:10 | mmf.trainers.callbacks.logistics: [39mprogress: 3500/22000, train/mmh/cross_entropy: 0.6646, train/mmh/cross_entropy/avg: 0.6739, train/total_loss: 0.6646, train/total_loss/avg: 0.6739, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 1.61, time: 01m 02s 525ms, time_since_start: 46m 49s 928ms, eta: 04h 13m 07s 831ms
[32m2022-10-17T12:28:13 | mmf.trainers.callbacks.logistics: [39mprogress: 3600/22000, train/mmh/cross_entropy: 0.6591, train/mmh/cross_entropy/avg: 0.6726, train/total_loss: 0.6591, train/total_loss/avg: 0.6726, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 1.59, time: 01m 03s 240ms, time_since_start: 47m 53s 168ms, eta: 04h 14m 38s 330ms
[32m2022-10-17T12:29:17 | mmf.trainers.callbacks.logistics: [39mprogress: 3700/22000, train/mmh/cross_entropy: 0.6591, train/mmh/cross_entropy/avg: 0.6730, train/total_loss: 0.6591, train/total_loss/avg: 0.6730, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 1.59, time: 01m 03s 776ms, time_since_start: 48m 56s 945ms, eta: 04h 15m 24s 237ms
[32m2022-10-17T12:30:20 | mmf.trainers.callbacks.logistics: [39mprogress: 3800/22000, train/mmh/cross_entropy: 0.6591, train/mmh/cross_entropy/avg: 0.6714, train/total_loss: 0.6591, train/total_loss/avg: 0.6714, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 1.59, time: 01m 03s 108ms, time_since_start: 50m 053ms, eta: 04h 11m 20s 697ms
[32m2022-10-17T12:31:22 | mmf.trainers.callbacks.logistics: [39mprogress: 3900/22000, train/mmh/cross_entropy: 0.6591, train/mmh/cross_entropy/avg: 0.6670, train/total_loss: 0.6591, train/total_loss/avg: 0.6670, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 1.61, time: 01m 02s 434ms, time_since_start: 51m 02s 488ms, eta: 04h 07m 17s 790ms
[32m2022-10-17T12:32:25 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T12:32:25 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T12:32:26 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T12:32:29 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T12:32:29 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, train/mmh/cross_entropy: 0.6591, train/mmh/cross_entropy/avg: 0.6678, train/total_loss: 0.6591, train/total_loss/avg: 0.6678, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.52, time: 01m 06s 450ms, time_since_start: 52m 08s 939ms, eta: 04h 21m 44s 947ms
[32m2022-10-17T12:32:29 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T12:32:29 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T12:32:29 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T12:35:30 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T12:35:30 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T12:35:30 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T12:35:33 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T12:35:36 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T12:35:39 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T12:35:39 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, val/mmh/cross_entropy: 0.6423, val/total_loss: 0.6423, val/mmh/accuracy: 0.7170, val/mmh/binary_f1: 0.8280, val/mmh/roc_auc: 0.6088, num_updates: 4000, epoch: 1, iterations: 4000, max_updates: 22000, val_time: 03m 10s 212ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.608790
[32m2022-10-17T12:36:43 | mmf.trainers.callbacks.logistics: [39mprogress: 4100/22000, train/mmh/cross_entropy: 0.6646, train/mmh/cross_entropy/avg: 0.6688, train/total_loss: 0.6646, train/total_loss/avg: 0.6688, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 1.56, time: 01m 04s 260ms, time_since_start: 56m 23s 413ms, eta: 04h 11m 42s 934ms
[32m2022-10-17T12:37:45 | mmf.trainers.callbacks.logistics: [39mprogress: 4200/22000, train/mmh/cross_entropy: 0.6799, train/mmh/cross_entropy/avg: 0.6693, train/total_loss: 0.6799, train/total_loss/avg: 0.6693, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 236ms, time_since_start: 57m 25s 650ms, eta: 04h 02m 25s 650ms
[32m2022-10-17T12:38:48 | mmf.trainers.callbacks.logistics: [39mprogress: 4300/22000, train/mmh/cross_entropy: 0.6799, train/mmh/cross_entropy/avg: 0.6689, train/total_loss: 0.6799, train/total_loss/avg: 0.6689, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 772ms, time_since_start: 58m 28s 423ms, eta: 04h 03m 08s 428ms
[32m2022-10-17T12:39:52 | mmf.trainers.callbacks.logistics: [39mprogress: 4400/22000, train/mmh/cross_entropy: 0.6799, train/mmh/cross_entropy/avg: 0.6699, train/total_loss: 0.6799, train/total_loss/avg: 0.6699, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 1.56, time: 01m 04s 196ms, time_since_start: 59m 32s 620ms, eta: 04h 07m 15s 117ms
[32m2022-10-17T12:40:55 | mmf.trainers.callbacks.logistics: [39mprogress: 4500/22000, train/mmh/cross_entropy: 0.6646, train/mmh/cross_entropy/avg: 0.6680, train/total_loss: 0.6646, train/total_loss/avg: 0.6680, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 776ms, time_since_start: 01h 35s 396ms, eta: 04h 24s 436ms
[32m2022-10-17T12:41:57 | mmf.trainers.callbacks.logistics: [39mprogress: 4600/22000, train/mmh/cross_entropy: 0.6646, train/mmh/cross_entropy/avg: 0.6679, train/total_loss: 0.6646, train/total_loss/avg: 0.6679, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 373ms, time_since_start: 01h 01m 37s 770ms, eta: 03h 57m 30s 045ms
[32m2022-10-17T12:43:00 | mmf.trainers.callbacks.logistics: [39mprogress: 4700/22000, train/mmh/cross_entropy: 0.6646, train/mmh/cross_entropy/avg: 0.6722, train/total_loss: 0.6646, train/total_loss/avg: 0.6722, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 195ms, time_since_start: 01h 02m 39s 966ms, eta: 03h 55m 27s 720ms
[32m2022-10-17T12:44:02 | mmf.trainers.callbacks.logistics: [39mprogress: 4800/22000, train/mmh/cross_entropy: 0.6646, train/mmh/cross_entropy/avg: 0.6747, train/total_loss: 0.6646, train/total_loss/avg: 0.6747, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 426ms, time_since_start: 01h 03m 42s 392ms, eta: 03h 54m 58s 053ms
[32m2022-10-17T12:45:06 | mmf.trainers.callbacks.logistics: [39mprogress: 4900/22000, train/mmh/cross_entropy: 0.6646, train/mmh/cross_entropy/avg: 0.6736, train/total_loss: 0.6646, train/total_loss/avg: 0.6736, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 1.59, time: 01m 03s 598ms, time_since_start: 01h 04m 45s 991ms, eta: 03h 57m 59s 379ms
[32m2022-10-17T12:46:08 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T12:46:08 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T12:46:09 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T12:46:12 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T12:46:12 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, train/mmh/cross_entropy: 0.6631, train/mmh/cross_entropy/avg: 0.6729, train/total_loss: 0.6631, train/total_loss/avg: 0.6729, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.52, time: 01m 06s 029ms, time_since_start: 01h 05m 52s 021ms, eta: 04h 05m 38s 546ms
[32m2022-10-17T12:46:12 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T12:46:12 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T12:46:12 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T12:49:11 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T12:49:11 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T12:49:11 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T12:49:14 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T12:49:16 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T12:49:16 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, val/mmh/cross_entropy: 0.6282, val/total_loss: 0.6282, val/mmh/accuracy: 0.7312, val/mmh/binary_f1: 0.8409, val/mmh/roc_auc: 0.5913, num_updates: 5000, epoch: 1, iterations: 5000, max_updates: 22000, val_time: 03m 04s 834ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.608790
[32m2022-10-17T12:50:22 | mmf.trainers.callbacks.logistics: [39mprogress: 5100/22000, train/mmh/cross_entropy: 0.6631, train/mmh/cross_entropy/avg: 0.6750, train/total_loss: 0.6631, train/total_loss/avg: 0.6750, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 1.54, time: 01m 05s 889ms, time_since_start: 01h 10m 02s 748ms, eta: 04h 03m 40s 708ms
[32m2022-10-17T12:51:25 | mmf.trainers.callbacks.logistics: [39mprogress: 5200/22000, train/mmh/cross_entropy: 0.6631, train/mmh/cross_entropy/avg: 0.6746, train/total_loss: 0.6631, train/total_loss/avg: 0.6746, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 911ms, time_since_start: 01h 11m 05s 659ms, eta: 03h 51m 17s 164ms
[32m2022-10-17T12:52:28 | mmf.trainers.callbacks.logistics: [39mprogress: 5300/22000, train/mmh/cross_entropy: 0.6631, train/mmh/cross_entropy/avg: 0.6769, train/total_loss: 0.6631, train/total_loss/avg: 0.6769, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 285ms, time_since_start: 01h 12m 07s 945ms, eta: 03h 47m 37s 471ms
[32m2022-10-17T12:53:30 | mmf.trainers.callbacks.logistics: [39mprogress: 5400/22000, train/mmh/cross_entropy: 0.6631, train/mmh/cross_entropy/avg: 0.6766, train/total_loss: 0.6631, train/total_loss/avg: 0.6766, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 126ms, time_since_start: 01h 13m 10s 071ms, eta: 03h 45m 40s 871ms
[32m2022-10-17T12:54:32 | mmf.trainers.callbacks.logistics: [39mprogress: 5500/22000, train/mmh/cross_entropy: 0.6631, train/mmh/cross_entropy/avg: 0.6749, train/total_loss: 0.6631, train/total_loss/avg: 0.6749, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 088ms, time_since_start: 01h 14m 12s 159ms, eta: 03h 44m 11s 091ms
[32m2022-10-17T12:55:34 | mmf.trainers.callbacks.logistics: [39mprogress: 5600/22000, train/mmh/cross_entropy: 0.6642, train/mmh/cross_entropy/avg: 0.6747, train/total_loss: 0.6642, train/total_loss/avg: 0.6747, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 592ms, time_since_start: 01h 15m 14s 752ms, eta: 03h 44m 38s 247ms
[32m2022-10-17T12:56:36 | mmf.trainers.callbacks.logistics: [39mprogress: 5700/22000, train/mmh/cross_entropy: 0.6642, train/mmh/cross_entropy/avg: 0.6750, train/total_loss: 0.6642, train/total_loss/avg: 0.6750, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 010ms, time_since_start: 01h 16m 16s 763ms, eta: 03h 41m 11s 489ms
[32m2022-10-17T12:57:39 | mmf.trainers.callbacks.logistics: [39mprogress: 5800/22000, train/mmh/cross_entropy: 0.6657, train/mmh/cross_entropy/avg: 0.6754, train/total_loss: 0.6657, train/total_loss/avg: 0.6754, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 380ms, time_since_start: 01h 17m 19s 143ms, eta: 03h 41m 08s 610ms
[32m2022-10-17T12:58:41 | mmf.trainers.callbacks.logistics: [39mprogress: 5900/22000, train/mmh/cross_entropy: 0.6714, train/mmh/cross_entropy/avg: 0.6753, train/total_loss: 0.6714, train/total_loss/avg: 0.6753, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 851ms, time_since_start: 01h 18m 20s 994ms, eta: 03h 37m 55s 007ms
[32m2022-10-17T12:59:42 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T12:59:42 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T12:59:43 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T12:59:46 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T12:59:46 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, train/mmh/cross_entropy: 0.6714, train/mmh/cross_entropy/avg: 0.6762, train/total_loss: 0.6714, train/total_loss/avg: 0.6762, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.54, time: 01m 05s 203ms, time_since_start: 01h 19m 26s 198ms, eta: 03h 48m 17s 994ms
[32m2022-10-17T12:59:46 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T12:59:46 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T12:59:46 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T13:02:39 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T13:02:39 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T13:02:39 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T13:02:42 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T13:02:45 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T13:02:48 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T13:02:48 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, val/mmh/cross_entropy: 0.5697, val/total_loss: 0.5697, val/mmh/accuracy: 0.7438, val/mmh/binary_f1: 0.8531, val/mmh/roc_auc: 0.6127, num_updates: 6000, epoch: 1, iterations: 6000, max_updates: 22000, val_time: 03m 02s 017ms, best_update: 6000, best_iteration: 6000, best_val/mmh/roc_auc: 0.612748
[32m2022-10-17T13:03:51 | mmf.trainers.callbacks.logistics: [39mprogress: 6100/22000, train/mmh/cross_entropy: 0.6714, train/mmh/cross_entropy/avg: 0.6772, train/total_loss: 0.6714, train/total_loss/avg: 0.6772, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 1.59, time: 01m 03s 164ms, time_since_start: 01h 23m 31s 382ms, eta: 03h 39m 46s 693ms
[32m2022-10-17T13:04:54 | mmf.trainers.callbacks.logistics: [39mprogress: 6200/22000, train/mmh/cross_entropy: 0.6714, train/mmh/cross_entropy/avg: 0.6775, train/total_loss: 0.6714, train/total_loss/avg: 0.6775, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 728ms, time_since_start: 01h 24m 34s 111ms, eta: 03h 36m 53s 260ms
[32m2022-10-17T13:05:56 | mmf.trainers.callbacks.logistics: [39mprogress: 6300/22000, train/mmh/cross_entropy: 0.6882, train/mmh/cross_entropy/avg: 0.6776, train/total_loss: 0.6882, train/total_loss/avg: 0.6776, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 308ms, time_since_start: 01h 25m 36s 419ms, eta: 03h 34m 04s 326ms
[32m2022-10-17T13:06:58 | mmf.trainers.callbacks.logistics: [39mprogress: 6400/22000, train/mmh/cross_entropy: 0.6882, train/mmh/cross_entropy/avg: 0.6795, train/total_loss: 0.6882, train/total_loss/avg: 0.6795, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 918ms, time_since_start: 01h 26m 38s 338ms, eta: 03h 31m 22s 619ms
[32m2022-10-17T13:08:00 | mmf.trainers.callbacks.logistics: [39mprogress: 6500/22000, train/mmh/cross_entropy: 0.6927, train/mmh/cross_entropy/avg: 0.6803, train/total_loss: 0.6927, train/total_loss/avg: 0.6803, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 933ms, time_since_start: 01h 27m 40s 271ms, eta: 03h 30m 04s 320ms
[32m2022-10-17T13:09:02 | mmf.trainers.callbacks.logistics: [39mprogress: 6600/22000, train/mmh/cross_entropy: 0.6927, train/mmh/cross_entropy/avg: 0.6797, train/total_loss: 0.6927, train/total_loss/avg: 0.6797, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 803ms, time_since_start: 01h 28m 42s 075ms, eta: 03h 28m 16s 867ms
[32m2022-10-17T13:10:03 | mmf.trainers.callbacks.logistics: [39mprogress: 6700/22000, train/mmh/cross_entropy: 0.6882, train/mmh/cross_entropy/avg: 0.6773, train/total_loss: 0.6882, train/total_loss/avg: 0.6773, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 620ms, time_since_start: 01h 29m 43s 695ms, eta: 03h 26m 18s 886ms
[32m2022-10-17T13:11:05 | mmf.trainers.callbacks.logistics: [39mprogress: 6800/22000, train/mmh/cross_entropy: 0.6882, train/mmh/cross_entropy/avg: 0.6775, train/total_loss: 0.6882, train/total_loss/avg: 0.6775, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 640ms, time_since_start: 01h 30m 45s 336ms, eta: 03h 25m 01s 913ms
[32m2022-10-17T13:12:07 | mmf.trainers.callbacks.logistics: [39mprogress: 6900/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6784, train/total_loss: 0.6883, train/total_loss/avg: 0.6784, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 893ms, time_since_start: 01h 31m 47s 229ms, eta: 03h 24m 31s 157ms
[32m2022-10-17T13:13:08 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T13:13:08 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T13:13:09 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T13:13:12 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T13:13:12 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6778, train/total_loss: 0.6883, train/total_loss/avg: 0.6778, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.56, time: 01m 04s 807ms, time_since_start: 01h 32m 52s 037ms, eta: 03h 32m 43s 877ms
[32m2022-10-17T13:13:12 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T13:13:12 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T13:13:12 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T13:16:12 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T13:16:12 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T13:16:12 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T13:16:15 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T13:16:18 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T13:16:18 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, val/mmh/cross_entropy: 0.6439, val/total_loss: 0.6439, val/mmh/accuracy: 0.7158, val/mmh/binary_f1: 0.8245, val/mmh/roc_auc: 0.6108, num_updates: 7000, epoch: 1, iterations: 7000, max_updates: 22000, val_time: 03m 06s 195ms, best_update: 6000, best_iteration: 6000, best_val/mmh/roc_auc: 0.612748
[32m2022-10-17T13:17:20 | mmf.trainers.callbacks.logistics: [39mprogress: 7100/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6796, train/total_loss: 0.6883, train/total_loss/avg: 0.6796, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 806ms, time_since_start: 01h 37m 041ms, eta: 03h 21m 31s 702ms
[32m2022-10-17T13:18:22 | mmf.trainers.callbacks.logistics: [39mprogress: 7200/22000, train/mmh/cross_entropy: 0.6883, train/mmh/cross_entropy/avg: 0.6792, train/total_loss: 0.6883, train/total_loss/avg: 0.6792, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 895ms, time_since_start: 01h 38m 01s 937ms, eta: 03h 20m 27s 707ms
[32m2022-10-17T13:19:23 | mmf.trainers.callbacks.logistics: [39mprogress: 7300/22000, train/mmh/cross_entropy: 0.6882, train/mmh/cross_entropy/avg: 0.6784, train/total_loss: 0.6882, train/total_loss/avg: 0.6784, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 800ms, time_since_start: 01h 39m 03s 737ms, eta: 03h 18m 48s 236ms
[32m2022-10-17T13:20:25 | mmf.trainers.callbacks.logistics: [39mprogress: 7400/22000, train/mmh/cross_entropy: 0.6882, train/mmh/cross_entropy/avg: 0.6780, train/total_loss: 0.6882, train/total_loss/avg: 0.6780, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 914ms, time_since_start: 01h 40m 05s 652ms, eta: 03h 17m 48s 877ms
[32m2022-10-17T13:21:27 | mmf.trainers.callbacks.logistics: [39mprogress: 7500/22000, train/mmh/cross_entropy: 0.6882, train/mmh/cross_entropy/avg: 0.6774, train/total_loss: 0.6882, train/total_loss/avg: 0.6774, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 672ms, time_since_start: 01h 41m 07s 324ms, eta: 03h 15m 41s 514ms
[32m2022-10-17T13:22:29 | mmf.trainers.callbacks.logistics: [39mprogress: 7600/22000, train/mmh/cross_entropy: 0.6882, train/mmh/cross_entropy/avg: 0.6770, train/total_loss: 0.6882, train/total_loss/avg: 0.6770, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 1.61, time: 01m 02s 190ms, time_since_start: 01h 42m 09s 515ms, eta: 03h 15m 58s 468ms
[32m2022-10-17T13:23:31 | mmf.trainers.callbacks.logistics: [39mprogress: 7700/22000, train/mmh/cross_entropy: 0.6714, train/mmh/cross_entropy/avg: 0.6767, train/total_loss: 0.6714, train/total_loss/avg: 0.6767, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 700ms, time_since_start: 01h 43m 11s 216ms, eta: 03h 13m 04s 877ms
[32m2022-10-17T13:24:33 | mmf.trainers.callbacks.logistics: [39mprogress: 7800/22000, train/mmh/cross_entropy: 0.6714, train/mmh/cross_entropy/avg: 0.6773, train/total_loss: 0.6714, train/total_loss/avg: 0.6773, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 778ms, time_since_start: 01h 44m 12s 994ms, eta: 03h 11m 58s 276ms
[32m2022-10-17T13:25:34 | mmf.trainers.callbacks.logistics: [39mprogress: 7900/22000, train/mmh/cross_entropy: 0.6882, train/mmh/cross_entropy/avg: 0.6779, train/total_loss: 0.6882, train/total_loss/avg: 0.6779, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 1.64, time: 01m 01s 817ms, time_since_start: 01h 45m 14s 811ms, eta: 03h 10m 44s 420ms
[32m2022-10-17T13:26:36 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T13:26:36 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T13:26:37 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T13:26:40 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T13:26:40 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, train/mmh/cross_entropy: 0.6592, train/mmh/cross_entropy/avg: 0.6771, train/total_loss: 0.6592, train/total_loss/avg: 0.6771, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.54, time: 01m 05s 112ms, time_since_start: 01h 46m 19s 923ms, eta: 03h 19m 28s 985ms
[32m2022-10-17T13:26:40 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T13:26:40 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T13:26:40 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T13:29:40 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T13:29:40 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T13:29:40 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T13:29:43 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T13:29:46 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T13:29:49 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T13:29:49 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, val/mmh/cross_entropy: 0.5930, val/total_loss: 0.5930, val/mmh/accuracy: 0.7438, val/mmh/binary_f1: 0.8531, val/mmh/roc_auc: 0.6179, num_updates: 8000, epoch: 1, iterations: 8000, max_updates: 22000, val_time: 03m 09s 135ms, best_update: 8000, best_iteration: 8000, best_val/mmh/roc_auc: 0.617863
[32m2022-10-17T13:30:53 | mmf.trainers.callbacks.logistics: [39mprogress: 8100/22000, train/mmh/cross_entropy: 0.6540, train/mmh/cross_entropy/avg: 0.6768, train/total_loss: 0.6540, train/total_loss/avg: 0.6768, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 1.56, time: 01m 04s 107ms, time_since_start: 01h 50m 33s 168ms, eta: 03h 14m 59s 994ms
[32m2022-10-17T13:31:55 | mmf.trainers.callbacks.logistics: [39mprogress: 8200/22000, train/mmh/cross_entropy: 0.6540, train/mmh/cross_entropy/avg: 0.6770, train/total_loss: 0.6540, train/total_loss/avg: 0.6770, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 496ms, time_since_start: 01h 51m 35s 665ms, eta: 03h 08m 44s 072ms
[32m2022-10-17T13:32:57 | mmf.trainers.callbacks.logistics: [39mprogress: 8300/22000, train/mmh/cross_entropy: 0.6523, train/mmh/cross_entropy/avg: 0.6767, train/total_loss: 0.6523, train/total_loss/avg: 0.6767, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 065ms, time_since_start: 01h 52m 37s 730ms, eta: 03h 06m 04s 409ms
[32m2022-10-17T13:33:59 | mmf.trainers.callbacks.logistics: [39mprogress: 8400/22000, train/mmh/cross_entropy: 0.6523, train/mmh/cross_entropy/avg: 0.6769, train/total_loss: 0.6523, train/total_loss/avg: 0.6769, max mem: 691.0, experiment: mmh, epoch: 1, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 175ms, time_since_start: 01h 53m 39s 906ms, eta: 03h 05m 02s 563ms
[32m2022-10-17T13:35:01 | mmf.trainers.callbacks.logistics: [39mprogress: 8500/22000, train/mmh/cross_entropy: 0.6512, train/mmh/cross_entropy/avg: 0.6764, train/total_loss: 0.6512, train/total_loss/avg: 0.6764, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 248ms, time_since_start: 01h 54m 41s 154ms, eta: 03h 56s 585ms
[32m2022-10-17T13:36:02 | mmf.trainers.callbacks.logistics: [39mprogress: 8600/22000, train/mmh/cross_entropy: 0.6523, train/mmh/cross_entropy/avg: 0.6766, train/total_loss: 0.6523, train/total_loss/avg: 0.6766, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 382ms, time_since_start: 01h 55m 42s 537ms, eta: 02h 59m 59s 805ms
[32m2022-10-17T13:37:04 | mmf.trainers.callbacks.logistics: [39mprogress: 8700/22000, train/mmh/cross_entropy: 0.6540, train/mmh/cross_entropy/avg: 0.6766, train/total_loss: 0.6540, train/total_loss/avg: 0.6766, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 541ms, time_since_start: 01h 56m 44s 078ms, eta: 02h 59m 06s 911ms
[32m2022-10-17T13:38:06 | mmf.trainers.callbacks.logistics: [39mprogress: 8800/22000, train/mmh/cross_entropy: 0.6523, train/mmh/cross_entropy/avg: 0.6761, train/total_loss: 0.6523, train/total_loss/avg: 0.6761, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 213ms, time_since_start: 01h 57m 46s 292ms, eta: 02h 59m 42s 657ms
[32m2022-10-17T13:39:09 | mmf.trainers.callbacks.logistics: [39mprogress: 8900/22000, train/mmh/cross_entropy: 0.6512, train/mmh/cross_entropy/avg: 0.6749, train/total_loss: 0.6512, train/total_loss/avg: 0.6749, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 868ms, time_since_start: 01h 58m 49s 161ms, eta: 03h 13s 606ms
[32m2022-10-17T13:40:12 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T13:40:12 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T13:40:13 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T13:40:15 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T13:40:15 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, train/mmh/cross_entropy: 0.6512, train/mmh/cross_entropy/avg: 0.6742, train/total_loss: 0.6512, train/total_loss/avg: 0.6742, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 1.52, time: 01m 06s 701ms, time_since_start: 01h 59m 55s 862ms, eta: 03h 09m 45s 255ms
[32m2022-10-17T13:40:15 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T13:40:15 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T13:40:15 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T13:43:15 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T13:43:15 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T13:43:15 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T13:43:18 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T13:43:20 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T13:43:23 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T13:43:23 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, val/mmh/cross_entropy: 0.6081, val/total_loss: 0.6081, val/mmh/accuracy: 0.7418, val/mmh/binary_f1: 0.8514, val/mmh/roc_auc: 0.6212, num_updates: 9000, epoch: 2, iterations: 9000, max_updates: 22000, val_time: 03m 07s 875ms, best_update: 9000, best_iteration: 9000, best_val/mmh/roc_auc: 0.621179
[32m2022-10-17T13:44:33 | mmf.trainers.callbacks.logistics: [39mprogress: 9100/22000, train/mmh/cross_entropy: 0.6469, train/mmh/cross_entropy/avg: 0.6738, train/total_loss: 0.6469, train/total_loss/avg: 0.6738, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 1.45, time: 01m 09s 666ms, time_since_start: 02h 04m 13s 406ms, eta: 03h 16m 39s 920ms
[32m2022-10-17T13:45:38 | mmf.trainers.callbacks.logistics: [39mprogress: 9200/22000, train/mmh/cross_entropy: 0.6469, train/mmh/cross_entropy/avg: 0.6738, train/total_loss: 0.6469, train/total_loss/avg: 0.6738, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 1.54, time: 01m 05s 212ms, time_since_start: 02h 05m 18s 619ms, eta: 03h 02m 39s 846ms
[32m2022-10-17T13:46:42 | mmf.trainers.callbacks.logistics: [39mprogress: 9300/22000, train/mmh/cross_entropy: 0.6512, train/mmh/cross_entropy/avg: 0.6736, train/total_loss: 0.6512, train/total_loss/avg: 0.6736, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 1.59, time: 01m 03s 885ms, time_since_start: 02h 06m 22s 504ms, eta: 02h 57m 32s 919ms
[32m2022-10-17T13:47:45 | mmf.trainers.callbacks.logistics: [39mprogress: 9400/22000, train/mmh/cross_entropy: 0.6523, train/mmh/cross_entropy/avg: 0.6740, train/total_loss: 0.6523, train/total_loss/avg: 0.6740, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 821ms, time_since_start: 02h 07m 25s 325ms, eta: 02h 53m 13s 041ms
[32m2022-10-17T13:48:47 | mmf.trainers.callbacks.logistics: [39mprogress: 9500/22000, train/mmh/cross_entropy: 0.6523, train/mmh/cross_entropy/avg: 0.6734, train/total_loss: 0.6523, train/total_loss/avg: 0.6734, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 378ms, time_since_start: 02h 08m 27s 704ms, eta: 02h 50m 37s 897ms
[32m2022-10-17T13:49:50 | mmf.trainers.callbacks.logistics: [39mprogress: 9600/22000, train/mmh/cross_entropy: 0.6523, train/mmh/cross_entropy/avg: 0.6731, train/total_loss: 0.6523, train/total_loss/avg: 0.6731, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 522ms, time_since_start: 02h 09m 30s 227ms, eta: 02h 49m 39s 495ms
[32m2022-10-17T13:50:52 | mmf.trainers.callbacks.logistics: [39mprogress: 9700/22000, train/mmh/cross_entropy: 0.6523, train/mmh/cross_entropy/avg: 0.6734, train/total_loss: 0.6523, train/total_loss/avg: 0.6734, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 557ms, time_since_start: 02h 10m 32s 784ms, eta: 02h 48m 22s 915ms
[32m2022-10-17T13:51:55 | mmf.trainers.callbacks.logistics: [39mprogress: 9800/22000, train/mmh/cross_entropy: 0.6512, train/mmh/cross_entropy/avg: 0.6730, train/total_loss: 0.6512, train/total_loss/avg: 0.6730, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 407ms, time_since_start: 02h 11m 35s 191ms, eta: 02h 46m 36s 794ms
[32m2022-10-17T13:52:57 | mmf.trainers.callbacks.logistics: [39mprogress: 9900/22000, train/mmh/cross_entropy: 0.6509, train/mmh/cross_entropy/avg: 0.6723, train/total_loss: 0.6509, train/total_loss/avg: 0.6723, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 958ms, time_since_start: 02h 12m 37s 150ms, eta: 02h 44m 03s 497ms
[32m2022-10-17T13:53:58 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T13:53:58 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T13:53:59 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T13:54:02 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T13:54:02 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, train/mmh/cross_entropy: 0.6509, train/mmh/cross_entropy/avg: 0.6718, train/total_loss: 0.6509, train/total_loss/avg: 0.6718, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 1.54, time: 01m 05s 068ms, time_since_start: 02h 13m 42s 218ms, eta: 02h 50m 52s 216ms
[32m2022-10-17T13:54:02 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T13:54:02 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T13:54:02 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T13:57:03 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T13:57:03 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T13:57:03 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T13:57:06 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T13:57:09 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T13:57:12 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T13:57:12 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, val/mmh/cross_entropy: 0.5899, val/total_loss: 0.5899, val/mmh/accuracy: 0.7438, val/mmh/binary_f1: 0.8530, val/mmh/roc_auc: 0.6222, num_updates: 10000, epoch: 2, iterations: 10000, max_updates: 22000, val_time: 03m 10s 014ms, best_update: 10000, best_iteration: 10000, best_val/mmh/roc_auc: 0.622159
[32m2022-10-17T13:58:14 | mmf.trainers.callbacks.logistics: [39mprogress: 10100/22000, train/mmh/cross_entropy: 0.6409, train/mmh/cross_entropy/avg: 0.6714, train/total_loss: 0.6409, train/total_loss/avg: 0.6714, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 049ms, time_since_start: 02h 17m 54s 288ms, eta: 02h 41m 35s 124ms
[32m2022-10-17T13:59:16 | mmf.trainers.callbacks.logistics: [39mprogress: 10200/22000, train/mmh/cross_entropy: 0.6409, train/mmh/cross_entropy/avg: 0.6712, train/total_loss: 0.6409, train/total_loss/avg: 0.6712, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 799ms, time_since_start: 02h 18m 56s 088ms, eta: 02h 39m 34s 914ms
[32m2022-10-17T14:00:18 | mmf.trainers.callbacks.logistics: [39mprogress: 10300/22000, train/mmh/cross_entropy: 0.6385, train/mmh/cross_entropy/avg: 0.6708, train/total_loss: 0.6385, train/total_loss/avg: 0.6708, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 089ms, time_since_start: 02h 19m 58s 178ms, eta: 02h 38m 58s 199ms
[32m2022-10-17T14:01:20 | mmf.trainers.callbacks.logistics: [39mprogress: 10400/22000, train/mmh/cross_entropy: 0.6385, train/mmh/cross_entropy/avg: 0.6709, train/total_loss: 0.6385, train/total_loss/avg: 0.6709, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 158ms, time_since_start: 02h 21m 336ms, eta: 02h 37m 47s 213ms
[32m2022-10-17T14:02:22 | mmf.trainers.callbacks.logistics: [39mprogress: 10500/22000, train/mmh/cross_entropy: 0.6330, train/mmh/cross_entropy/avg: 0.6705, train/total_loss: 0.6330, train/total_loss/avg: 0.6705, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 975ms, time_since_start: 02h 22m 02s 312ms, eta: 02h 35m 58s 019ms
[32m2022-10-17T14:03:24 | mmf.trainers.callbacks.logistics: [39mprogress: 10600/22000, train/mmh/cross_entropy: 0.6330, train/mmh/cross_entropy/avg: 0.6702, train/total_loss: 0.6330, train/total_loss/avg: 0.6702, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 110ms, time_since_start: 02h 23m 04s 423ms, eta: 02h 34m 56s 889ms
[32m2022-10-17T14:04:26 | mmf.trainers.callbacks.logistics: [39mprogress: 10700/22000, train/mmh/cross_entropy: 0.6330, train/mmh/cross_entropy/avg: 0.6701, train/total_loss: 0.6330, train/total_loss/avg: 0.6701, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 1.61, time: 01m 02s 172ms, time_since_start: 02h 24m 06s 595ms, eta: 02h 33m 44s 533ms
[32m2022-10-17T14:05:28 | mmf.trainers.callbacks.logistics: [39mprogress: 10800/22000, train/mmh/cross_entropy: 0.6385, train/mmh/cross_entropy/avg: 0.6700, train/total_loss: 0.6385, train/total_loss/avg: 0.6700, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 831ms, time_since_start: 02h 25m 08s 427ms, eta: 02h 31m 32s 712ms
[32m2022-10-17T14:06:30 | mmf.trainers.callbacks.logistics: [39mprogress: 10900/22000, train/mmh/cross_entropy: 0.6386, train/mmh/cross_entropy/avg: 0.6697, train/total_loss: 0.6386, train/total_loss/avg: 0.6697, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 622ms, time_since_start: 02h 26m 10s 050ms, eta: 02h 29m 41s 093ms
[32m2022-10-17T14:07:31 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T14:07:31 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T14:07:32 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T14:07:35 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T14:07:35 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, train/mmh/cross_entropy: 0.6387, train/mmh/cross_entropy/avg: 0.6700, train/total_loss: 0.6387, train/total_loss/avg: 0.6700, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 1.54, time: 01m 05s 239ms, time_since_start: 02h 27m 15s 290ms, eta: 02h 37m 02s 605ms
[32m2022-10-17T14:07:35 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T14:07:35 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T14:07:35 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T14:10:35 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T14:10:35 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T14:10:35 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T14:10:37 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T14:10:40 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T14:10:43 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T14:10:43 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, val/mmh/cross_entropy: 0.6018, val/total_loss: 0.6018, val/mmh/accuracy: 0.7402, val/mmh/binary_f1: 0.8501, val/mmh/roc_auc: 0.6265, num_updates: 11000, epoch: 2, iterations: 11000, max_updates: 22000, val_time: 03m 08s 157ms, best_update: 11000, best_iteration: 11000, best_val/mmh/roc_auc: 0.626474
[32m2022-10-17T14:11:45 | mmf.trainers.callbacks.logistics: [39mprogress: 11100/22000, train/mmh/cross_entropy: 0.6387, train/mmh/cross_entropy/avg: 0.6697, train/total_loss: 0.6387, train/total_loss/avg: 0.6697, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 909ms, time_since_start: 02h 31m 25s 360ms, eta: 02h 27m 40s 367ms
[32m2022-10-17T14:12:47 | mmf.trainers.callbacks.logistics: [39mprogress: 11200/22000, train/mmh/cross_entropy: 0.6386, train/mmh/cross_entropy/avg: 0.6692, train/total_loss: 0.6386, train/total_loss/avg: 0.6692, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 847ms, time_since_start: 02h 32m 27s 207ms, eta: 02h 26m 10s 237ms
[32m2022-10-17T14:13:48 | mmf.trainers.callbacks.logistics: [39mprogress: 11300/22000, train/mmh/cross_entropy: 0.6386, train/mmh/cross_entropy/avg: 0.6704, train/total_loss: 0.6386, train/total_loss/avg: 0.6704, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 621ms, time_since_start: 02h 33m 28s 829ms, eta: 02h 24m 17s 336ms
[32m2022-10-17T14:14:50 | mmf.trainers.callbacks.logistics: [39mprogress: 11400/22000, train/mmh/cross_entropy: 0.6386, train/mmh/cross_entropy/avg: 0.6704, train/total_loss: 0.6386, train/total_loss/avg: 0.6704, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 861ms, time_since_start: 02h 34m 30s 691ms, eta: 02h 23m 29s 711ms
[32m2022-10-17T14:15:52 | mmf.trainers.callbacks.logistics: [39mprogress: 11500/22000, train/mmh/cross_entropy: 0.6387, train/mmh/cross_entropy/avg: 0.6705, train/total_loss: 0.6387, train/total_loss/avg: 0.6705, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 922ms, time_since_start: 02h 35m 32s 613ms, eta: 02h 22m 16s 893ms
[32m2022-10-17T14:16:54 | mmf.trainers.callbacks.logistics: [39mprogress: 11600/22000, train/mmh/cross_entropy: 0.6387, train/mmh/cross_entropy/avg: 0.6706, train/total_loss: 0.6387, train/total_loss/avg: 0.6706, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 478ms, time_since_start: 02h 36m 34s 091ms, eta: 02h 19m 54s 978ms
[32m2022-10-17T14:17:55 | mmf.trainers.callbacks.logistics: [39mprogress: 11700/22000, train/mmh/cross_entropy: 0.6387, train/mmh/cross_entropy/avg: 0.6705, train/total_loss: 0.6387, train/total_loss/avg: 0.6705, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 590ms, time_since_start: 02h 37m 35s 681ms, eta: 02h 18m 49s 415ms
[32m2022-10-17T14:18:57 | mmf.trainers.callbacks.logistics: [39mprogress: 11800/22000, train/mmh/cross_entropy: 0.6475, train/mmh/cross_entropy/avg: 0.6706, train/total_loss: 0.6475, train/total_loss/avg: 0.6706, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 806ms, time_since_start: 02h 38m 37s 488ms, eta: 02h 17m 57s 527ms
[32m2022-10-17T14:19:59 | mmf.trainers.callbacks.logistics: [39mprogress: 11900/22000, train/mmh/cross_entropy: 0.6560, train/mmh/cross_entropy/avg: 0.6708, train/total_loss: 0.6560, train/total_loss/avg: 0.6708, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 1.64, time: 01m 01s 472ms, time_since_start: 02h 39m 38s 961ms, eta: 02h 15m 52s 112ms
[32m2022-10-17T14:21:00 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T14:21:00 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T14:21:01 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T14:21:04 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T14:21:04 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, train/mmh/cross_entropy: 0.6610, train/mmh/cross_entropy/avg: 0.6712, train/total_loss: 0.6610, train/total_loss/avg: 0.6712, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 1.54, time: 01m 05s 049ms, time_since_start: 02h 40m 44s 011ms, eta: 02h 22m 21s 035ms
[32m2022-10-17T14:21:04 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T14:21:04 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T14:21:04 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T14:24:04 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T14:24:04 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T14:24:04 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T14:24:07 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T14:24:10 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T14:24:10 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, val/mmh/cross_entropy: 0.5969, val/total_loss: 0.5969, val/mmh/accuracy: 0.7406, val/mmh/binary_f1: 0.8507, val/mmh/roc_auc: 0.6241, num_updates: 12000, epoch: 2, iterations: 12000, max_updates: 22000, val_time: 03m 05s 905ms, best_update: 11000, best_iteration: 11000, best_val/mmh/roc_auc: 0.626474
[32m2022-10-17T14:25:11 | mmf.trainers.callbacks.logistics: [39mprogress: 12100/22000, train/mmh/cross_entropy: 0.6622, train/mmh/cross_entropy/avg: 0.6715, train/total_loss: 0.6622, train/total_loss/avg: 0.6715, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 913ms, time_since_start: 02h 44m 51s 833ms, eta: 02h 14m 07s 992ms
[32m2022-10-17T14:26:13 | mmf.trainers.callbacks.logistics: [39mprogress: 12200/22000, train/mmh/cross_entropy: 0.6669, train/mmh/cross_entropy/avg: 0.6715, train/total_loss: 0.6669, train/total_loss/avg: 0.6715, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 987ms, time_since_start: 02h 45m 53s 821ms, eta: 02h 12m 56s 223ms
[32m2022-10-17T14:27:15 | mmf.trainers.callbacks.logistics: [39mprogress: 12300/22000, train/mmh/cross_entropy: 0.6669, train/mmh/cross_entropy/avg: 0.6713, train/total_loss: 0.6669, train/total_loss/avg: 0.6713, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 510ms, time_since_start: 02h 46m 55s 331ms, eta: 02h 10m 33s 988ms
[32m2022-10-17T14:28:17 | mmf.trainers.callbacks.logistics: [39mprogress: 12400/22000, train/mmh/cross_entropy: 0.6669, train/mmh/cross_entropy/avg: 0.6717, train/total_loss: 0.6669, train/total_loss/avg: 0.6717, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 886ms, time_since_start: 02h 47m 57s 217ms, eta: 02h 10m 622ms
[32m2022-10-17T14:29:19 | mmf.trainers.callbacks.logistics: [39mprogress: 12500/22000, train/mmh/cross_entropy: 0.6669, train/mmh/cross_entropy/avg: 0.6712, train/total_loss: 0.6669, train/total_loss/avg: 0.6712, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 722ms, time_since_start: 02h 48m 58s 939ms, eta: 02h 08m 18s 907ms
[32m2022-10-17T14:30:20 | mmf.trainers.callbacks.logistics: [39mprogress: 12600/22000, train/mmh/cross_entropy: 0.6680, train/mmh/cross_entropy/avg: 0.6712, train/total_loss: 0.6680, train/total_loss/avg: 0.6712, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 729ms, time_since_start: 02h 50m 669ms, eta: 02h 06m 58s 748ms
[32m2022-10-17T14:31:22 | mmf.trainers.callbacks.logistics: [39mprogress: 12700/22000, train/mmh/cross_entropy: 0.6680, train/mmh/cross_entropy/avg: 0.6703, train/total_loss: 0.6680, train/total_loss/avg: 0.6703, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 570ms, time_since_start: 02h 51m 02s 239ms, eta: 02h 05m 18s 274ms
[32m2022-10-17T14:32:23 | mmf.trainers.callbacks.logistics: [39mprogress: 12800/22000, train/mmh/cross_entropy: 0.6680, train/mmh/cross_entropy/avg: 0.6701, train/total_loss: 0.6680, train/total_loss/avg: 0.6701, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 514ms, time_since_start: 02h 52m 03s 753ms, eta: 02h 03m 50s 680ms
[32m2022-10-17T14:33:25 | mmf.trainers.callbacks.logistics: [39mprogress: 12900/22000, train/mmh/cross_entropy: 0.6680, train/mmh/cross_entropy/avg: 0.6696, train/total_loss: 0.6680, train/total_loss/avg: 0.6696, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 310ms, time_since_start: 02h 53m 05s 064ms, eta: 02h 02m 05s 588ms
[32m2022-10-17T14:34:26 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T14:34:26 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T14:34:27 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T14:34:30 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T14:34:30 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, train/mmh/cross_entropy: 0.6680, train/mmh/cross_entropy/avg: 0.6697, train/total_loss: 0.6680, train/total_loss/avg: 0.6697, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 1.54, time: 01m 05s 001ms, time_since_start: 02h 54m 10s 065ms, eta: 02h 08m 01s 203ms
[32m2022-10-17T14:34:30 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T14:34:30 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T14:34:30 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T14:37:30 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T14:37:30 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T14:37:30 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T14:37:33 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T14:37:36 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T14:37:40 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T14:37:40 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, val/mmh/cross_entropy: 0.6007, val/total_loss: 0.6007, val/mmh/accuracy: 0.7418, val/mmh/binary_f1: 0.8513, val/mmh/roc_auc: 0.6276, num_updates: 13000, epoch: 2, iterations: 13000, max_updates: 22000, val_time: 03m 10s 050ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.627591
[32m2022-10-17T14:38:42 | mmf.trainers.callbacks.logistics: [39mprogress: 13100/22000, train/mmh/cross_entropy: 0.6680, train/mmh/cross_entropy/avg: 0.6696, train/total_loss: 0.6680, train/total_loss/avg: 0.6696, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 1.61, time: 01m 02s 249ms, time_since_start: 02h 58m 22s 370ms, eta: 02h 01m 14s 326ms
[32m2022-10-17T14:39:44 | mmf.trainers.callbacks.logistics: [39mprogress: 13200/22000, train/mmh/cross_entropy: 0.6720, train/mmh/cross_entropy/avg: 0.6701, train/total_loss: 0.6720, train/total_loss/avg: 0.6701, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 743ms, time_since_start: 02h 59m 24s 113ms, eta: 01h 58m 54s 091ms
[32m2022-10-17T14:40:45 | mmf.trainers.callbacks.logistics: [39mprogress: 13300/22000, train/mmh/cross_entropy: 0.6720, train/mmh/cross_entropy/avg: 0.6706, train/total_loss: 0.6720, train/total_loss/avg: 0.6706, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 684ms, time_since_start: 03h 25s 798ms, eta: 01h 57m 26s 300ms
[32m2022-10-17T14:41:47 | mmf.trainers.callbacks.logistics: [39mprogress: 13400/22000, train/mmh/cross_entropy: 0.6720, train/mmh/cross_entropy/avg: 0.6704, train/total_loss: 0.6720, train/total_loss/avg: 0.6704, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 796ms, time_since_start: 03h 01m 27s 594ms, eta: 01h 56m 17s 940ms
[32m2022-10-17T14:42:49 | mmf.trainers.callbacks.logistics: [39mprogress: 13500/22000, train/mmh/cross_entropy: 0.6669, train/mmh/cross_entropy/avg: 0.6700, train/total_loss: 0.6669, train/total_loss/avg: 0.6700, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 860ms, time_since_start: 03h 02m 29s 455ms, eta: 01h 55m 03s 991ms
[32m2022-10-17T14:43:51 | mmf.trainers.callbacks.logistics: [39mprogress: 13600/22000, train/mmh/cross_entropy: 0.6652, train/mmh/cross_entropy/avg: 0.6696, train/total_loss: 0.6652, train/total_loss/avg: 0.6696, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 610ms, time_since_start: 03h 03m 31s 066ms, eta: 01h 53m 15s 186ms
[32m2022-10-17T14:44:53 | mmf.trainers.callbacks.logistics: [39mprogress: 13700/22000, train/mmh/cross_entropy: 0.6669, train/mmh/cross_entropy/avg: 0.6697, train/total_loss: 0.6669, train/total_loss/avg: 0.6697, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 1.61, time: 01m 02s 537ms, time_since_start: 03h 04m 33s 603ms, eta: 01h 53m 35s 248ms
[32m2022-10-17T14:45:55 | mmf.trainers.callbacks.logistics: [39mprogress: 13800/22000, train/mmh/cross_entropy: 0.6652, train/mmh/cross_entropy/avg: 0.6696, train/total_loss: 0.6652, train/total_loss/avg: 0.6696, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 1.61, time: 01m 02s 164ms, time_since_start: 03h 05m 35s 768ms, eta: 01h 51m 32s 986ms
[32m2022-10-17T14:46:57 | mmf.trainers.callbacks.logistics: [39mprogress: 13900/22000, train/mmh/cross_entropy: 0.6652, train/mmh/cross_entropy/avg: 0.6697, train/total_loss: 0.6652, train/total_loss/avg: 0.6697, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 553ms, time_since_start: 03h 06m 37s 322ms, eta: 01h 49m 06s 431ms
[32m2022-10-17T14:47:59 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T14:47:59 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T14:47:59 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T14:48:02 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T14:48:02 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, train/mmh/cross_entropy: 0.6532, train/mmh/cross_entropy/avg: 0.6695, train/total_loss: 0.6532, train/total_loss/avg: 0.6695, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 1.54, time: 01m 05s 123ms, time_since_start: 03h 07m 42s 445ms, eta: 01h 54m 545ms
[32m2022-10-17T14:48:02 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T14:48:02 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T14:48:02 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T14:51:02 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T14:51:02 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T14:51:02 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T14:51:05 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T14:51:08 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T14:51:08 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, val/mmh/cross_entropy: 0.5945, val/total_loss: 0.5945, val/mmh/accuracy: 0.7418, val/mmh/binary_f1: 0.8515, val/mmh/roc_auc: 0.6273, num_updates: 14000, epoch: 2, iterations: 14000, max_updates: 22000, val_time: 03m 06s 093ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.627591
[32m2022-10-17T14:52:10 | mmf.trainers.callbacks.logistics: [39mprogress: 14100/22000, train/mmh/cross_entropy: 0.6532, train/mmh/cross_entropy/avg: 0.6697, train/total_loss: 0.6532, train/total_loss/avg: 0.6697, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 701ms, time_since_start: 03h 11m 50s 246ms, eta: 01h 46m 40s 120ms
[32m2022-10-17T14:53:12 | mmf.trainers.callbacks.logistics: [39mprogress: 14200/22000, train/mmh/cross_entropy: 0.6517, train/mmh/cross_entropy/avg: 0.6693, train/total_loss: 0.6517, train/total_loss/avg: 0.6693, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 993ms, time_since_start: 03h 12m 52s 240ms, eta: 01h 45m 49s 013ms
[32m2022-10-17T14:54:14 | mmf.trainers.callbacks.logistics: [39mprogress: 14300/22000, train/mmh/cross_entropy: 0.6459, train/mmh/cross_entropy/avg: 0.6690, train/total_loss: 0.6459, train/total_loss/avg: 0.6690, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 976ms, time_since_start: 03h 13m 54s 216ms, eta: 01h 44m 25s 891ms
[32m2022-10-17T14:55:16 | mmf.trainers.callbacks.logistics: [39mprogress: 14400/22000, train/mmh/cross_entropy: 0.6459, train/mmh/cross_entropy/avg: 0.6683, train/total_loss: 0.6459, train/total_loss/avg: 0.6683, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 797ms, time_since_start: 03h 14m 56s 014ms, eta: 01h 42m 46s 693ms
[32m2022-10-17T14:56:17 | mmf.trainers.callbacks.logistics: [39mprogress: 14500/22000, train/mmh/cross_entropy: 0.6459, train/mmh/cross_entropy/avg: 0.6685, train/total_loss: 0.6459, train/total_loss/avg: 0.6685, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 568ms, time_since_start: 03h 15m 57s 583ms, eta: 01h 41m 02s 947ms
[32m2022-10-17T14:57:19 | mmf.trainers.callbacks.logistics: [39mprogress: 14600/22000, train/mmh/cross_entropy: 0.6459, train/mmh/cross_entropy/avg: 0.6687, train/total_loss: 0.6459, train/total_loss/avg: 0.6687, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 873ms, time_since_start: 03h 16m 59s 456ms, eta: 01h 40m 11s 718ms
[32m2022-10-17T14:58:21 | mmf.trainers.callbacks.logistics: [39mprogress: 14700/22000, train/mmh/cross_entropy: 0.6532, train/mmh/cross_entropy/avg: 0.6688, train/total_loss: 0.6532, train/total_loss/avg: 0.6688, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 946ms, time_since_start: 03h 18m 01s 402ms, eta: 01h 38m 57s 472ms
[32m2022-10-17T14:59:23 | mmf.trainers.callbacks.logistics: [39mprogress: 14800/22000, train/mmh/cross_entropy: 0.6532, train/mmh/cross_entropy/avg: 0.6684, train/total_loss: 0.6532, train/total_loss/avg: 0.6684, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 995ms, time_since_start: 03h 19m 03s 398ms, eta: 01h 37m 40s 824ms
[32m2022-10-17T15:00:25 | mmf.trainers.callbacks.logistics: [39mprogress: 14900/22000, train/mmh/cross_entropy: 0.6532, train/mmh/cross_entropy/avg: 0.6679, train/total_loss: 0.6532, train/total_loss/avg: 0.6679, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 800ms, time_since_start: 03h 20m 05s 198ms, eta: 01h 36m 01s 208ms
[32m2022-10-17T15:01:27 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T15:01:27 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T15:01:27 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T15:01:30 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T15:01:30 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, train/mmh/cross_entropy: 0.6459, train/mmh/cross_entropy/avg: 0.6677, train/total_loss: 0.6459, train/total_loss/avg: 0.6677, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 1.54, time: 01m 05s 012ms, time_since_start: 03h 21m 10s 210ms, eta: 01h 39m 35s 276ms
[32m2022-10-17T15:01:30 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T15:01:30 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T15:01:30 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T15:04:31 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T15:04:31 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T15:04:31 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T15:04:34 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T15:04:36 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T15:04:39 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T15:04:39 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, val/mmh/cross_entropy: 0.5948, val/total_loss: 0.5948, val/mmh/accuracy: 0.7398, val/mmh/binary_f1: 0.8498, val/mmh/roc_auc: 0.6278, num_updates: 15000, epoch: 2, iterations: 15000, max_updates: 22000, val_time: 03m 09s 284ms, best_update: 15000, best_iteration: 15000, best_val/mmh/roc_auc: 0.627791
[32m2022-10-17T15:05:41 | mmf.trainers.callbacks.logistics: [39mprogress: 15100/22000, train/mmh/cross_entropy: 0.6459, train/mmh/cross_entropy/avg: 0.6675, train/total_loss: 0.6459, train/total_loss/avg: 0.6675, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 856ms, time_since_start: 03h 25m 21s 357ms, eta: 01h 33m 24s 057ms
[32m2022-10-17T15:06:43 | mmf.trainers.callbacks.logistics: [39mprogress: 15200/22000, train/mmh/cross_entropy: 0.6459, train/mmh/cross_entropy/avg: 0.6673, train/total_loss: 0.6459, train/total_loss/avg: 0.6673, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 933ms, time_since_start: 03h 26m 23s 290ms, eta: 01h 32m 09s 663ms
[32m2022-10-17T15:07:45 | mmf.trainers.callbacks.logistics: [39mprogress: 15300/22000, train/mmh/cross_entropy: 0.6418, train/mmh/cross_entropy/avg: 0.6671, train/total_loss: 0.6418, train/total_loss/avg: 0.6671, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 921ms, time_since_start: 03h 27m 25s 212ms, eta: 01h 30m 47s 263ms
[32m2022-10-17T15:08:46 | mmf.trainers.callbacks.logistics: [39mprogress: 15400/22000, train/mmh/cross_entropy: 0.6386, train/mmh/cross_entropy/avg: 0.6668, train/total_loss: 0.6386, train/total_loss/avg: 0.6668, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 558ms, time_since_start: 03h 28m 26s 770ms, eta: 01h 28m 54s 550ms
[32m2022-10-17T15:09:48 | mmf.trainers.callbacks.logistics: [39mprogress: 15500/22000, train/mmh/cross_entropy: 0.6418, train/mmh/cross_entropy/avg: 0.6671, train/total_loss: 0.6418, train/total_loss/avg: 0.6671, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 724ms, time_since_start: 03h 29m 28s 495ms, eta: 01h 27m 47s 871ms
[32m2022-10-17T15:10:50 | mmf.trainers.callbacks.logistics: [39mprogress: 15600/22000, train/mmh/cross_entropy: 0.6459, train/mmh/cross_entropy/avg: 0.6677, train/total_loss: 0.6459, train/total_loss/avg: 0.6677, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 594ms, time_since_start: 03h 30m 30s 089ms, eta: 01h 26m 15s 874ms
[32m2022-10-17T15:11:51 | mmf.trainers.callbacks.logistics: [39mprogress: 15700/22000, train/mmh/cross_entropy: 0.6418, train/mmh/cross_entropy/avg: 0.6675, train/total_loss: 0.6418, train/total_loss/avg: 0.6675, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 791ms, time_since_start: 03h 31m 31s 880ms, eta: 01h 25m 11s 300ms
[32m2022-10-17T15:12:53 | mmf.trainers.callbacks.logistics: [39mprogress: 15800/22000, train/mmh/cross_entropy: 0.6418, train/mmh/cross_entropy/avg: 0.6678, train/total_loss: 0.6418, train/total_loss/avg: 0.6678, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 576ms, time_since_start: 03h 32m 33s 457ms, eta: 01h 23m 32s 733ms
[32m2022-10-17T15:13:55 | mmf.trainers.callbacks.logistics: [39mprogress: 15900/22000, train/mmh/cross_entropy: 0.6386, train/mmh/cross_entropy/avg: 0.6676, train/total_loss: 0.6386, train/total_loss/avg: 0.6676, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 1.64, time: 01m 01s 496ms, time_since_start: 03h 33m 34s 954ms, eta: 01h 22m 05s 454ms
[32m2022-10-17T15:14:56 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T15:14:56 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T15:14:57 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T15:15:00 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T15:15:00 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, train/mmh/cross_entropy: 0.6386, train/mmh/cross_entropy/avg: 0.6678, train/total_loss: 0.6386, train/total_loss/avg: 0.6678, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 1.54, time: 01m 05s 013ms, time_since_start: 03h 34m 39s 967ms, eta: 01h 25m 21s 795ms
[32m2022-10-17T15:15:00 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T15:15:00 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T15:15:00 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T15:18:00 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T15:18:00 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T15:18:00 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T15:18:03 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T15:18:06 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T15:18:09 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T15:18:09 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, val/mmh/cross_entropy: 0.5895, val/total_loss: 0.5895, val/mmh/accuracy: 0.7404, val/mmh/binary_f1: 0.8503, val/mmh/roc_auc: 0.6317, num_updates: 16000, epoch: 2, iterations: 16000, max_updates: 22000, val_time: 03m 08s 958ms, best_update: 16000, best_iteration: 16000, best_val/mmh/roc_auc: 0.631662
[32m2022-10-17T15:19:10 | mmf.trainers.callbacks.logistics: [39mprogress: 16100/22000, train/mmh/cross_entropy: 0.6370, train/mmh/cross_entropy/avg: 0.6675, train/total_loss: 0.6370, train/total_loss/avg: 0.6675, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 765ms, time_since_start: 03h 38m 50s 698ms, eta: 01h 19m 44s 795ms
[32m2022-10-17T15:20:12 | mmf.trainers.callbacks.logistics: [39mprogress: 16200/22000, train/mmh/cross_entropy: 0.6386, train/mmh/cross_entropy/avg: 0.6675, train/total_loss: 0.6386, train/total_loss/avg: 0.6675, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 886ms, time_since_start: 03h 39m 52s 584ms, eta: 01h 18m 32s 883ms
[32m2022-10-17T15:21:14 | mmf.trainers.callbacks.logistics: [39mprogress: 16300/22000, train/mmh/cross_entropy: 0.6418, train/mmh/cross_entropy/avg: 0.6677, train/total_loss: 0.6418, train/total_loss/avg: 0.6677, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 620ms, time_since_start: 03h 40m 54s 204ms, eta: 01h 16m 51s 705ms
[32m2022-10-17T15:22:16 | mmf.trainers.callbacks.logistics: [39mprogress: 16400/22000, train/mmh/cross_entropy: 0.6461, train/mmh/cross_entropy/avg: 0.6683, train/total_loss: 0.6461, train/total_loss/avg: 0.6683, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 752ms, time_since_start: 03h 41m 55s 957ms, eta: 01h 15m 40s 552ms
[32m2022-10-17T15:23:17 | mmf.trainers.callbacks.logistics: [39mprogress: 16500/22000, train/mmh/cross_entropy: 0.6418, train/mmh/cross_entropy/avg: 0.6681, train/total_loss: 0.6418, train/total_loss/avg: 0.6681, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 546ms, time_since_start: 03h 42m 57s 503ms, eta: 01h 14m 04s 581ms
[32m2022-10-17T15:24:19 | mmf.trainers.callbacks.logistics: [39mprogress: 16600/22000, train/mmh/cross_entropy: 0.6386, train/mmh/cross_entropy/avg: 0.6678, train/total_loss: 0.6386, train/total_loss/avg: 0.6678, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 744ms, time_since_start: 03h 43m 59s 248ms, eta: 01h 12m 57s 787ms
[32m2022-10-17T15:25:20 | mmf.trainers.callbacks.logistics: [39mprogress: 16700/22000, train/mmh/cross_entropy: 0.6386, train/mmh/cross_entropy/avg: 0.6680, train/total_loss: 0.6386, train/total_loss/avg: 0.6680, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 546ms, time_since_start: 03h 45m 794ms, eta: 01h 11m 22s 941ms
[32m2022-10-17T15:26:22 | mmf.trainers.callbacks.logistics: [39mprogress: 16800/22000, train/mmh/cross_entropy: 0.6418, train/mmh/cross_entropy/avg: 0.6681, train/total_loss: 0.6418, train/total_loss/avg: 0.6681, max mem: 691.0, experiment: mmh, epoch: 2, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 894ms, time_since_start: 03h 46m 02s 688ms, eta: 01h 10m 25s 895ms
[32m2022-10-17T15:27:23 | mmf.trainers.callbacks.logistics: [39mprogress: 16900/22000, train/mmh/cross_entropy: 0.6418, train/mmh/cross_entropy/avg: 0.6679, train/total_loss: 0.6418, train/total_loss/avg: 0.6679, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 1.64, time: 01m 01s 041ms, time_since_start: 03h 47m 03s 729ms, eta: 01h 08m 07s 505ms
[32m2022-10-17T15:28:25 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T15:28:25 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T15:28:25 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T15:28:28 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T15:28:28 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, train/mmh/cross_entropy: 0.6461, train/mmh/cross_entropy/avg: 0.6681, train/total_loss: 0.6461, train/total_loss/avg: 0.6681, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 1.56, time: 01m 04s 666ms, time_since_start: 03h 48m 08s 396ms, eta: 01h 10m 45s 354ms
[32m2022-10-17T15:28:28 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T15:28:28 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T15:28:28 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T15:31:28 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T15:31:28 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T15:31:28 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T15:31:31 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T15:31:34 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T15:31:37 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T15:31:37 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, val/mmh/cross_entropy: 0.5974, val/total_loss: 0.5974, val/mmh/accuracy: 0.7372, val/mmh/binary_f1: 0.8476, val/mmh/roc_auc: 0.6326, num_updates: 17000, epoch: 3, iterations: 17000, max_updates: 22000, val_time: 03m 08s 720ms, best_update: 17000, best_iteration: 17000, best_val/mmh/roc_auc: 0.632642
[32m2022-10-17T15:32:40 | mmf.trainers.callbacks.logistics: [39mprogress: 17100/22000, train/mmh/cross_entropy: 0.6591, train/mmh/cross_entropy/avg: 0.6681, train/total_loss: 0.6591, train/total_loss/avg: 0.6681, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 558ms, time_since_start: 03h 52m 20s 681ms, eta: 01h 08m 09s 174ms
[32m2022-10-17T15:33:44 | mmf.trainers.callbacks.logistics: [39mprogress: 17200/22000, train/mmh/cross_entropy: 0.6591, train/mmh/cross_entropy/avg: 0.6680, train/total_loss: 0.6591, train/total_loss/avg: 0.6680, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 721ms, time_since_start: 03h 53m 24s 403ms, eta: 01h 06m 56s 008ms
[32m2022-10-17T15:34:48 | mmf.trainers.callbacks.logistics: [39mprogress: 17300/22000, train/mmh/cross_entropy: 0.6591, train/mmh/cross_entropy/avg: 0.6676, train/total_loss: 0.6591, train/total_loss/avg: 0.6676, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 1.56, time: 01m 04s 026ms, time_since_start: 03h 54m 28s 430ms, eta: 01h 05m 51s 167ms
[32m2022-10-17T15:35:52 | mmf.trainers.callbacks.logistics: [39mprogress: 17400/22000, train/mmh/cross_entropy: 0.6705, train/mmh/cross_entropy/avg: 0.6679, train/total_loss: 0.6705, train/total_loss/avg: 0.6679, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 1.56, time: 01m 04s 021ms, time_since_start: 03h 55m 32s 451ms, eta: 01h 04m 26s 771ms
[32m2022-10-17T15:36:56 | mmf.trainers.callbacks.logistics: [39mprogress: 17500/22000, train/mmh/cross_entropy: 0.6705, train/mmh/cross_entropy/avg: 0.6680, train/total_loss: 0.6705, train/total_loss/avg: 0.6680, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 496ms, time_since_start: 03h 56m 35s 948ms, eta: 01h 02m 31s 700ms
[32m2022-10-17T15:37:59 | mmf.trainers.callbacks.logistics: [39mprogress: 17600/22000, train/mmh/cross_entropy: 0.6705, train/mmh/cross_entropy/avg: 0.6681, train/total_loss: 0.6705, train/total_loss/avg: 0.6681, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 345ms, time_since_start: 03h 57m 39s 294ms, eta: 01h 59s 611ms
[32m2022-10-17T15:39:02 | mmf.trainers.callbacks.logistics: [39mprogress: 17700/22000, train/mmh/cross_entropy: 0.6705, train/mmh/cross_entropy/avg: 0.6677, train/total_loss: 0.6705, train/total_loss/avg: 0.6677, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 414ms, time_since_start: 03h 58m 42s 708ms, eta: 59m 40s 313ms
[32m2022-10-17T15:40:06 | mmf.trainers.callbacks.logistics: [39mprogress: 17800/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6676, train/total_loss: 0.6608, train/total_loss/avg: 0.6676, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 722ms, time_since_start: 03h 59m 46s 430ms, eta: 58m 34s 021ms
[32m2022-10-17T15:41:09 | mmf.trainers.callbacks.logistics: [39mprogress: 17900/22000, train/mmh/cross_entropy: 0.6705, train/mmh/cross_entropy/avg: 0.6677, train/total_loss: 0.6705, train/total_loss/avg: 0.6677, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 397ms, time_since_start: 04h 49s 828ms, eta: 56m 52s 898ms
[32m2022-10-17T15:42:13 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T15:42:13 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T15:42:14 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T15:42:17 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T15:42:17 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6676, train/total_loss: 0.6608, train/total_loss/avg: 0.6676, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 1.49, time: 01m 07s 088ms, time_since_start: 04h 01m 56s 917ms, eta: 58m 43s 481ms
[32m2022-10-17T15:42:17 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T15:42:17 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T15:42:17 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T15:45:17 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T15:45:17 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T15:45:17 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T15:45:20 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T15:45:23 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T15:45:26 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T15:45:26 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, val/mmh/cross_entropy: 0.6037, val/total_loss: 0.6037, val/mmh/accuracy: 0.7364, val/mmh/binary_f1: 0.8460, val/mmh/roc_auc: 0.6345, num_updates: 18000, epoch: 3, iterations: 18000, max_updates: 22000, val_time: 03m 09s 345ms, best_update: 18000, best_iteration: 18000, best_val/mmh/roc_auc: 0.634509
[32m2022-10-17T15:46:31 | mmf.trainers.callbacks.logistics: [39mprogress: 18100/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6674, train/total_loss: 0.6608, train/total_loss/avg: 0.6674, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 1.54, time: 01m 05s 518ms, time_since_start: 04h 06m 11s 783ms, eta: 55m 55s 020ms
[32m2022-10-17T15:47:37 | mmf.trainers.callbacks.logistics: [39mprogress: 18200/22000, train/mmh/cross_entropy: 0.6705, train/mmh/cross_entropy/avg: 0.6676, train/total_loss: 0.6705, train/total_loss/avg: 0.6676, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 1.54, time: 01m 05s 171ms, time_since_start: 04h 07m 16s 955ms, eta: 54m 11s 682ms
[32m2022-10-17T15:48:41 | mmf.trainers.callbacks.logistics: [39mprogress: 18300/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6675, train/total_loss: 0.6608, train/total_loss/avg: 0.6675, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 1.56, time: 01m 04s 725ms, time_since_start: 04h 08m 21s 680ms, eta: 52m 24s 419ms
[32m2022-10-17T15:49:45 | mmf.trainers.callbacks.logistics: [39mprogress: 18400/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6676, train/total_loss: 0.6608, train/total_loss/avg: 0.6676, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 806ms, time_since_start: 04h 09m 25s 486ms, eta: 50m 16s 009ms
[32m2022-10-17T15:50:49 | mmf.trainers.callbacks.logistics: [39mprogress: 18500/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6674, train/total_loss: 0.6608, train/total_loss/avg: 0.6674, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 776ms, time_since_start: 04h 10m 29s 263ms, eta: 48m 50s 848ms
[32m2022-10-17T15:51:53 | mmf.trainers.callbacks.logistics: [39mprogress: 18600/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6672, train/total_loss: 0.6608, train/total_loss/avg: 0.6672, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 1.56, time: 01m 04s 388ms, time_since_start: 04h 11m 33s 651ms, eta: 47m 54s 413ms
[32m2022-10-17T15:52:58 | mmf.trainers.callbacks.logistics: [39mprogress: 18700/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6676, train/total_loss: 0.6608, train/total_loss/avg: 0.6676, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 1.56, time: 01m 04s 266ms, time_since_start: 04h 12m 37s 917ms, eta: 46m 24s 582ms
[32m2022-10-17T15:54:01 | mmf.trainers.callbacks.logistics: [39mprogress: 18800/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6678, train/total_loss: 0.6608, train/total_loss/avg: 0.6678, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 690ms, time_since_start: 04h 13m 41s 607ms, eta: 44m 35s 999ms
[32m2022-10-17T15:55:05 | mmf.trainers.callbacks.logistics: [39mprogress: 18900/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6676, train/total_loss: 0.6608, train/total_loss/avg: 0.6676, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 497ms, time_since_start: 04h 14m 45s 105ms, eta: 43m 04s 549ms
[32m2022-10-17T15:56:08 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T15:56:08 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T15:56:10 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T15:56:12 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T15:56:12 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6676, train/total_loss: 0.6608, train/total_loss/avg: 0.6676, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 1.49, time: 01m 07s 693ms, time_since_start: 04h 15m 52s 799ms, eta: 44m 26s 465ms
[32m2022-10-17T15:56:12 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T15:56:12 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T15:56:12 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T15:59:06 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T15:59:06 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T15:59:06 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T15:59:09 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T15:59:12 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T15:59:14 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T15:59:14 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, val/mmh/cross_entropy: 0.5844, val/total_loss: 0.5844, val/mmh/accuracy: 0.7412, val/mmh/binary_f1: 0.8510, val/mmh/roc_auc: 0.6347, num_updates: 19000, epoch: 3, iterations: 19000, max_updates: 22000, val_time: 03m 02s 049ms, best_update: 19000, best_iteration: 19000, best_val/mmh/roc_auc: 0.634660
[32m2022-10-17T16:00:19 | mmf.trainers.callbacks.logistics: [39mprogress: 19100/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6676, train/total_loss: 0.6608, train/total_loss/avg: 0.6676, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 1.56, time: 01m 04s 990ms, time_since_start: 04h 19m 59s 846ms, eta: 41m 14s 656ms
[32m2022-10-17T16:01:23 | mmf.trainers.callbacks.logistics: [39mprogress: 19200/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6672, train/total_loss: 0.6608, train/total_loss/avg: 0.6672, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 941ms, time_since_start: 04h 21m 03s 787ms, eta: 39m 10s 753ms
[32m2022-10-17T16:02:27 | mmf.trainers.callbacks.logistics: [39mprogress: 19300/22000, train/mmh/cross_entropy: 0.6608, train/mmh/cross_entropy/avg: 0.6667, train/total_loss: 0.6608, train/total_loss/avg: 0.6667, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 560ms, time_since_start: 04h 22m 07s 348ms, eta: 37m 33s 281ms
[32m2022-10-17T16:03:30 | mmf.trainers.callbacks.logistics: [39mprogress: 19400/22000, train/mmh/cross_entropy: 0.6544, train/mmh/cross_entropy/avg: 0.6665, train/total_loss: 0.6544, train/total_loss/avg: 0.6665, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 112ms, time_since_start: 04h 23m 10s 460ms, eta: 35m 54s 530ms
[32m2022-10-17T16:04:33 | mmf.trainers.callbacks.logistics: [39mprogress: 19500/22000, train/mmh/cross_entropy: 0.6411, train/mmh/cross_entropy/avg: 0.6660, train/total_loss: 0.6411, train/total_loss/avg: 0.6660, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 956ms, time_since_start: 04h 24m 13s 416ms, eta: 34m 26s 536ms
[32m2022-10-17T16:05:36 | mmf.trainers.callbacks.logistics: [39mprogress: 19600/22000, train/mmh/cross_entropy: 0.6387, train/mmh/cross_entropy/avg: 0.6657, train/total_loss: 0.6387, train/total_loss/avg: 0.6657, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 1.59, time: 01m 03s 209ms, time_since_start: 04h 25m 16s 626ms, eta: 33m 11s 851ms
[32m2022-10-17T16:06:39 | mmf.trainers.callbacks.logistics: [39mprogress: 19700/22000, train/mmh/cross_entropy: 0.6411, train/mmh/cross_entropy/avg: 0.6659, train/total_loss: 0.6411, train/total_loss/avg: 0.6659, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 787ms, time_since_start: 04h 26m 19s 413ms, eta: 31m 36s 108ms
[32m2022-10-17T16:07:42 | mmf.trainers.callbacks.logistics: [39mprogress: 19800/22000, train/mmh/cross_entropy: 0.6411, train/mmh/cross_entropy/avg: 0.6661, train/total_loss: 0.6411, train/total_loss/avg: 0.6661, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 672ms, time_since_start: 04h 27m 22s 086ms, eta: 30m 10s 369ms
[32m2022-10-17T16:08:44 | mmf.trainers.callbacks.logistics: [39mprogress: 19900/22000, train/mmh/cross_entropy: 0.6387, train/mmh/cross_entropy/avg: 0.6659, train/total_loss: 0.6387, train/total_loss/avg: 0.6659, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 1.61, time: 01m 02s 248ms, time_since_start: 04h 28m 24s 334ms, eta: 28m 36s 366ms
[32m2022-10-17T16:09:46 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T16:09:46 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T16:09:47 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T16:09:50 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T16:09:50 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, train/mmh/cross_entropy: 0.6387, train/mmh/cross_entropy/avg: 0.6659, train/total_loss: 0.6387, train/total_loss/avg: 0.6659, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 1.52, time: 01m 06s 089ms, time_since_start: 04h 29m 30s 423ms, eta: 28m 55s 502ms
[32m2022-10-17T16:09:50 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T16:09:50 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T16:09:50 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T16:12:50 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T16:12:50 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T16:12:50 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T16:12:53 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T16:12:56 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T16:12:59 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T16:12:59 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, val/mmh/cross_entropy: 0.5908, val/total_loss: 0.5908, val/mmh/accuracy: 0.7392, val/mmh/binary_f1: 0.8493, val/mmh/roc_auc: 0.6366, num_updates: 20000, epoch: 3, iterations: 20000, max_updates: 22000, val_time: 03m 08s 506ms, best_update: 20000, best_iteration: 20000, best_val/mmh/roc_auc: 0.636570
[32m2022-10-17T16:14:01 | mmf.trainers.callbacks.logistics: [39mprogress: 20100/22000, train/mmh/cross_entropy: 0.6542, train/mmh/cross_entropy/avg: 0.6658, train/total_loss: 0.6542, train/total_loss/avg: 0.6658, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 518ms, time_since_start: 04h 33m 41s 451ms, eta: 25m 59s 649ms
[32m2022-10-17T16:15:03 | mmf.trainers.callbacks.logistics: [39mprogress: 20200/22000, train/mmh/cross_entropy: 0.6275, train/mmh/cross_entropy/avg: 0.6655, train/total_loss: 0.6275, train/total_loss/avg: 0.6655, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 436ms, time_since_start: 04h 34m 43s 887ms, eta: 24m 35s 627ms
[32m2022-10-17T16:16:06 | mmf.trainers.callbacks.logistics: [39mprogress: 20300/22000, train/mmh/cross_entropy: 0.6275, train/mmh/cross_entropy/avg: 0.6656, train/total_loss: 0.6275, train/total_loss/avg: 0.6656, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 146ms, time_since_start: 04h 35m 46s 034ms, eta: 23m 07s 170ms
[32m2022-10-17T16:17:08 | mmf.trainers.callbacks.logistics: [39mprogress: 20400/22000, train/mmh/cross_entropy: 0.6260, train/mmh/cross_entropy/avg: 0.6654, train/total_loss: 0.6260, train/total_loss/avg: 0.6654, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 442ms, time_since_start: 04h 36m 48s 476ms, eta: 21m 51s 783ms
[32m2022-10-17T16:18:10 | mmf.trainers.callbacks.logistics: [39mprogress: 20500/22000, train/mmh/cross_entropy: 0.6275, train/mmh/cross_entropy/avg: 0.6653, train/total_loss: 0.6275, train/total_loss/avg: 0.6653, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 092ms, time_since_start: 04h 37m 50s 568ms, eta: 20m 22s 910ms
[32m2022-10-17T16:19:12 | mmf.trainers.callbacks.logistics: [39mprogress: 20600/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6652, train/total_loss: 0.6471, train/total_loss/avg: 0.6652, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 137ms, time_since_start: 04h 38m 52s 706ms, eta: 19m 02s 211ms
[32m2022-10-17T16:20:14 | mmf.trainers.callbacks.logistics: [39mprogress: 20700/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6651, train/total_loss: 0.6471, train/total_loss/avg: 0.6651, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 116ms, time_since_start: 04h 39m 54s 822ms, eta: 17m 40s 269ms
[32m2022-10-17T16:21:16 | mmf.trainers.callbacks.logistics: [39mprogress: 20800/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6655, train/total_loss: 0.6471, train/total_loss/avg: 0.6655, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 012ms, time_since_start: 04h 40m 56s 835ms, eta: 16m 17s 075ms
[32m2022-10-17T16:22:18 | mmf.trainers.callbacks.logistics: [39mprogress: 20900/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6652, train/total_loss: 0.6471, train/total_loss/avg: 0.6652, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 798ms, time_since_start: 04h 41m 58s 634ms, eta: 14m 52s 551ms
[32m2022-10-17T16:23:20 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T16:23:20 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T16:23:20 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T16:23:23 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T16:23:23 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6654, train/total_loss: 0.6471, train/total_loss/avg: 0.6654, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 1.56, time: 01m 04s 886ms, time_since_start: 04h 43m 03s 520ms, eta: 14m 11s 954ms
[32m2022-10-17T16:23:23 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T16:23:23 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T16:23:23 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T16:26:23 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T16:26:23 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T16:26:23 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T16:26:26 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T16:26:29 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T16:26:29 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, val/mmh/cross_entropy: 0.6014, val/total_loss: 0.6014, val/mmh/accuracy: 0.7334, val/mmh/binary_f1: 0.8434, val/mmh/roc_auc: 0.6365, num_updates: 21000, epoch: 3, iterations: 21000, max_updates: 22000, val_time: 03m 06s 134ms, best_update: 20000, best_iteration: 20000, best_val/mmh/roc_auc: 0.636570
[32m2022-10-17T16:27:31 | mmf.trainers.callbacks.logistics: [39mprogress: 21100/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6654, train/total_loss: 0.6471, train/total_loss/avg: 0.6654, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 010ms, time_since_start: 04h 47m 11s 670ms, eta: 12m 12s 775ms
[32m2022-10-17T16:28:33 | mmf.trainers.callbacks.logistics: [39mprogress: 21200/22000, train/mmh/cross_entropy: 0.6492, train/mmh/cross_entropy/avg: 0.6656, train/total_loss: 0.6492, train/total_loss/avg: 0.6656, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 905ms, time_since_start: 04h 48m 13s 576ms, eta: 10m 50s 252ms
[32m2022-10-17T16:29:35 | mmf.trainers.callbacks.logistics: [39mprogress: 21300/22000, train/mmh/cross_entropy: 0.6492, train/mmh/cross_entropy/avg: 0.6655, train/total_loss: 0.6492, train/total_loss/avg: 0.6655, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 967ms, time_since_start: 04h 49m 15s 543ms, eta: 09m 29s 539ms
[32m2022-10-17T16:30:37 | mmf.trainers.callbacks.logistics: [39mprogress: 21400/22000, train/mmh/cross_entropy: 0.6492, train/mmh/cross_entropy/avg: 0.6652, train/total_loss: 0.6492, train/total_loss/avg: 0.6652, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 942ms, time_since_start: 04h 50m 17s 485ms, eta: 08m 07s 984ms
[32m2022-10-17T16:31:39 | mmf.trainers.callbacks.logistics: [39mprogress: 21500/22000, train/mmh/cross_entropy: 0.6492, train/mmh/cross_entropy/avg: 0.6650, train/total_loss: 0.6492, train/total_loss/avg: 0.6650, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 887ms, time_since_start: 04h 51m 19s 373ms, eta: 06m 46s 293ms
[32m2022-10-17T16:32:41 | mmf.trainers.callbacks.logistics: [39mprogress: 21600/22000, train/mmh/cross_entropy: 0.6492, train/mmh/cross_entropy/avg: 0.6645, train/total_loss: 0.6492, train/total_loss/avg: 0.6645, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 956ms, time_since_start: 04h 52m 21s 330ms, eta: 05m 25s 396ms
[32m2022-10-17T16:33:43 | mmf.trainers.callbacks.logistics: [39mprogress: 21700/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6644, train/total_loss: 0.6471, train/total_loss/avg: 0.6644, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 889ms, time_since_start: 04h 53m 23s 219ms, eta: 04m 03s 781ms
[32m2022-10-17T16:34:45 | mmf.trainers.callbacks.logistics: [39mprogress: 21800/22000, train/mmh/cross_entropy: 0.6471, train/mmh/cross_entropy/avg: 0.6647, train/total_loss: 0.6471, train/total_loss/avg: 0.6647, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.61, time: 01m 02s 043ms, time_since_start: 04h 54m 25s 262ms, eta: 02m 42s 924ms
[32m2022-10-17T16:35:47 | mmf.trainers.callbacks.logistics: [39mprogress: 21900/22000, train/mmh/cross_entropy: 0.6492, train/mmh/cross_entropy/avg: 0.6649, train/total_loss: 0.6492, train/total_loss/avg: 0.6649, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.64, time: 01m 01s 893ms, time_since_start: 04h 55m 27s 156ms, eta: 01m 21s 266ms
[32m2022-10-17T16:36:48 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T16:36:48 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T16:36:49 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T16:36:52 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T16:36:52 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, train/mmh/cross_entropy: 0.6492, train/mmh/cross_entropy/avg: 0.6650, train/total_loss: 0.6492, train/total_loss/avg: 0.6650, max mem: 691.0, experiment: mmh, epoch: 3, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 1.54, time: 01m 05s 525ms, time_since_start: 04h 56m 32s 681ms, eta: 0ms
[32m2022-10-17T16:36:52 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T16:36:52 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T16:36:52 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T16:39:52 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T16:39:52 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T16:39:52 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T16:39:56 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T16:40:00 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T16:40:00 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, val/mmh/cross_entropy: 0.5961, val/total_loss: 0.5961, val/mmh/accuracy: 0.7378, val/mmh/binary_f1: 0.8472, val/mmh/roc_auc: 0.6364, num_updates: 22000, epoch: 3, iterations: 22000, max_updates: 22000, val_time: 03m 07s 273ms, best_update: 20000, best_iteration: 20000, best_val/mmh/roc_auc: 0.636570
[32m2022-10-17T16:40:00 | mmf.trainers.core.training_loop: [39mStepping into final validation check
[32m2022-10-17T16:40:00 | mmf.utils.checkpoint: [39mRestoring checkpoint
[32m2022-10-17T16:40:00 | mmf.utils.checkpoint: [39mLoading checkpoint
[32m2022-10-17T16:40:03 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-17T16:40:03 | mmf.utils.checkpoint: [39mCurrent num updates: 20000
[32m2022-10-17T16:40:03 | mmf.utils.checkpoint: [39mCurrent iteration: 20000
[32m2022-10-17T16:40:03 | mmf.utils.checkpoint: [39mCurrent epoch: 3
[32m2022-10-17T16:40:03 | mmf.utils.checkpoint: [39mSalvando o modelo final..
[32m2022-10-17T16:40:03 | mmf.trainers.mmf_trainer: [39mStarting inference on val set
[32m2022-10-17T16:40:03 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T16:40:03 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False





















100% 313/313 [00:43<00:00,  7.22it/s]
[32m2022-10-17T16:40:47 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T16:40:47 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T16:40:47 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, val/mmh/cross_entropy: 0.5908, val/total_loss: 0.5908, val/mmh/accuracy: 0.7392, val/mmh/binary_f1: 0.8493, val/mmh/roc_auc: 0.6366
[32m2022-10-17T16:40:47 | mmf.trainers.callbacks.logistics: [39mFinished run in 05h 27s 000ms
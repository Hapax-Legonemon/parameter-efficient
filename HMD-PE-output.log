
[32m2022-10-18T11:43:50 | mmf.utils.checkpoint: [39mLoading checkpoint
[31m[5mWARNING[39m[25m [32m2022-10-18T11:43:54 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-18T11:43:54 | mmf: [39mKey distributed is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-18T11:43:54 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-18T11:43:54 | mmf: [39mKey distributed is not present in registry, returning default value of None
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mPretrained model loaded
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCurrent num updates: 0
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCurrent iteration: 0
[32m2022-10-18T11:43:54 | mmf.utils.checkpoint: [39mCurrent epoch: 0
[32m2022-10-18T11:43:54 | mmf.trainers.mmf_trainer: [39m===== Model x=====
[32m2022-10-18T11:43:54 | mmf.trainers.mmf_trainer: [39mVisualBERT(
  (model): VisualBERTForClassification(
    (bert): VisualBERTBase(
      (embeddings): BertVisioLinguisticEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (token_type_embeddings_visual): Embedding(2, 768)
        (position_embeddings_visual): Embedding(512, 768)
        (projection): Linear(in_features=2048, out_features=768, bias=True)
      )
      (encoder): BertEncoderJit(
        (layer): ModuleList(
          (0): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (1): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (2): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (3): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (4): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (5): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (6): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (7): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (8): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (9): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (10): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
          (11): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (AdaptComb1): Linear(in_features=768, out_features=76, bias=True)
              (AdaptComb2): Linear(in_features=76, out_features=768, bias=True)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Sequential(
      (0): BertPredictionHeadTransform(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (1): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-10-18T11:43:54 | mmf.utils.general: [39mTotal Parameters: 113455250. Trained Parameters: 2004626
[32m2022-10-18T11:43:54 | mmf.trainers.core.training_loop: [39mStarting training...
[32m2022-10-18T11:44:51 | mmf.trainers.callbacks.logistics: [39mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.5715, train/hateful_memes/cross_entropy/avg: 0.5715, train/total_loss: 0.5715, train/total_loss/avg: 0.5715, max mem: 3189.0, experiment: hmd_a10, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.75, time: 57s 048ms, time_since_start: 01m 08s 662ms, eta: 03h 34m 53s 513ms
[32m2022-10-18T11:45:47 | mmf.trainers.callbacks.logistics: [39mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.5715, train/hateful_memes/cross_entropy/avg: 0.6039, train/total_loss: 0.5715, train/total_loss/avg: 0.6039, max mem: 3189.0, experiment: hmd_a10, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 507ms, time_since_start: 02m 04s 170ms, eta: 03h 28m 07s 963ms
[32m2022-10-18T11:46:43 | mmf.trainers.callbacks.logistics: [39mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6362, train/hateful_memes/cross_entropy/avg: 0.6287, train/total_loss: 0.6362, train/total_loss/avg: 0.6287, max mem: 3227.0, experiment: hmd_a10, epoch: 1, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 828ms, time_since_start: 02m 59s 998ms, eta: 03h 28m 22s 359ms
[32m2022-10-18T11:47:39 | mmf.trainers.callbacks.logistics: [39mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.5842, train/hateful_memes/cross_entropy/avg: 0.6175, train/total_loss: 0.5842, train/total_loss/avg: 0.6175, max mem: 3238.0, experiment: hmd_a10, epoch: 1, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 826ms, time_since_start: 03m 55s 824ms, eta: 03h 27m 24s 363ms
[32m2022-10-18T11:48:34 | mmf.trainers.callbacks.logistics: [39mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.5960, train/hateful_memes/cross_entropy/avg: 0.6132, train/total_loss: 0.5960, train/total_loss/avg: 0.6132, max mem: 3238.0, experiment: hmd_a10, epoch: 1, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.82, time: 55s 507ms, time_since_start: 04m 51s 332ms, eta: 03h 25m 16s 016ms
[32m2022-10-18T11:49:14 | mmf.trainers.callbacks.logistics: [39mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.5960, train/hateful_memes/cross_entropy/avg: 0.6166, train/total_loss: 0.5960, train/total_loss/avg: 0.6166, max mem: 3274.0, experiment: hmd_a10, epoch: 2, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 2.50, time: 40s 087ms, time_since_start: 05m 31s 419ms, eta: 02h 27m 33s 261ms
[32m2022-10-18T11:49:49 | mmf.trainers.callbacks.logistics: [39mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6084, train/hateful_memes/cross_entropy/avg: 0.6155, train/total_loss: 0.6084, train/total_loss/avg: 0.6155, max mem: 3302.0, experiment: hmd_a10, epoch: 2, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 2.86, time: 35s 150ms, time_since_start: 06m 06s 569ms, eta: 02h 08m 46s 578ms
[32m2022-10-18T11:50:27 | mmf.trainers.callbacks.logistics: [39mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.6072, train/hateful_memes/cross_entropy/avg: 0.6144, train/total_loss: 0.6072, train/total_loss/avg: 0.6144, max mem: 3302.0, experiment: hmd_a10, epoch: 2, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 2.70, time: 37s 396ms, time_since_start: 06m 43s 966ms, eta: 02h 16m 21s 825ms
[32m2022-10-18T11:51:09 | mmf.trainers.callbacks.logistics: [39mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6072, train/hateful_memes/cross_entropy/avg: 0.6052, train/total_loss: 0.6072, train/total_loss/avg: 0.6052, max mem: 3315.0, experiment: hmd_a10, epoch: 2, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 2.38, time: 42s 256ms, time_since_start: 07m 26s 222ms, eta: 02h 33m 21s 375ms
[32m2022-10-18T11:51:59 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T11:51:59 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T11:52:00 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T11:52:01 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T11:52:01 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.6072, train/hateful_memes/cross_entropy/avg: 0.6208, train/total_loss: 0.6072, train/total_loss/avg: 0.6208, max mem: 3338.0, experiment: hmd_a10, epoch: 2, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.96, time: 51s 716ms, time_since_start: 08m 17s 939ms, eta: 03h 06m 47s 979ms
[32m2022-10-18T11:52:01 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T11:52:01 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T11:52:01 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T11:52:13 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T11:52:13 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T11:52:14 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T11:52:17 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T11:52:17 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T11:52:20 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T11:52:20 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.7074, val/total_loss: 0.7074, val/hateful_memes/accuracy: 0.5600, val/hateful_memes/binary_f1: 0.3714, val/hateful_memes/roc_auc: 0.5811, num_updates: 1000, epoch: 2, iterations: 1000, max_updates: 22000, val_time: 19s 662ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.581124
[32m2022-10-18T11:53:09 | mmf.trainers.callbacks.logistics: [39mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.6084, train/hateful_memes/cross_entropy/avg: 0.6208, train/total_loss: 0.6084, train/total_loss/avg: 0.6208, max mem: 3338.0, experiment: hmd_a10, epoch: 3, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 2.08, time: 48s 180ms, time_since_start: 09m 25s 783ms, eta: 02h 53m 11s 922ms
[32m2022-10-18T11:53:39 | mmf.trainers.callbacks.logistics: [39mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.6072, train/hateful_memes/cross_entropy/avg: 0.6130, train/total_loss: 0.6072, train/total_loss/avg: 0.6130, max mem: 3338.0, experiment: hmd_a10, epoch: 3, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 3.45, time: 29s 968ms, time_since_start: 09m 55s 752ms, eta: 01h 47m 12s 958ms
[32m2022-10-18T11:54:13 | mmf.trainers.callbacks.logistics: [39mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.6084, train/hateful_memes/cross_entropy/avg: 0.6158, train/total_loss: 0.6084, train/total_loss/avg: 0.6158, max mem: 3338.0, experiment: hmd_a10, epoch: 3, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 2.94, time: 34s 176ms, time_since_start: 10m 29s 929ms, eta: 02h 01m 40s 950ms
[32m2022-10-18T11:54:49 | mmf.trainers.callbacks.logistics: [39mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.6084, train/hateful_memes/cross_entropy/avg: 0.6260, train/total_loss: 0.6084, train/total_loss/avg: 0.6260, max mem: 3338.0, experiment: hmd_a10, epoch: 3, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 2.78, time: 36s 612ms, time_since_start: 11m 06s 541ms, eta: 02h 09m 43s 498ms
[32m2022-10-18T11:55:29 | mmf.trainers.callbacks.logistics: [39mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.6207, train/hateful_memes/cross_entropy/avg: 0.6258, train/total_loss: 0.6207, train/total_loss/avg: 0.6258, max mem: 3338.0, experiment: hmd_a10, epoch: 3, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 2.56, time: 39s 306ms, time_since_start: 11m 45s 848ms, eta: 02h 18m 35s 696ms
[32m2022-10-18T11:56:12 | mmf.trainers.callbacks.logistics: [39mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.6084, train/hateful_memes/cross_entropy/avg: 0.6115, train/total_loss: 0.6084, train/total_loss/avg: 0.6115, max mem: 3338.0, experiment: hmd_a10, epoch: 4, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 2.33, time: 43s 692ms, time_since_start: 12m 29s 540ms, eta: 02h 33m 18s 495ms
[32m2022-10-18T11:56:34 | mmf.trainers.callbacks.logistics: [39mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.6084, train/hateful_memes/cross_entropy/avg: 0.6078, train/total_loss: 0.6084, train/total_loss/avg: 0.6078, max mem: 3338.0, experiment: hmd_a10, epoch: 4, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 394ms, time_since_start: 12m 50s 934ms, eta: 01h 14m 42s 002ms
[32m2022-10-18T11:56:55 | mmf.trainers.callbacks.logistics: [39mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.6072, train/hateful_memes/cross_entropy/avg: 0.5931, train/total_loss: 0.6072, train/total_loss/avg: 0.5931, max mem: 3338.0, experiment: hmd_a10, epoch: 4, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 4.76, time: 21s 765ms, time_since_start: 13m 12s 700ms, eta: 01h 15m 37s 302ms
[32m2022-10-18T11:57:18 | mmf.trainers.callbacks.logistics: [39mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.6072, train/hateful_memes/cross_entropy/avg: 0.5920, train/total_loss: 0.6072, train/total_loss/avg: 0.5920, max mem: 3338.0, experiment: hmd_a10, epoch: 4, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 4.55, time: 22s 705ms, time_since_start: 13m 35s 405ms, eta: 01h 18m 29s 846ms
[32m2022-10-18T11:57:45 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T11:57:45 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T11:57:46 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T11:57:49 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T11:57:49 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.6072, train/hateful_memes/cross_entropy/avg: 0.5959, train/total_loss: 0.6072, train/total_loss/avg: 0.5959, max mem: 3338.0, experiment: hmd_a10, epoch: 4, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 3.33, time: 30s 647ms, time_since_start: 14m 06s 053ms, eta: 01h 45m 25s 666ms
[32m2022-10-18T11:57:49 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T11:57:49 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T11:57:49 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T11:58:02 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T11:58:02 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T11:58:02 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T11:58:05 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T11:58:08 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T11:58:11 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T11:58:11 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.9047, val/total_loss: 0.9047, val/hateful_memes/accuracy: 0.5600, val/hateful_memes/binary_f1: 0.2667, val/hateful_memes/roc_auc: 0.6652, num_updates: 2000, epoch: 4, iterations: 2000, max_updates: 22000, val_time: 22s 487ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.665216
[32m2022-10-18T11:58:56 | mmf.trainers.callbacks.logistics: [39mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.6072, train/hateful_memes/cross_entropy/avg: 0.5845, train/total_loss: 0.6072, train/total_loss/avg: 0.5845, max mem: 3338.0, experiment: hmd_a10, epoch: 4, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 2.27, time: 44s 736ms, time_since_start: 15m 13s 282ms, eta: 02h 33m 07s 521ms
[32m2022-10-18T11:59:28 | mmf.trainers.callbacks.logistics: [39mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.5960, train/hateful_memes/cross_entropy/avg: 0.5843, train/total_loss: 0.5960, train/total_loss/avg: 0.5843, max mem: 3338.0, experiment: hmd_a10, epoch: 5, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 3.12, time: 32s 193ms, time_since_start: 15m 45s 476ms, eta: 01h 49m 38s 310ms
[32m2022-10-18T11:59:57 | mmf.trainers.callbacks.logistics: [39mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.5842, train/hateful_memes/cross_entropy/avg: 0.5775, train/total_loss: 0.5842, train/total_loss/avg: 0.5775, max mem: 3338.0, experiment: hmd_a10, epoch: 5, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 3.57, time: 28s 289ms, time_since_start: 16m 13s 766ms, eta: 01h 35m 51s 386ms
[32m2022-10-18T12:00:27 | mmf.trainers.callbacks.logistics: [39mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.5806, train/hateful_memes/cross_entropy/avg: 0.5665, train/total_loss: 0.5806, train/total_loss/avg: 0.5665, max mem: 3338.0, experiment: hmd_a10, epoch: 5, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 3.33, time: 30s 732ms, time_since_start: 16m 44s 498ms, eta: 01h 43m 36s 347ms
[32m2022-10-18T12:01:01 | mmf.trainers.callbacks.logistics: [39mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.5720, train/hateful_memes/cross_entropy/avg: 0.5606, train/total_loss: 0.5720, train/total_loss/avg: 0.5606, max mem: 3338.0, experiment: hmd_a10, epoch: 5, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 2.94, time: 34s 021ms, time_since_start: 17m 18s 519ms, eta: 01h 54m 06s 402ms
[32m2022-10-18T12:01:35 | mmf.trainers.callbacks.logistics: [39mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.5720, train/hateful_memes/cross_entropy/avg: 0.5743, train/total_loss: 0.5720, train/total_loss/avg: 0.5743, max mem: 3338.0, experiment: hmd_a10, epoch: 5, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 3.03, time: 33s 960ms, time_since_start: 17m 52s 480ms, eta: 01h 53m 19s 117ms
[32m2022-10-18T12:02:05 | mmf.trainers.callbacks.logistics: [39mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.5485, train/hateful_memes/cross_entropy/avg: 0.5663, train/total_loss: 0.5485, train/total_loss/avg: 0.5663, max mem: 3338.0, experiment: hmd_a10, epoch: 6, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 3.45, time: 29s 948ms, time_since_start: 18m 22s 428ms, eta: 01h 39m 25s 080ms
[32m2022-10-18T12:02:24 | mmf.trainers.callbacks.logistics: [39mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.5318, train/hateful_memes/cross_entropy/avg: 0.5559, train/total_loss: 0.5318, train/total_loss/avg: 0.5559, max mem: 3338.0, experiment: hmd_a10, epoch: 6, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 5.56, time: 18s 927ms, time_since_start: 18m 41s 356ms, eta: 01h 02m 30s 305ms
[32m2022-10-18T12:02:43 | mmf.trainers.callbacks.logistics: [39mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.5280, train/hateful_memes/cross_entropy/avg: 0.5449, train/total_loss: 0.5280, train/total_loss/avg: 0.5449, max mem: 3338.0, experiment: hmd_a10, epoch: 6, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 5.26, time: 19s 102ms, time_since_start: 19m 458ms, eta: 01h 02m 45s 372ms
[32m2022-10-18T12:03:03 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:03:03 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:03:04 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:03:07 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:03:07 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.4274, train/hateful_memes/cross_entropy/avg: 0.5406, train/total_loss: 0.4274, train/total_loss/avg: 0.5406, max mem: 3338.0, experiment: hmd_a10, epoch: 6, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 4.35, time: 23s 454ms, time_since_start: 19m 23s 913ms, eta: 01h 16m 39s 006ms
[32m2022-10-18T12:03:07 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:03:07 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:03:07 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:03:20 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:03:20 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:03:20 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:03:23 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T12:03:27 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:03:31 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:03:31 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, val/hateful_memes/cross_entropy: 0.7451, val/total_loss: 0.7451, val/hateful_memes/accuracy: 0.6260, val/hateful_memes/binary_f1: 0.5428, val/hateful_memes/roc_auc: 0.7015, num_updates: 3000, epoch: 6, iterations: 3000, max_updates: 22000, val_time: 24s 306ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.701525
[32m2022-10-18T12:04:06 | mmf.trainers.callbacks.logistics: [39mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.4175, train/hateful_memes/cross_entropy/avg: 0.5311, train/total_loss: 0.4175, train/total_loss/avg: 0.5311, max mem: 3338.0, experiment: hmd_a10, epoch: 6, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 2.86, time: 35s 353ms, time_since_start: 20m 23s 575ms, eta: 01h 54m 55s 696ms
[32m2022-10-18T12:04:44 | mmf.trainers.callbacks.logistics: [39mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.4146, train/hateful_memes/cross_entropy/avg: 0.5197, train/total_loss: 0.4146, train/total_loss/avg: 0.5197, max mem: 3338.0, experiment: hmd_a10, epoch: 7, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 2.70, time: 37s 741ms, time_since_start: 21m 01s 316ms, eta: 02h 02m 02s 477ms
[32m2022-10-18T12:05:07 | mmf.trainers.callbacks.logistics: [39mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.4146, train/hateful_memes/cross_entropy/avg: 0.5226, train/total_loss: 0.4146, train/total_loss/avg: 0.5226, max mem: 3338.0, experiment: hmd_a10, epoch: 7, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 4.55, time: 22s 458ms, time_since_start: 21m 23s 775ms, eta: 01h 12m 14s 057ms
[32m2022-10-18T12:05:30 | mmf.trainers.callbacks.logistics: [39mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.3971, train/hateful_memes/cross_entropy/avg: 0.5166, train/total_loss: 0.3971, train/total_loss/avg: 0.5166, max mem: 3338.0, experiment: hmd_a10, epoch: 7, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 4.35, time: 23s 450ms, time_since_start: 21m 47s 225ms, eta: 01h 15m 01s 327ms
[32m2022-10-18T12:05:55 | mmf.trainers.callbacks.logistics: [39mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.3971, train/hateful_memes/cross_entropy/avg: 0.5155, train/total_loss: 0.3971, train/total_loss/avg: 0.5155, max mem: 3338.0, experiment: hmd_a10, epoch: 7, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 4.00, time: 25s 251ms, time_since_start: 22m 12s 477ms, eta: 01h 20m 21s 086ms
[32m2022-10-18T12:06:21 | mmf.trainers.callbacks.logistics: [39mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.4146, train/hateful_memes/cross_entropy/avg: 0.5193, train/total_loss: 0.4146, train/total_loss/avg: 0.5193, max mem: 3338.0, experiment: hmd_a10, epoch: 7, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 4.00, time: 25s 524ms, time_since_start: 22m 38s 001ms, eta: 01h 20m 46s 842ms
[32m2022-10-18T12:06:47 | mmf.trainers.callbacks.logistics: [39mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.3637, train/hateful_memes/cross_entropy/avg: 0.5151, train/total_loss: 0.3637, train/total_loss/avg: 0.5151, max mem: 3338.0, experiment: hmd_a10, epoch: 7, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 3.85, time: 26s 334ms, time_since_start: 23m 04s 336ms, eta: 01h 22m 53s 356ms
[32m2022-10-18T12:07:07 | mmf.trainers.callbacks.logistics: [39mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.3637, train/hateful_memes/cross_entropy/avg: 0.5076, train/total_loss: 0.3637, train/total_loss/avg: 0.5076, max mem: 3338.0, experiment: hmd_a10, epoch: 8, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 5.00, time: 20s 288ms, time_since_start: 23m 24s 625ms, eta: 01h 03m 30s 758ms
[32m2022-10-18T12:07:25 | mmf.trainers.callbacks.logistics: [39mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.3637, train/hateful_memes/cross_entropy/avg: 0.5044, train/total_loss: 0.3637, train/total_loss/avg: 0.5044, max mem: 3338.0, experiment: hmd_a10, epoch: 8, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 5.88, time: 17s 850ms, time_since_start: 23m 42s 475ms, eta: 55m 34s 281ms
[32m2022-10-18T12:07:43 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:07:43 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:07:44 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:07:48 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:07:48 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.3591, train/hateful_memes/cross_entropy/avg: 0.5002, train/total_loss: 0.3591, train/total_loss/avg: 0.5002, max mem: 3338.0, experiment: hmd_a10, epoch: 8, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 4.55, time: 22s 398ms, time_since_start: 24m 04s 873ms, eta: 01h 09m 20s 688ms
[32m2022-10-18T12:07:48 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:07:48 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:07:48 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:07:53 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:07:53 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:07:53 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:07:57 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T12:08:01 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:08:04 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:08:04 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, val/hateful_memes/cross_entropy: 0.7293, val/total_loss: 0.7293, val/hateful_memes/accuracy: 0.6560, val/hateful_memes/binary_f1: 0.6000, val/hateful_memes/roc_auc: 0.7123, num_updates: 4000, epoch: 8, iterations: 4000, max_updates: 22000, val_time: 16s 584ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.712327
[32m2022-10-18T12:08:29 | mmf.trainers.callbacks.logistics: [39mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.3637, train/hateful_memes/cross_entropy/avg: 0.4984, train/total_loss: 0.3637, train/total_loss/avg: 0.4984, max mem: 3338.0, experiment: hmd_a10, epoch: 8, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 4.17, time: 24s 430ms, time_since_start: 24m 45s 891ms, eta: 01h 15m 13s 085ms
[32m2022-10-18T12:08:54 | mmf.trainers.callbacks.logistics: [39mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.3637, train/hateful_memes/cross_entropy/avg: 0.4975, train/total_loss: 0.3637, train/total_loss/avg: 0.4975, max mem: 3338.0, experiment: hmd_a10, epoch: 8, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 4.17, time: 24s 986ms, time_since_start: 25m 10s 878ms, eta: 01h 16m 29s 938ms
[32m2022-10-18T12:09:18 | mmf.trainers.callbacks.logistics: [39mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.3591, train/hateful_memes/cross_entropy/avg: 0.4937, train/total_loss: 0.3591, train/total_loss/avg: 0.4937, max mem: 3338.0, experiment: hmd_a10, epoch: 9, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 4.17, time: 24s 663ms, time_since_start: 25m 35s 541ms, eta: 01h 15m 05s 086ms
[32m2022-10-18T12:09:40 | mmf.trainers.callbacks.logistics: [39mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.3591, train/hateful_memes/cross_entropy/avg: 0.4905, train/total_loss: 0.3591, train/total_loss/avg: 0.4905, max mem: 3338.0, experiment: hmd_a10, epoch: 9, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 399ms, time_since_start: 25m 56s 940ms, eta: 01h 04m 46s 845ms
[32m2022-10-18T12:10:02 | mmf.trainers.callbacks.logistics: [39mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.3541, train/hateful_memes/cross_entropy/avg: 0.4857, train/total_loss: 0.3541, train/total_loss/avg: 0.4857, max mem: 3338.0, experiment: hmd_a10, epoch: 9, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 4.55, time: 22s 367ms, time_since_start: 26m 19s 308ms, eta: 01h 07m 19s 616ms
[32m2022-10-18T12:10:25 | mmf.trainers.callbacks.logistics: [39mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.3372, train/hateful_memes/cross_entropy/avg: 0.4815, train/total_loss: 0.3372, train/total_loss/avg: 0.4815, max mem: 3338.0, experiment: hmd_a10, epoch: 9, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 4.55, time: 22s 980ms, time_since_start: 26m 42s 289ms, eta: 01h 08m 46s 531ms
[32m2022-10-18T12:10:48 | mmf.trainers.callbacks.logistics: [39mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.3350, train/hateful_memes/cross_entropy/avg: 0.4758, train/total_loss: 0.3350, train/total_loss/avg: 0.4758, max mem: 3338.0, experiment: hmd_a10, epoch: 9, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 4.35, time: 23s 258ms, time_since_start: 27m 05s 547ms, eta: 01h 09m 12s 559ms
[32m2022-10-18T12:11:11 | mmf.trainers.callbacks.logistics: [39mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.3350, train/hateful_memes/cross_entropy/avg: 0.4686, train/total_loss: 0.3350, train/total_loss/avg: 0.4686, max mem: 3338.0, experiment: hmd_a10, epoch: 10, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 4.55, time: 22s 653ms, time_since_start: 27m 28s 201ms, eta: 01h 07m 01s 009ms
[32m2022-10-18T12:11:29 | mmf.trainers.callbacks.logistics: [39mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.3350, train/hateful_memes/cross_entropy/avg: 0.4644, train/total_loss: 0.3350, train/total_loss/avg: 0.4644, max mem: 3339.0, experiment: hmd_a10, epoch: 10, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 5.88, time: 17s 849ms, time_since_start: 27m 46s 050ms, eta: 52m 29s 979ms
[32m2022-10-18T12:11:47 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:11:47 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:11:47 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:11:50 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:11:50 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.3184, train/hateful_memes/cross_entropy/avg: 0.4577, train/total_loss: 0.3184, train/total_loss/avg: 0.4577, max mem: 3383.0, experiment: hmd_a10, epoch: 10, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 345ms, time_since_start: 28m 07s 396ms, eta: 01h 02m 24s 819ms
[32m2022-10-18T12:11:50 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:11:50 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:11:50 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:12:00 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:12:00 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:12:00 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:12:03 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T12:12:07 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:12:11 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:12:11 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, val/hateful_memes/cross_entropy: 0.9009, val/total_loss: 0.9009, val/hateful_memes/accuracy: 0.6200, val/hateful_memes/binary_f1: 0.5250, val/hateful_memes/roc_auc: 0.7134, num_updates: 5000, epoch: 10, iterations: 5000, max_updates: 22000, val_time: 20s 667ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.713431
[32m2022-10-18T12:12:37 | mmf.trainers.callbacks.logistics: [39mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.3350, train/hateful_memes/cross_entropy/avg: 0.4576, train/total_loss: 0.3350, train/total_loss/avg: 0.4576, max mem: 3383.0, experiment: hmd_a10, epoch: 10, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 3.85, time: 26s 432ms, time_since_start: 28m 54s 497ms, eta: 01h 16m 50s 029ms
[32m2022-10-18T12:13:04 | mmf.trainers.callbacks.logistics: [39mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.3350, train/hateful_memes/cross_entropy/avg: 0.4513, train/total_loss: 0.3350, train/total_loss/avg: 0.4513, max mem: 3383.0, experiment: hmd_a10, epoch: 10, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 3.85, time: 26s 850ms, time_since_start: 29m 21s 347ms, eta: 01h 17m 35s 193ms
[32m2022-10-18T12:13:34 | mmf.trainers.callbacks.logistics: [39mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.3350, train/hateful_memes/cross_entropy/avg: 0.4513, train/total_loss: 0.3350, train/total_loss/avg: 0.4513, max mem: 3383.0, experiment: hmd_a10, epoch: 10, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 3.45, time: 29s 545ms, time_since_start: 29m 50s 892ms, eta: 01h 24m 51s 918ms
[32m2022-10-18T12:13:58 | mmf.trainers.callbacks.logistics: [39mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.3350, train/hateful_memes/cross_entropy/avg: 0.4476, train/total_loss: 0.3350, train/total_loss/avg: 0.4476, max mem: 3383.0, experiment: hmd_a10, epoch: 11, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 4.17, time: 24s 590ms, time_since_start: 30m 15s 483ms, eta: 01h 10m 12s 627ms
[32m2022-10-18T12:14:22 | mmf.trainers.callbacks.logistics: [39mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.2919, train/hateful_memes/cross_entropy/avg: 0.4424, train/total_loss: 0.2919, train/total_loss/avg: 0.4424, max mem: 3383.0, experiment: hmd_a10, epoch: 11, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 4.35, time: 23s 378ms, time_since_start: 30m 38s 861ms, eta: 01h 06m 20s 830ms
[32m2022-10-18T12:14:47 | mmf.trainers.callbacks.logistics: [39mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.2919, train/hateful_memes/cross_entropy/avg: 0.4399, train/total_loss: 0.2919, train/total_loss/avg: 0.4399, max mem: 3383.0, experiment: hmd_a10, epoch: 11, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 4.00, time: 25s 671ms, time_since_start: 31m 04s 532ms, eta: 01h 12m 24s 876ms
[32m2022-10-18T12:15:13 | mmf.trainers.callbacks.logistics: [39mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.2919, train/hateful_memes/cross_entropy/avg: 0.4375, train/total_loss: 0.2919, train/total_loss/avg: 0.4375, max mem: 3383.0, experiment: hmd_a10, epoch: 11, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 3.85, time: 26s 112ms, time_since_start: 31m 30s 644ms, eta: 01h 13m 12s 469ms
[32m2022-10-18T12:15:39 | mmf.trainers.callbacks.logistics: [39mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.3006, train/hateful_memes/cross_entropy/avg: 0.4364, train/total_loss: 0.3006, train/total_loss/avg: 0.4364, max mem: 3383.0, experiment: hmd_a10, epoch: 11, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 4.00, time: 25s 397ms, time_since_start: 31m 56s 042ms, eta: 01h 10m 46s 136ms
[32m2022-10-18T12:16:00 | mmf.trainers.callbacks.logistics: [39mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.2919, train/hateful_memes/cross_entropy/avg: 0.4330, train/total_loss: 0.2919, train/total_loss/avg: 0.4330, max mem: 3383.0, experiment: hmd_a10, epoch: 12, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 603ms, time_since_start: 32m 17s 646ms, eta: 59m 49s 549ms
[32m2022-10-18T12:16:18 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:16:18 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:16:19 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:16:23 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:16:23 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.2745, train/hateful_memes/cross_entropy/avg: 0.4300, train/total_loss: 0.2745, train/total_loss/avg: 0.4300, max mem: 3408.0, experiment: hmd_a10, epoch: 12, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 4.55, time: 22s 189ms, time_since_start: 32m 39s 836ms, eta: 01h 01m 03s 955ms
[32m2022-10-18T12:16:23 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:16:23 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:16:23 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:16:28 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:16:28 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:16:28 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:16:32 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T12:16:36 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:16:40 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:16:40 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, val/hateful_memes/cross_entropy: 1.1126, val/total_loss: 1.1126, val/hateful_memes/accuracy: 0.6280, val/hateful_memes/binary_f1: 0.5105, val/hateful_memes/roc_auc: 0.7176, num_updates: 6000, epoch: 12, iterations: 6000, max_updates: 22000, val_time: 17s 316ms, best_update: 6000, best_iteration: 6000, best_val/hateful_memes/roc_auc: 0.717575
[32m2022-10-18T12:17:06 | mmf.trainers.callbacks.logistics: [39mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.2655, train/hateful_memes/cross_entropy/avg: 0.4268, train/total_loss: 0.2655, train/total_loss/avg: 0.4268, max mem: 3408.0, experiment: hmd_a10, epoch: 12, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 4.00, time: 25s 707ms, time_since_start: 33m 22s 862ms, eta: 01h 10m 18s 329ms
[32m2022-10-18T12:17:32 | mmf.trainers.callbacks.logistics: [39mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.2655, train/hateful_memes/cross_entropy/avg: 0.4244, train/total_loss: 0.2655, train/total_loss/avg: 0.4244, max mem: 3408.0, experiment: hmd_a10, epoch: 12, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 3.85, time: 26s 625ms, time_since_start: 33m 49s 487ms, eta: 01h 12m 21s 419ms
[32m2022-10-18T12:18:01 | mmf.trainers.callbacks.logistics: [39mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.2561, train/hateful_memes/cross_entropy/avg: 0.4210, train/total_loss: 0.2561, train/total_loss/avg: 0.4210, max mem: 3408.0, experiment: hmd_a10, epoch: 12, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 3.57, time: 28s 825ms, time_since_start: 34m 18s 313ms, eta: 01h 17m 50s 405ms
[32m2022-10-18T12:18:31 | mmf.trainers.callbacks.logistics: [39mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.2561, train/hateful_memes/cross_entropy/avg: 0.4226, train/total_loss: 0.2561, train/total_loss/avg: 0.4226, max mem: 3408.0, experiment: hmd_a10, epoch: 13, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 3.45, time: 29s 439ms, time_since_start: 34m 47s 753ms, eta: 01h 18m 59s 604ms
[32m2022-10-18T12:18:52 | mmf.trainers.callbacks.logistics: [39mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.2489, train/hateful_memes/cross_entropy/avg: 0.4197, train/total_loss: 0.2489, train/total_loss/avg: 0.4197, max mem: 3408.0, experiment: hmd_a10, epoch: 13, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 374ms, time_since_start: 35m 09s 127ms, eta: 56m 59s 011ms
[32m2022-10-18T12:19:14 | mmf.trainers.callbacks.logistics: [39mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.2383, train/hateful_memes/cross_entropy/avg: 0.4161, train/total_loss: 0.2383, train/total_loss/avg: 0.4161, max mem: 3408.0, experiment: hmd_a10, epoch: 13, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 722ms, time_since_start: 35m 30s 850ms, eta: 57m 32s 366ms
[32m2022-10-18T12:19:35 | mmf.trainers.callbacks.logistics: [39mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.2383, train/hateful_memes/cross_entropy/avg: 0.4133, train/total_loss: 0.2383, train/total_loss/avg: 0.4133, max mem: 3408.0, experiment: hmd_a10, epoch: 13, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 590ms, time_since_start: 35m 52s 440ms, eta: 56m 49s 092ms
[32m2022-10-18T12:19:57 | mmf.trainers.callbacks.logistics: [39mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.2489, train/hateful_memes/cross_entropy/avg: 0.4127, train/total_loss: 0.2489, train/total_loss/avg: 0.4127, max mem: 3408.0, experiment: hmd_a10, epoch: 13, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 444ms, time_since_start: 36m 13s 885ms, eta: 56m 03s 819ms
[32m2022-10-18T12:20:19 | mmf.trainers.callbacks.logistics: [39mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.2383, train/hateful_memes/cross_entropy/avg: 0.4094, train/total_loss: 0.2383, train/total_loss/avg: 0.4094, max mem: 3408.0, experiment: hmd_a10, epoch: 13, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 4.55, time: 22s 070ms, time_since_start: 36m 35s 955ms, eta: 57m 19s 322ms
[32m2022-10-18T12:20:37 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:20:37 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:20:38 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:20:41 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:20:41 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.2383, train/hateful_memes/cross_entropy/avg: 0.4043, train/total_loss: 0.2383, train/total_loss/avg: 0.4043, max mem: 3408.0, experiment: hmd_a10, epoch: 14, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 4.55, time: 22s 483ms, time_since_start: 36m 58s 439ms, eta: 58m 458ms
[32m2022-10-18T12:20:41 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:20:41 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:20:41 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:20:55 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:20:56 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:20:56 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:20:59 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T12:21:03 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:21:07 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:21:07 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, val/hateful_memes/cross_entropy: 1.1332, val/total_loss: 1.1332, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.5575, val/hateful_memes/roc_auc: 0.7227, num_updates: 7000, epoch: 14, iterations: 7000, max_updates: 22000, val_time: 25s 696ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.722680
[32m2022-10-18T12:21:32 | mmf.trainers.callbacks.logistics: [39mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.2330, train/hateful_memes/cross_entropy/avg: 0.4011, train/total_loss: 0.2330, train/total_loss/avg: 0.4011, max mem: 3408.0, experiment: hmd_a10, epoch: 14, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 4.00, time: 25s 070ms, time_since_start: 37m 49s 213ms, eta: 01h 04m 15s 094ms
[32m2022-10-18T12:21:56 | mmf.trainers.callbacks.logistics: [39mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.2330, train/hateful_memes/cross_entropy/avg: 0.3983, train/total_loss: 0.2330, train/total_loss/avg: 0.3983, max mem: 3408.0, experiment: hmd_a10, epoch: 14, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 4.17, time: 24s 391ms, time_since_start: 38m 13s 604ms, eta: 01h 02m 05s 429ms
[32m2022-10-18T12:22:24 | mmf.trainers.callbacks.logistics: [39mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.2330, train/hateful_memes/cross_entropy/avg: 0.3988, train/total_loss: 0.2330, train/total_loss/avg: 0.3988, max mem: 3408.0, experiment: hmd_a10, epoch: 14, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 3.70, time: 27s 245ms, time_since_start: 38m 40s 850ms, eta: 01h 08m 53s 231ms
[32m2022-10-18T12:22:56 | mmf.trainers.callbacks.logistics: [39mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.2317, train/hateful_memes/cross_entropy/avg: 0.3949, train/total_loss: 0.2317, train/total_loss/avg: 0.3949, max mem: 3408.0, experiment: hmd_a10, epoch: 14, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 3.12, time: 32s 187ms, time_since_start: 39m 13s 037ms, eta: 01h 20m 49s 700ms
[32m2022-10-18T12:23:24 | mmf.trainers.callbacks.logistics: [39mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.2330, train/hateful_memes/cross_entropy/avg: 0.3928, train/total_loss: 0.2330, train/total_loss/avg: 0.3928, max mem: 3408.0, experiment: hmd_a10, epoch: 15, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 3.70, time: 27s 712ms, time_since_start: 39m 40s 750ms, eta: 01h 09m 06s 953ms
[32m2022-10-18T12:23:44 | mmf.trainers.callbacks.logistics: [39mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.2330, train/hateful_memes/cross_entropy/avg: 0.3922, train/total_loss: 0.2330, train/total_loss/avg: 0.3922, max mem: 3408.0, experiment: hmd_a10, epoch: 15, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 5.00, time: 20s 945ms, time_since_start: 40m 01s 695ms, eta: 51m 52s 679ms
[32m2022-10-18T12:24:05 | mmf.trainers.callbacks.logistics: [39mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.2330, train/hateful_memes/cross_entropy/avg: 0.3921, train/total_loss: 0.2330, train/total_loss/avg: 0.3921, max mem: 3408.0, experiment: hmd_a10, epoch: 15, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 5.00, time: 20s 986ms, time_since_start: 40m 22s 682ms, eta: 51m 37s 152ms
[32m2022-10-18T12:24:26 | mmf.trainers.callbacks.logistics: [39mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.2317, train/hateful_memes/cross_entropy/avg: 0.3882, train/total_loss: 0.2317, train/total_loss/avg: 0.3882, max mem: 3408.0, experiment: hmd_a10, epoch: 15, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 4.76, time: 21s 006ms, time_since_start: 40m 43s 689ms, eta: 51m 18s 398ms
[32m2022-10-18T12:24:47 | mmf.trainers.callbacks.logistics: [39mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.2284, train/hateful_memes/cross_entropy/avg: 0.3856, train/total_loss: 0.2284, train/total_loss/avg: 0.3856, max mem: 3408.0, experiment: hmd_a10, epoch: 15, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 5.00, time: 20s 268ms, time_since_start: 41m 03s 957ms, eta: 49m 09s 300ms
[32m2022-10-18T12:25:07 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:25:07 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:25:08 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:25:13 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:25:13 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.2109, train/hateful_memes/cross_entropy/avg: 0.3818, train/total_loss: 0.2109, train/total_loss/avg: 0.3818, max mem: 3418.0, experiment: hmd_a10, epoch: 16, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 3.85, time: 26s 675ms, time_since_start: 41m 30s 633ms, eta: 01h 04m 14s 068ms
[32m2022-10-18T12:25:13 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:25:13 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:25:13 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:25:21 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:25:21 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:25:21 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:25:24 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T12:25:27 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:25:30 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:25:30 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, val/hateful_memes/cross_entropy: 1.6365, val/total_loss: 1.6365, val/hateful_memes/accuracy: 0.5980, val/hateful_memes/binary_f1: 0.4274, val/hateful_memes/roc_auc: 0.7256, num_updates: 8000, epoch: 16, iterations: 8000, max_updates: 22000, val_time: 16s 777ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.725636
[32m2022-10-18T12:25:55 | mmf.trainers.callbacks.logistics: [39mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.2001, train/hateful_memes/cross_entropy/avg: 0.3779, train/total_loss: 0.2001, train/total_loss/avg: 0.3779, max mem: 3418.0, experiment: hmd_a10, epoch: 16, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 4.17, time: 24s 405ms, time_since_start: 42m 11s 821ms, eta: 58m 20s 853ms
[32m2022-10-18T12:26:20 | mmf.trainers.callbacks.logistics: [39mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.1829, train/hateful_memes/cross_entropy/avg: 0.3746, train/total_loss: 0.1829, train/total_loss/avg: 0.3746, max mem: 3418.0, experiment: hmd_a10, epoch: 16, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 4.00, time: 25s 006ms, time_since_start: 42m 36s 828ms, eta: 59m 21s 367ms
[32m2022-10-18T12:26:47 | mmf.trainers.callbacks.logistics: [39mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.1829, train/hateful_memes/cross_entropy/avg: 0.3715, train/total_loss: 0.1829, train/total_loss/avg: 0.3715, max mem: 3418.0, experiment: hmd_a10, epoch: 16, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 3.70, time: 27s 314ms, time_since_start: 43m 04s 142ms, eta: 01h 04m 21s 791ms
[32m2022-10-18T12:27:19 | mmf.trainers.callbacks.logistics: [39mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.1826, train/hateful_memes/cross_entropy/avg: 0.3690, train/total_loss: 0.1826, train/total_loss/avg: 0.3690, max mem: 3418.0, experiment: hmd_a10, epoch: 16, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 3.12, time: 32s 096ms, time_since_start: 43m 36s 238ms, eta: 01h 15m 04s 774ms
[32m2022-10-18T12:27:52 | mmf.trainers.callbacks.logistics: [39mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.1785, train/hateful_memes/cross_entropy/avg: 0.3657, train/total_loss: 0.1785, train/total_loss/avg: 0.3657, max mem: 3418.0, experiment: hmd_a10, epoch: 16, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 3.12, time: 32s 625ms, time_since_start: 44m 08s 864ms, eta: 01h 15m 45s 444ms
[32m2022-10-18T12:28:12 | mmf.trainers.callbacks.logistics: [39mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.1665, train/hateful_memes/cross_entropy/avg: 0.3620, train/total_loss: 0.1665, train/total_loss/avg: 0.3620, max mem: 3418.0, experiment: hmd_a10, epoch: 17, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 5.00, time: 20s 006ms, time_since_start: 44m 28s 871ms, eta: 46m 06s 695ms
[32m2022-10-18T12:28:30 | mmf.trainers.callbacks.logistics: [39mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.1181, train/hateful_memes/cross_entropy/avg: 0.3584, train/total_loss: 0.1181, train/total_loss/avg: 0.3584, max mem: 3418.0, experiment: hmd_a10, epoch: 17, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 5.56, time: 18s 218ms, time_since_start: 44m 47s 090ms, eta: 41m 40s 630ms
[32m2022-10-18T12:28:48 | mmf.trainers.callbacks.logistics: [39mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.1181, train/hateful_memes/cross_entropy/avg: 0.3584, train/total_loss: 0.1181, train/total_loss/avg: 0.3584, max mem: 3418.0, experiment: hmd_a10, epoch: 17, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 5.56, time: 18s 269ms, time_since_start: 45m 05s 359ms, eta: 41m 28s 684ms
[32m2022-10-18T12:29:07 | mmf.trainers.callbacks.logistics: [39mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.1181, train/hateful_memes/cross_entropy/avg: 0.3569, train/total_loss: 0.1181, train/total_loss/avg: 0.3569, max mem: 3418.0, experiment: hmd_a10, epoch: 17, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 5.56, time: 18s 597ms, time_since_start: 45m 23s 956ms, eta: 41m 54s 196ms
[32m2022-10-18T12:29:25 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:29:25 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:29:26 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:29:29 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:29:29 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.1665, train/hateful_memes/cross_entropy/avg: 0.3549, train/total_loss: 0.1665, train/total_loss/avg: 0.3549, max mem: 3418.0, experiment: hmd_a10, epoch: 17, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 4.55, time: 22s 067ms, time_since_start: 45m 46s 024ms, eta: 49m 20s 630ms
[32m2022-10-18T12:29:29 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:29:29 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:29:29 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:29:41 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:29:41 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:29:41 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:29:44 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:29:48 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:29:48 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, val/hateful_memes/cross_entropy: 1.4207, val/total_loss: 1.4207, val/hateful_memes/accuracy: 0.6180, val/hateful_memes/binary_f1: 0.5330, val/hateful_memes/roc_auc: 0.7196, num_updates: 9000, epoch: 17, iterations: 9000, max_updates: 22000, val_time: 18s 917ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.725636
[32m2022-10-18T12:30:13 | mmf.trainers.callbacks.logistics: [39mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.1181, train/hateful_memes/cross_entropy/avg: 0.3522, train/total_loss: 0.1181, train/total_loss/avg: 0.3522, max mem: 3418.0, experiment: hmd_a10, epoch: 18, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 4.00, time: 25s 693ms, time_since_start: 46m 30s 641ms, eta: 57m 538ms
[32m2022-10-18T12:30:35 | mmf.trainers.callbacks.logistics: [39mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.1101, train/hateful_memes/cross_entropy/avg: 0.3490, train/total_loss: 0.1101, train/total_loss/avg: 0.3490, max mem: 3418.0, experiment: hmd_a10, epoch: 18, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 4.76, time: 21s 221ms, time_since_start: 46m 51s 863ms, eta: 46m 43s 332ms
[32m2022-10-18T12:30:57 | mmf.trainers.callbacks.logistics: [39mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.1101, train/hateful_memes/cross_entropy/avg: 0.3470, train/total_loss: 0.1101, train/total_loss/avg: 0.3470, max mem: 3418.0, experiment: hmd_a10, epoch: 18, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 4.55, time: 22s 751ms, time_since_start: 47m 14s 614ms, eta: 49m 41s 873ms
[32m2022-10-18T12:31:23 | mmf.trainers.callbacks.logistics: [39mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.1181, train/hateful_memes/cross_entropy/avg: 0.3475, train/total_loss: 0.1181, train/total_loss/avg: 0.3475, max mem: 3418.0, experiment: hmd_a10, epoch: 18, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 4.00, time: 25s 290ms, time_since_start: 47m 39s 905ms, eta: 54m 48s 617ms
[32m2022-10-18T12:31:52 | mmf.trainers.callbacks.logistics: [39mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.1068, train/hateful_memes/cross_entropy/avg: 0.3448, train/total_loss: 0.1068, train/total_loss/avg: 0.3448, max mem: 3418.0, experiment: hmd_a10, epoch: 18, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 3.45, time: 29s 311ms, time_since_start: 48m 09s 217ms, eta: 01h 03m 01s 240ms
[32m2022-10-18T12:32:20 | mmf.trainers.callbacks.logistics: [39mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.1050, train/hateful_memes/cross_entropy/avg: 0.3422, train/total_loss: 0.1050, train/total_loss/avg: 0.3422, max mem: 3418.0, experiment: hmd_a10, epoch: 19, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 3.57, time: 28s 305ms, time_since_start: 48m 37s 522ms, eta: 01h 22s 189ms
[32m2022-10-18T12:32:38 | mmf.trainers.callbacks.logistics: [39mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0946, train/hateful_memes/cross_entropy/avg: 0.3389, train/total_loss: 0.0946, train/total_loss/avg: 0.3389, max mem: 3418.0, experiment: hmd_a10, epoch: 19, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 5.88, time: 17s 854ms, time_since_start: 48m 55s 376ms, eta: 37m 46s 358ms
[32m2022-10-18T12:32:56 | mmf.trainers.callbacks.logistics: [39mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0946, train/hateful_memes/cross_entropy/avg: 0.3359, train/total_loss: 0.0946, train/total_loss/avg: 0.3359, max mem: 3418.0, experiment: hmd_a10, epoch: 19, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 5.88, time: 17s 873ms, time_since_start: 49m 13s 250ms, eta: 37m 30s 334ms
[32m2022-10-18T12:33:14 | mmf.trainers.callbacks.logistics: [39mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0946, train/hateful_memes/cross_entropy/avg: 0.3342, train/total_loss: 0.0946, train/total_loss/avg: 0.3342, max mem: 3418.0, experiment: hmd_a10, epoch: 19, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 5.88, time: 17s 899ms, time_since_start: 49m 31s 150ms, eta: 37m 15s 204ms
[32m2022-10-18T12:33:32 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:33:32 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:33:32 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:33:36 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:33:36 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.1050, train/hateful_memes/cross_entropy/avg: 0.3331, train/total_loss: 0.1050, train/total_loss/avg: 0.3331, max mem: 3418.0, experiment: hmd_a10, epoch: 19, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 4.76, time: 21s 964ms, time_since_start: 49m 53s 115ms, eta: 45m 20s 133ms
[32m2022-10-18T12:33:36 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:33:36 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:33:36 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:33:42 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:33:42 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:33:42 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:33:46 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:33:50 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:33:50 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, val/hateful_memes/cross_entropy: 1.5516, val/total_loss: 1.5516, val/hateful_memes/accuracy: 0.6300, val/hateful_memes/binary_f1: 0.5499, val/hateful_memes/roc_auc: 0.7199, num_updates: 10000, epoch: 19, iterations: 10000, max_updates: 22000, val_time: 13s 943ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.725636
[32m2022-10-18T12:34:21 | mmf.trainers.callbacks.logistics: [39mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.1068, train/hateful_memes/cross_entropy/avg: 0.3314, train/total_loss: 0.1068, train/total_loss/avg: 0.3314, max mem: 3418.0, experiment: hmd_a10, epoch: 19, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 3.23, time: 31s 434ms, time_since_start: 50m 38s 495ms, eta: 01h 04m 20s 467ms
[32m2022-10-18T12:34:43 | mmf.trainers.callbacks.logistics: [39mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.1068, train/hateful_memes/cross_entropy/avg: 0.3283, train/total_loss: 0.1068, train/total_loss/avg: 0.3283, max mem: 3418.0, experiment: hmd_a10, epoch: 20, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 4.76, time: 21s 709ms, time_since_start: 51m 204ms, eta: 44m 03s 667ms
[32m2022-10-18T12:35:05 | mmf.trainers.callbacks.logistics: [39mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.1068, train/hateful_memes/cross_entropy/avg: 0.3295, train/total_loss: 0.1068, train/total_loss/avg: 0.3295, max mem: 3418.0, experiment: hmd_a10, epoch: 20, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 4.76, time: 21s 888ms, time_since_start: 51m 22s 093ms, eta: 44m 02s 953ms
[32m2022-10-18T12:35:28 | mmf.trainers.callbacks.logistics: [39mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0946, train/hateful_memes/cross_entropy/avg: 0.3267, train/total_loss: 0.0946, train/total_loss/avg: 0.3267, max mem: 3418.0, experiment: hmd_a10, epoch: 20, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 4.35, time: 23s 395ms, time_since_start: 51m 45s 489ms, eta: 46m 40s 747ms
[32m2022-10-18T12:35:54 | mmf.trainers.callbacks.logistics: [39mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.1068, train/hateful_memes/cross_entropy/avg: 0.3268, train/total_loss: 0.1068, train/total_loss/avg: 0.3268, max mem: 3418.0, experiment: hmd_a10, epoch: 20, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 4.00, time: 25s 925ms, time_since_start: 52m 11s 414ms, eta: 51m 16s 797ms
[32m2022-10-18T12:36:26 | mmf.trainers.callbacks.logistics: [39mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.1068, train/hateful_memes/cross_entropy/avg: 0.3247, train/total_loss: 0.1068, train/total_loss/avg: 0.3247, max mem: 3418.0, experiment: hmd_a10, epoch: 20, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 3.23, time: 31s 934ms, time_since_start: 52m 43s 349ms, eta: 01h 02m 37s 046ms
[32m2022-10-18T12:36:49 | mmf.trainers.callbacks.logistics: [39mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.1068, train/hateful_memes/cross_entropy/avg: 0.3219, train/total_loss: 0.1068, train/total_loss/avg: 0.3219, max mem: 3418.0, experiment: hmd_a10, epoch: 21, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 4.55, time: 22s 692ms, time_since_start: 53m 06s 041ms, eta: 44m 06s 339ms
[32m2022-10-18T12:37:07 | mmf.trainers.callbacks.logistics: [39mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0979, train/hateful_memes/cross_entropy/avg: 0.3191, train/total_loss: 0.0979, train/total_loss/avg: 0.3191, max mem: 3418.0, experiment: hmd_a10, epoch: 21, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 5.88, time: 17s 885ms, time_since_start: 53m 23s 927ms, eta: 34m 27s 309ms
[32m2022-10-18T12:37:25 | mmf.trainers.callbacks.logistics: [39mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0979, train/hateful_memes/cross_entropy/avg: 0.3188, train/total_loss: 0.0979, train/total_loss/avg: 0.3188, max mem: 3418.0, experiment: hmd_a10, epoch: 21, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 5.88, time: 17s 928ms, time_since_start: 53m 41s 856ms, eta: 34m 13s 755ms
[32m2022-10-18T12:37:43 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:37:43 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:37:43 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:37:47 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:37:47 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0979, train/hateful_memes/cross_entropy/avg: 0.3172, train/total_loss: 0.0979, train/total_loss/avg: 0.3172, max mem: 3418.0, experiment: hmd_a10, epoch: 21, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 4.55, time: 22s 313ms, time_since_start: 54m 04s 170ms, eta: 42m 13s 082ms
[32m2022-10-18T12:37:47 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:37:47 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:37:47 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:37:57 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:37:57 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:37:57 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:38:01 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:38:04 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:38:04 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, val/hateful_memes/cross_entropy: 1.5071, val/total_loss: 1.5071, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5828, val/hateful_memes/roc_auc: 0.7224, num_updates: 11000, epoch: 21, iterations: 11000, max_updates: 22000, val_time: 16s 757ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.725636
[32m2022-10-18T12:38:34 | mmf.trainers.callbacks.logistics: [39mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0946, train/hateful_memes/cross_entropy/avg: 0.3145, train/total_loss: 0.0946, train/total_loss/avg: 0.3145, max mem: 3418.0, experiment: hmd_a10, epoch: 21, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 3.33, time: 30s 532ms, time_since_start: 54m 51s 462ms, eta: 57m 14s 511ms
[32m2022-10-18T12:39:04 | mmf.trainers.callbacks.logistics: [39mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0946, train/hateful_memes/cross_entropy/avg: 0.3122, train/total_loss: 0.0946, train/total_loss/avg: 0.3122, max mem: 3418.0, experiment: hmd_a10, epoch: 22, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 3.45, time: 29s 494ms, time_since_start: 55m 20s 956ms, eta: 54m 47s 372ms
[32m2022-10-18T12:39:25 | mmf.trainers.callbacks.logistics: [39mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0939, train/hateful_memes/cross_entropy/avg: 0.3098, train/total_loss: 0.0939, train/total_loss/avg: 0.3098, max mem: 3418.0, experiment: hmd_a10, epoch: 22, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 4.76, time: 21s 144ms, time_since_start: 55m 42s 101ms, eta: 38m 54s 837ms
[32m2022-10-18T12:39:47 | mmf.trainers.callbacks.logistics: [39mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0546, train/hateful_memes/cross_entropy/avg: 0.3073, train/total_loss: 0.0546, train/total_loss/avg: 0.3073, max mem: 3418.0, experiment: hmd_a10, epoch: 22, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 4.55, time: 22s 405ms, time_since_start: 56m 04s 506ms, eta: 40m 50s 997ms
[32m2022-10-18T12:40:11 | mmf.trainers.callbacks.logistics: [39mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0546, train/hateful_memes/cross_entropy/avg: 0.3052, train/total_loss: 0.0546, train/total_loss/avg: 0.3052, max mem: 3418.0, experiment: hmd_a10, epoch: 22, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 4.35, time: 23s 645ms, time_since_start: 56m 28s 152ms, eta: 42m 42s 255ms
[32m2022-10-18T12:40:37 | mmf.trainers.callbacks.logistics: [39mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0546, train/hateful_memes/cross_entropy/avg: 0.3040, train/total_loss: 0.0546, train/total_loss/avg: 0.3040, max mem: 3418.0, experiment: hmd_a10, epoch: 22, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 3.85, time: 26s 454ms, time_since_start: 56m 54s 607ms, eta: 47m 19s 300ms
[32m2022-10-18T12:41:06 | mmf.trainers.callbacks.logistics: [39mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0597, train/hateful_memes/cross_entropy/avg: 0.3039, train/total_loss: 0.0597, train/total_loss/avg: 0.3039, max mem: 3418.0, experiment: hmd_a10, epoch: 22, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 3.57, time: 28s 777ms, time_since_start: 57m 23s 384ms, eta: 50m 58s 917ms
[32m2022-10-18T12:41:24 | mmf.trainers.callbacks.logistics: [39mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0793, train/hateful_memes/cross_entropy/avg: 0.3020, train/total_loss: 0.0793, train/total_loss/avg: 0.3020, max mem: 3418.0, experiment: hmd_a10, epoch: 23, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 5.56, time: 18s 129ms, time_since_start: 57m 41s 513ms, eta: 31m 48s 363ms
[32m2022-10-18T12:41:42 | mmf.trainers.callbacks.logistics: [39mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0722, train/hateful_memes/cross_entropy/avg: 0.3000, train/total_loss: 0.0722, train/total_loss/avg: 0.3000, max mem: 3429.0, experiment: hmd_a10, epoch: 23, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 5.88, time: 17s 952ms, time_since_start: 57m 59s 466ms, eta: 31m 11s 229ms
[32m2022-10-18T12:42:00 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:42:00 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:42:01 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:42:04 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:42:04 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0722, train/hateful_memes/cross_entropy/avg: 0.2982, train/total_loss: 0.0722, train/total_loss/avg: 0.2982, max mem: 3429.0, experiment: hmd_a10, epoch: 23, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 4.76, time: 21s 633ms, time_since_start: 58m 21s 099ms, eta: 37m 12s 574ms
[32m2022-10-18T12:42:04 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:42:04 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:42:04 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:42:10 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:42:10 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:42:10 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:42:13 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:42:16 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:42:16 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, val/hateful_memes/cross_entropy: 1.8141, val/total_loss: 1.8141, val/hateful_memes/accuracy: 0.6340, val/hateful_memes/binary_f1: 0.5569, val/hateful_memes/roc_auc: 0.7229, num_updates: 12000, epoch: 23, iterations: 12000, max_updates: 22000, val_time: 12s 571ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.725636
[32m2022-10-18T12:42:44 | mmf.trainers.callbacks.logistics: [39mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0722, train/hateful_memes/cross_entropy/avg: 0.2972, train/total_loss: 0.0722, train/total_loss/avg: 0.2972, max mem: 3429.0, experiment: hmd_a10, epoch: 23, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 3.70, time: 27s 041ms, time_since_start: 59m 718ms, eta: 46m 02s 770ms
[32m2022-10-18T12:43:11 | mmf.trainers.callbacks.logistics: [39mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0722, train/hateful_memes/cross_entropy/avg: 0.2953, train/total_loss: 0.0722, train/total_loss/avg: 0.2953, max mem: 3429.0, experiment: hmd_a10, epoch: 23, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 3.70, time: 27s 828ms, time_since_start: 59m 28s 546ms, eta: 46m 54s 425ms
[32m2022-10-18T12:43:36 | mmf.trainers.callbacks.logistics: [39mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.2932, train/total_loss: 0.0621, train/total_loss/avg: 0.2932, max mem: 3429.0, experiment: hmd_a10, epoch: 24, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 4.17, time: 24s 882ms, time_since_start: 59m 53s 428ms, eta: 41m 30s 818ms
[32m2022-10-18T12:43:58 | mmf.trainers.callbacks.logistics: [39mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.2910, train/total_loss: 0.0621, train/total_loss/avg: 0.2910, max mem: 3429.0, experiment: hmd_a10, epoch: 24, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 4.76, time: 21s 406ms, time_since_start: 01h 14s 835ms, eta: 35m 20s 822ms
[32m2022-10-18T12:44:20 | mmf.trainers.callbacks.logistics: [39mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.2900, train/total_loss: 0.0621, train/total_loss/avg: 0.2900, max mem: 3429.0, experiment: hmd_a10, epoch: 24, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 305ms, time_since_start: 01h 37s 141ms, eta: 36m 26s 869ms
[32m2022-10-18T12:44:44 | mmf.trainers.callbacks.logistics: [39mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0597, train/hateful_memes/cross_entropy/avg: 0.2878, train/total_loss: 0.0597, train/total_loss/avg: 0.2878, max mem: 3429.0, experiment: hmd_a10, epoch: 24, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 4.17, time: 24s 100ms, time_since_start: 01h 01m 01s 242ms, eta: 38m 57s 974ms
[32m2022-10-18T12:45:08 | mmf.trainers.callbacks.logistics: [39mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.2861, train/total_loss: 0.0621, train/total_loss/avg: 0.2861, max mem: 3429.0, experiment: hmd_a10, epoch: 24, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 4.17, time: 24s 131ms, time_since_start: 01h 01m 25s 373ms, eta: 38m 36s 047ms
[32m2022-10-18T12:45:31 | mmf.trainers.callbacks.logistics: [39mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0666, train/hateful_memes/cross_entropy/avg: 0.2873, train/total_loss: 0.0666, train/total_loss/avg: 0.2873, max mem: 3429.0, experiment: hmd_a10, epoch: 25, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 394ms, time_since_start: 01h 01m 47s 768ms, eta: 35m 26s 193ms
[32m2022-10-18T12:45:48 | mmf.trainers.callbacks.logistics: [39mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.2851, train/total_loss: 0.0621, train/total_loss/avg: 0.2851, max mem: 3429.0, experiment: hmd_a10, epoch: 25, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 5.88, time: 17s 875ms, time_since_start: 01h 02m 05s 643ms, eta: 27m 58s 755ms
[32m2022-10-18T12:46:06 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:46:06 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:46:07 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:46:10 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:46:10 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0597, train/hateful_memes/cross_entropy/avg: 0.2831, train/total_loss: 0.0597, train/total_loss/avg: 0.2831, max mem: 3429.0, experiment: hmd_a10, epoch: 25, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 4.76, time: 21s 455ms, time_since_start: 01h 02m 27s 099ms, eta: 33m 12s 751ms
[32m2022-10-18T12:46:10 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:46:10 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:46:10 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:46:20 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:46:20 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:46:20 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.1174, val/total_loss: 2.1174, val/hateful_memes/accuracy: 0.6220, val/hateful_memes/binary_f1: 0.5013, val/hateful_memes/roc_auc: 0.7193, num_updates: 13000, epoch: 25, iterations: 13000, max_updates: 22000, val_time: 10s 398ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.725636
[32m2022-10-18T12:46:46 | mmf.trainers.callbacks.logistics: [39mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0597, train/hateful_memes/cross_entropy/avg: 0.2811, train/total_loss: 0.0597, train/total_loss/avg: 0.2811, max mem: 3429.0, experiment: hmd_a10, epoch: 25, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 3.85, time: 26s 118ms, time_since_start: 01h 03m 03s 617ms, eta: 39m 58s 909ms
[32m2022-10-18T12:47:10 | mmf.trainers.callbacks.logistics: [39mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0597, train/hateful_memes/cross_entropy/avg: 0.2790, train/total_loss: 0.0597, train/total_loss/avg: 0.2790, max mem: 3429.0, experiment: hmd_a10, epoch: 25, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 4.35, time: 23s 652ms, time_since_start: 01h 03m 27s 270ms, eta: 35m 48s 059ms
[32m2022-10-18T12:47:35 | mmf.trainers.callbacks.logistics: [39mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.2787, train/total_loss: 0.0621, train/total_loss/avg: 0.2787, max mem: 3429.0, experiment: hmd_a10, epoch: 25, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 4.17, time: 24s 900ms, time_since_start: 01h 03m 52s 171ms, eta: 37m 15s 711ms
[32m2022-10-18T12:47:54 | mmf.trainers.callbacks.logistics: [39mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.2768, train/total_loss: 0.0621, train/total_loss/avg: 0.2768, max mem: 3429.0, experiment: hmd_a10, epoch: 26, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 993ms, time_since_start: 01h 04m 11s 164ms, eta: 28m 05s 731ms
[32m2022-10-18T12:48:13 | mmf.trainers.callbacks.logistics: [39mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0666, train/hateful_memes/cross_entropy/avg: 0.2752, train/total_loss: 0.0666, train/total_loss/avg: 0.2752, max mem: 3429.0, experiment: hmd_a10, epoch: 26, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 447ms, time_since_start: 01h 04m 30s 612ms, eta: 28m 25s 963ms
[32m2022-10-18T12:48:34 | mmf.trainers.callbacks.logistics: [39mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.2733, train/total_loss: 0.0621, train/total_loss/avg: 0.2733, max mem: 3429.0, experiment: hmd_a10, epoch: 26, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 5.00, time: 20s 183ms, time_since_start: 01h 04m 50s 796ms, eta: 29m 09s 692ms
[32m2022-10-18T12:48:54 | mmf.trainers.callbacks.logistics: [39mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.2722, train/total_loss: 0.0621, train/total_loss/avg: 0.2722, max mem: 3429.0, experiment: hmd_a10, epoch: 26, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 5.00, time: 20s 696ms, time_since_start: 01h 05m 11s 493ms, eta: 29m 32s 808ms
[32m2022-10-18T12:49:16 | mmf.trainers.callbacks.logistics: [39mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0621, train/hateful_memes/cross_entropy/avg: 0.2717, train/total_loss: 0.0621, train/total_loss/avg: 0.2717, max mem: 3429.0, experiment: hmd_a10, epoch: 26, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 4.76, time: 21s 266ms, time_since_start: 01h 05m 32s 759ms, eta: 29m 59s 626ms
[32m2022-10-18T12:49:35 | mmf.trainers.callbacks.logistics: [39mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0358, train/hateful_memes/cross_entropy/avg: 0.2700, train/total_loss: 0.0358, train/total_loss/avg: 0.2700, max mem: 3429.0, experiment: hmd_a10, epoch: 27, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 137ms, time_since_start: 01h 05m 51s 896ms, eta: 26m 39s 715ms
[32m2022-10-18T12:49:53 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:49:53 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:49:53 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:49:56 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:49:56 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0358, train/hateful_memes/cross_entropy/avg: 0.2700, train/total_loss: 0.0358, train/total_loss/avg: 0.2700, max mem: 3429.0, experiment: hmd_a10, epoch: 27, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 4.76, time: 21s 707ms, time_since_start: 01h 06m 13s 604ms, eta: 29m 52s 171ms
[32m2022-10-18T12:49:56 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:49:56 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:49:56 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:50:03 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:50:03 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:50:03 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.1099, val/total_loss: 2.1099, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5645, val/hateful_memes/roc_auc: 0.7211, num_updates: 14000, epoch: 27, iterations: 14000, max_updates: 22000, val_time: 06s 525ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.725636
[32m2022-10-18T12:50:28 | mmf.trainers.callbacks.logistics: [39mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0358, train/hateful_memes/cross_entropy/avg: 0.2703, train/total_loss: 0.0358, train/total_loss/avg: 0.2703, max mem: 3429.0, experiment: hmd_a10, epoch: 27, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 4.00, time: 25s 521ms, time_since_start: 01h 06m 45s 653ms, eta: 34m 40s 712ms
[32m2022-10-18T12:50:51 | mmf.trainers.callbacks.logistics: [39mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.2685, train/total_loss: 0.0258, train/total_loss/avg: 0.2685, max mem: 3429.0, experiment: hmd_a10, epoch: 27, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 520ms, time_since_start: 01h 07m 08s 173ms, eta: 30m 12s 808ms
[32m2022-10-18T12:51:13 | mmf.trainers.callbacks.logistics: [39mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.2669, train/total_loss: 0.0258, train/total_loss/avg: 0.2669, max mem: 3429.0, experiment: hmd_a10, epoch: 27, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 221ms, time_since_start: 01h 07m 30s 395ms, eta: 29m 25s 833ms
[32m2022-10-18T12:51:35 | mmf.trainers.callbacks.logistics: [39mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.2651, train/total_loss: 0.0258, train/total_loss/avg: 0.2651, max mem: 3429.0, experiment: hmd_a10, epoch: 28, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 4.76, time: 21s 555ms, time_since_start: 01h 07m 51s 950ms, eta: 28m 10s 623ms
[32m2022-10-18T12:51:54 | mmf.trainers.callbacks.logistics: [39mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0256, train/hateful_memes/cross_entropy/avg: 0.2634, train/total_loss: 0.0256, train/total_loss/avg: 0.2634, max mem: 3429.0, experiment: hmd_a10, epoch: 28, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 995ms, time_since_start: 01h 08m 10s 946ms, eta: 24m 30s 288ms
[32m2022-10-18T12:52:13 | mmf.trainers.callbacks.logistics: [39mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.2618, train/total_loss: 0.0258, train/total_loss/avg: 0.2618, max mem: 3429.0, experiment: hmd_a10, epoch: 28, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 453ms, time_since_start: 01h 08m 30s 400ms, eta: 24m 45s 656ms
[32m2022-10-18T12:52:33 | mmf.trainers.callbacks.logistics: [39mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0256, train/hateful_memes/cross_entropy/avg: 0.2601, train/total_loss: 0.0256, train/total_loss/avg: 0.2601, max mem: 3429.0, experiment: hmd_a10, epoch: 28, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 535ms, time_since_start: 01h 08m 49s 936ms, eta: 24m 31s 735ms
[32m2022-10-18T12:52:53 | mmf.trainers.callbacks.logistics: [39mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0256, train/hateful_memes/cross_entropy/avg: 0.2589, train/total_loss: 0.0256, train/total_loss/avg: 0.2589, max mem: 3429.0, experiment: hmd_a10, epoch: 28, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 5.00, time: 20s 341ms, time_since_start: 01h 09m 10s 277ms, eta: 25m 11s 465ms
[32m2022-10-18T12:53:14 | mmf.trainers.callbacks.logistics: [39mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.2579, train/total_loss: 0.0258, train/total_loss/avg: 0.2579, max mem: 3429.0, experiment: hmd_a10, epoch: 29, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 5.00, time: 20s 548ms, time_since_start: 01h 09m 30s 826ms, eta: 25m 05s 614ms
[32m2022-10-18T12:53:31 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:53:31 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:53:33 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:53:36 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:53:36 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.2563, train/total_loss: 0.0258, train/total_loss/avg: 0.2563, max mem: 3429.0, experiment: hmd_a10, epoch: 29, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 696ms, time_since_start: 01h 09m 53s 522ms, eta: 27m 19s 576ms
[32m2022-10-18T12:53:36 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:53:36 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:53:36 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:53:46 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:53:46 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:53:46 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.0658, val/total_loss: 2.0658, val/hateful_memes/accuracy: 0.6360, val/hateful_memes/binary_f1: 0.5495, val/hateful_memes/roc_auc: 0.7220, num_updates: 15000, epoch: 29, iterations: 15000, max_updates: 22000, val_time: 09s 961ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.725636
[32m2022-10-18T12:54:11 | mmf.trainers.callbacks.logistics: [39mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0256, train/hateful_memes/cross_entropy/avg: 0.2546, train/total_loss: 0.0256, train/total_loss/avg: 0.2546, max mem: 3429.0, experiment: hmd_a10, epoch: 29, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 4.17, time: 24s 564ms, time_since_start: 01h 10m 28s 053ms, eta: 29m 09s 193ms
[32m2022-10-18T12:54:33 | mmf.trainers.callbacks.logistics: [39mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0322, train/hateful_memes/cross_entropy/avg: 0.2556, train/total_loss: 0.0322, train/total_loss/avg: 0.2556, max mem: 3429.0, experiment: hmd_a10, epoch: 29, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 572ms, time_since_start: 01h 10m 50s 626ms, eta: 26m 24s 044ms
[32m2022-10-18T12:54:56 | mmf.trainers.callbacks.logistics: [39mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0322, train/hateful_memes/cross_entropy/avg: 0.2541, train/total_loss: 0.0322, train/total_loss/avg: 0.2541, max mem: 3429.0, experiment: hmd_a10, epoch: 29, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 4.55, time: 22s 599ms, time_since_start: 01h 11m 13s 225ms, eta: 26m 02s 611ms
[32m2022-10-18T12:55:19 | mmf.trainers.callbacks.logistics: [39mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0337, train/hateful_memes/cross_entropy/avg: 0.2535, train/total_loss: 0.0337, train/total_loss/avg: 0.2535, max mem: 3429.0, experiment: hmd_a10, epoch: 29, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 4.35, time: 23s 187ms, time_since_start: 01h 11m 36s 413ms, eta: 26m 19s 370ms
[32m2022-10-18T12:55:38 | mmf.trainers.callbacks.logistics: [39mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0322, train/hateful_memes/cross_entropy/avg: 0.2520, train/total_loss: 0.0322, train/total_loss/avg: 0.2520, max mem: 3429.0, experiment: hmd_a10, epoch: 30, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 5.26, time: 19s 190ms, time_since_start: 01h 11m 55s 604ms, eta: 21m 27s 312ms
[32m2022-10-18T12:55:56 | mmf.trainers.callbacks.logistics: [39mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0337, train/hateful_memes/cross_entropy/avg: 0.2507, train/total_loss: 0.0337, train/total_loss/avg: 0.2507, max mem: 3429.0, experiment: hmd_a10, epoch: 30, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 5.88, time: 17s 985ms, time_since_start: 01h 12m 13s 589ms, eta: 19m 47s 882ms
[32m2022-10-18T12:56:14 | mmf.trainers.callbacks.logistics: [39mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0337, train/hateful_memes/cross_entropy/avg: 0.2499, train/total_loss: 0.0337, train/total_loss/avg: 0.2499, max mem: 3429.0, experiment: hmd_a10, epoch: 30, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 121ms, time_since_start: 01h 12m 31s 710ms, eta: 19m 38s 164ms
[32m2022-10-18T12:56:33 | mmf.trainers.callbacks.logistics: [39mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0337, train/hateful_memes/cross_entropy/avg: 0.2495, train/total_loss: 0.0337, train/total_loss/avg: 0.2495, max mem: 3429.0, experiment: hmd_a10, epoch: 30, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 245ms, time_since_start: 01h 12m 49s 955ms, eta: 19m 27s 403ms
[32m2022-10-18T12:56:51 | mmf.trainers.callbacks.logistics: [39mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0390, train/hateful_memes/cross_entropy/avg: 0.2502, train/total_loss: 0.0390, train/total_loss/avg: 0.2502, max mem: 3429.0, experiment: hmd_a10, epoch: 30, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 5.56, time: 18s 196ms, time_since_start: 01h 13m 08s 152ms, eta: 19m 05s 485ms
[32m2022-10-18T12:57:09 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T12:57:09 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:57:10 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:57:16 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:57:16 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0390, train/hateful_memes/cross_entropy/avg: 0.2490, train/total_loss: 0.0390, train/total_loss/avg: 0.2490, max mem: 3429.0, experiment: hmd_a10, epoch: 31, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 4.00, time: 25s 061ms, time_since_start: 01h 13m 33s 213ms, eta: 25m 51s 832ms
[32m2022-10-18T12:57:16 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T12:57:16 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T12:57:16 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T12:57:21 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T12:57:21 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T12:57:21 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T12:57:24 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T12:57:27 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T12:57:30 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T12:57:30 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, val/hateful_memes/cross_entropy: 1.9096, val/total_loss: 1.9096, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5728, val/hateful_memes/roc_auc: 0.7267, num_updates: 16000, epoch: 31, iterations: 16000, max_updates: 22000, val_time: 14s 039ms, best_update: 16000, best_iteration: 16000, best_val/hateful_memes/roc_auc: 0.726681
[32m2022-10-18T12:57:54 | mmf.trainers.callbacks.logistics: [39mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0390, train/hateful_memes/cross_entropy/avg: 0.2478, train/total_loss: 0.0390, train/total_loss/avg: 0.2478, max mem: 3429.0, experiment: hmd_a10, epoch: 31, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 4.35, time: 23s 878ms, time_since_start: 01h 14m 11s 134ms, eta: 24m 13s 924ms
[32m2022-10-18T12:58:18 | mmf.trainers.callbacks.logistics: [39mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0501, train/hateful_memes/cross_entropy/avg: 0.2466, train/total_loss: 0.0501, train/total_loss/avg: 0.2466, max mem: 3429.0, experiment: hmd_a10, epoch: 31, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 4.17, time: 24s 267ms, time_since_start: 01h 14m 35s 402ms, eta: 24m 12s 557ms
[32m2022-10-18T12:58:44 | mmf.trainers.callbacks.logistics: [39mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0501, train/hateful_memes/cross_entropy/avg: 0.2453, train/total_loss: 0.0501, train/total_loss/avg: 0.2453, max mem: 3429.0, experiment: hmd_a10, epoch: 31, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 4.00, time: 25s 959ms, time_since_start: 01h 15m 01s 361ms, eta: 25m 27s 056ms
[32m2022-10-18T12:59:13 | mmf.trainers.callbacks.logistics: [39mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0501, train/hateful_memes/cross_entropy/avg: 0.2439, train/total_loss: 0.0501, train/total_loss/avg: 0.2439, max mem: 3429.0, experiment: hmd_a10, epoch: 31, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 3.45, time: 29s 042ms, time_since_start: 01h 15m 30s 404ms, eta: 27m 58s 420ms
[32m2022-10-18T12:59:44 | mmf.trainers.callbacks.logistics: [39mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0538, train/hateful_memes/cross_entropy/avg: 0.2433, train/total_loss: 0.0538, train/total_loss/avg: 0.2433, max mem: 3429.0, experiment: hmd_a10, epoch: 32, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 3.33, time: 30s 521ms, time_since_start: 01h 16m 925ms, eta: 28m 52s 375ms
[32m2022-10-18T13:00:03 | mmf.trainers.callbacks.logistics: [39mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0538, train/hateful_memes/cross_entropy/avg: 0.2418, train/total_loss: 0.0538, train/total_loss/avg: 0.2418, max mem: 3429.0, experiment: hmd_a10, epoch: 32, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 5.26, time: 19s 009ms, time_since_start: 01h 16m 19s 935ms, eta: 17m 39s 379ms
[32m2022-10-18T13:00:22 | mmf.trainers.callbacks.logistics: [39mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0538, train/hateful_memes/cross_entropy/avg: 0.2405, train/total_loss: 0.0538, train/total_loss/avg: 0.2405, max mem: 3429.0, experiment: hmd_a10, epoch: 32, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 5.26, time: 19s 250ms, time_since_start: 01h 16m 39s 185ms, eta: 17m 32s 904ms
[32m2022-10-18T13:00:41 | mmf.trainers.callbacks.logistics: [39mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0501, train/hateful_memes/cross_entropy/avg: 0.2393, train/total_loss: 0.0501, train/total_loss/avg: 0.2393, max mem: 3429.0, experiment: hmd_a10, epoch: 32, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 5.26, time: 19s 197ms, time_since_start: 01h 16m 58s 382ms, eta: 17m 10s 204ms
[32m2022-10-18T13:01:00 | mmf.trainers.callbacks.logistics: [39mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0337, train/hateful_memes/cross_entropy/avg: 0.2379, train/total_loss: 0.0337, train/total_loss/avg: 0.2379, max mem: 3429.0, experiment: hmd_a10, epoch: 32, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 5.56, time: 18s 959ms, time_since_start: 01h 17m 17s 341ms, eta: 16m 37s 862ms
[32m2022-10-18T13:01:19 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T13:01:19 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:01:20 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:01:24 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:01:24 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0501, train/hateful_memes/cross_entropy/avg: 0.2382, train/total_loss: 0.0501, train/total_loss/avg: 0.2382, max mem: 3429.0, experiment: hmd_a10, epoch: 32, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 4.35, time: 23s 567ms, time_since_start: 01h 17m 40s 909ms, eta: 20m 16s 112ms
[32m2022-10-18T13:01:24 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T13:01:24 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T13:01:24 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T13:01:39 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T13:01:39 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T13:01:39 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:01:42 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:01:45 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:01:45 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, val/hateful_memes/cross_entropy: 2.1780, val/total_loss: 2.1780, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.5441, val/hateful_memes/roc_auc: 0.7259, num_updates: 17000, epoch: 32, iterations: 17000, max_updates: 22000, val_time: 21s 448ms, best_update: 16000, best_iteration: 16000, best_val/hateful_memes/roc_auc: 0.726681
[32m2022-10-18T13:02:09 | mmf.trainers.callbacks.logistics: [39mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0501, train/hateful_memes/cross_entropy/avg: 0.2368, train/total_loss: 0.0501, train/total_loss/avg: 0.2368, max mem: 3429.0, experiment: hmd_a10, epoch: 33, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 4.35, time: 23s 684ms, time_since_start: 01h 18m 26s 045ms, eta: 19m 57s 686ms
[32m2022-10-18T13:02:31 | mmf.trainers.callbacks.logistics: [39mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0337, train/hateful_memes/cross_entropy/avg: 0.2354, train/total_loss: 0.0337, train/total_loss/avg: 0.2354, max mem: 3429.0, experiment: hmd_a10, epoch: 33, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 4.76, time: 21s 880ms, time_since_start: 01h 18m 47s 925ms, eta: 18m 03s 869ms
[32m2022-10-18T13:02:53 | mmf.trainers.callbacks.logistics: [39mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0335, train/hateful_memes/cross_entropy/avg: 0.2341, train/total_loss: 0.0335, train/total_loss/avg: 0.2341, max mem: 3429.0, experiment: hmd_a10, epoch: 33, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 4.55, time: 22s 698ms, time_since_start: 01h 19m 10s 624ms, eta: 18m 20s 984ms
[32m2022-10-18T13:03:18 | mmf.trainers.callbacks.logistics: [39mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0335, train/hateful_memes/cross_entropy/avg: 0.2338, train/total_loss: 0.0335, train/total_loss/avg: 0.2338, max mem: 3429.0, experiment: hmd_a10, epoch: 33, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 4.17, time: 24s 641ms, time_since_start: 01h 19m 35s 266ms, eta: 19m 29s 792ms
[32m2022-10-18T13:03:48 | mmf.trainers.callbacks.logistics: [39mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0335, train/hateful_memes/cross_entropy/avg: 0.2326, train/total_loss: 0.0335, train/total_loss/avg: 0.2326, max mem: 3429.0, experiment: hmd_a10, epoch: 33, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 3.33, time: 30s 016ms, time_since_start: 01h 20m 05s 283ms, eta: 23m 13s 985ms
[32m2022-10-18T13:04:16 | mmf.trainers.callbacks.logistics: [39mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0268, train/hateful_memes/cross_entropy/avg: 0.2313, train/total_loss: 0.0268, train/total_loss/avg: 0.2313, max mem: 3429.0, experiment: hmd_a10, epoch: 34, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 3.57, time: 28s 177ms, time_since_start: 01h 20m 33s 460ms, eta: 21m 19s 483ms
[32m2022-10-18T13:04:34 | mmf.trainers.callbacks.logistics: [39mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0233, train/hateful_memes/cross_entropy/avg: 0.2301, train/total_loss: 0.0233, train/total_loss/avg: 0.2301, max mem: 3429.0, experiment: hmd_a10, epoch: 34, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 5.88, time: 17s 904ms, time_since_start: 01h 20m 51s 364ms, eta: 13m 14s 518ms
[32m2022-10-18T13:04:52 | mmf.trainers.callbacks.logistics: [39mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0233, train/hateful_memes/cross_entropy/avg: 0.2308, train/total_loss: 0.0233, train/total_loss/avg: 0.2308, max mem: 3429.0, experiment: hmd_a10, epoch: 34, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 5.56, time: 18s 001ms, time_since_start: 01h 21m 09s 366ms, eta: 13m 242ms
[32m2022-10-18T13:05:10 | mmf.trainers.callbacks.logistics: [39mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0233, train/hateful_memes/cross_entropy/avg: 0.2297, train/total_loss: 0.0233, train/total_loss/avg: 0.2297, max mem: 3429.0, experiment: hmd_a10, epoch: 34, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 5.56, time: 18s 092ms, time_since_start: 01h 21m 27s 458ms, eta: 12m 45s 532ms
[32m2022-10-18T13:05:28 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T13:05:28 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:05:29 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:05:32 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:05:32 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0212, train/hateful_memes/cross_entropy/avg: 0.2285, train/total_loss: 0.0212, train/total_loss/avg: 0.2285, max mem: 3429.0, experiment: hmd_a10, epoch: 34, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 4.76, time: 21s 558ms, time_since_start: 01h 21m 49s 017ms, eta: 14m 49s 944ms
[32m2022-10-18T13:05:32 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T13:05:32 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T13:05:32 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T13:05:37 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T13:05:37 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T13:05:37 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:05:40 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T13:05:43 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:05:46 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:05:46 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, val/hateful_memes/cross_entropy: 2.0124, val/total_loss: 2.0124, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5769, val/hateful_memes/roc_auc: 0.7278, num_updates: 18000, epoch: 34, iterations: 18000, max_updates: 22000, val_time: 13s 843ms, best_update: 18000, best_iteration: 18000, best_val/hateful_memes/roc_auc: 0.727781
[32m2022-10-18T13:06:21 | mmf.trainers.callbacks.logistics: [39mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0212, train/hateful_memes/cross_entropy/avg: 0.2280, train/total_loss: 0.0212, train/total_loss/avg: 0.2280, max mem: 3429.0, experiment: hmd_a10, epoch: 35, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 2.86, time: 35s 377ms, time_since_start: 01h 22m 38s 240ms, eta: 23m 43s 858ms
[32m2022-10-18T13:06:43 | mmf.trainers.callbacks.logistics: [39mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0206, train/hateful_memes/cross_entropy/avg: 0.2267, train/total_loss: 0.0206, train/total_loss/avg: 0.2267, max mem: 3429.0, experiment: hmd_a10, epoch: 35, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 4.76, time: 21s 669ms, time_since_start: 01h 22m 59s 909ms, eta: 14m 09s 777ms
[32m2022-10-18T13:07:06 | mmf.trainers.callbacks.logistics: [39mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0206, train/hateful_memes/cross_entropy/avg: 0.2259, train/total_loss: 0.0206, train/total_loss/avg: 0.2259, max mem: 3429.0, experiment: hmd_a10, epoch: 35, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 4.35, time: 23s 402ms, time_since_start: 01h 23m 23s 312ms, eta: 14m 53s 599ms
[32m2022-10-18T13:07:31 | mmf.trainers.callbacks.logistics: [39mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0189, train/hateful_memes/cross_entropy/avg: 0.2247, train/total_loss: 0.0189, train/total_loss/avg: 0.2247, max mem: 3429.0, experiment: hmd_a10, epoch: 35, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 4.17, time: 24s 491ms, time_since_start: 01h 23m 47s 803ms, eta: 15m 09s 913ms
[32m2022-10-18T13:07:56 | mmf.trainers.callbacks.logistics: [39mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0189, train/hateful_memes/cross_entropy/avg: 0.2240, train/total_loss: 0.0189, train/total_loss/avg: 0.2240, max mem: 3429.0, experiment: hmd_a10, epoch: 35, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 4.00, time: 25s 727ms, time_since_start: 01h 24m 13s 531ms, eta: 15m 29s 272ms
[32m2022-10-18T13:08:22 | mmf.trainers.callbacks.logistics: [39mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0189, train/hateful_memes/cross_entropy/avg: 0.2228, train/total_loss: 0.0189, train/total_loss/avg: 0.2228, max mem: 3429.0, experiment: hmd_a10, epoch: 35, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 4.00, time: 25s 890ms, time_since_start: 01h 24m 39s 421ms, eta: 15m 08s 437ms
[32m2022-10-18T13:08:41 | mmf.trainers.callbacks.logistics: [39mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.2216, train/total_loss: 0.0134, train/total_loss/avg: 0.2216, max mem: 3429.0, experiment: hmd_a10, epoch: 36, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 5.26, time: 19s 228ms, time_since_start: 01h 24m 58s 649ms, eta: 10m 54s 839ms
[32m2022-10-18T13:08:59 | mmf.trainers.callbacks.logistics: [39mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.2208, train/total_loss: 0.0134, train/total_loss/avg: 0.2208, max mem: 3429.0, experiment: hmd_a10, epoch: 36, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 5.88, time: 17s 852ms, time_since_start: 01h 25m 16s 502ms, eta: 09m 49s 560ms
[32m2022-10-18T13:09:17 | mmf.trainers.callbacks.logistics: [39mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0206, train/hateful_memes/cross_entropy/avg: 0.2208, train/total_loss: 0.0206, train/total_loss/avg: 0.2208, max mem: 3429.0, experiment: hmd_a10, epoch: 36, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 5.88, time: 17s 929ms, time_since_start: 01h 25m 34s 432ms, eta: 09m 33s 605ms
[32m2022-10-18T13:09:35 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T13:09:35 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:09:36 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:09:39 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:09:39 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.2196, train/total_loss: 0.0134, train/total_loss/avg: 0.2196, max mem: 3429.0, experiment: hmd_a10, epoch: 36, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 4.76, time: 21s 342ms, time_since_start: 01h 25m 55s 774ms, eta: 11m 758ms
[32m2022-10-18T13:09:39 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T13:09:39 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T13:09:39 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T13:09:49 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T13:09:49 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T13:09:49 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:09:52 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-18T13:09:55 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:09:58 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:09:58 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, val/hateful_memes/cross_entropy: 2.3691, val/total_loss: 2.3691, val/hateful_memes/accuracy: 0.6320, val/hateful_memes/binary_f1: 0.5354, val/hateful_memes/roc_auc: 0.7281, num_updates: 19000, epoch: 36, iterations: 19000, max_updates: 22000, val_time: 19s 037ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.728077
[32m2022-10-18T13:10:25 | mmf.trainers.callbacks.logistics: [39mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0206, train/hateful_memes/cross_entropy/avg: 0.2191, train/total_loss: 0.0206, train/total_loss/avg: 0.2191, max mem: 3429.0, experiment: hmd_a10, epoch: 36, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 3.70, time: 27s 769ms, time_since_start: 01h 26m 42s 582ms, eta: 13m 51s 080ms
[32m2022-10-18T13:10:51 | mmf.trainers.callbacks.logistics: [39mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0206, train/hateful_memes/cross_entropy/avg: 0.2180, train/total_loss: 0.0206, train/total_loss/avg: 0.2180, max mem: 3429.0, experiment: hmd_a10, epoch: 37, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 4.00, time: 25s 182ms, time_since_start: 01h 27m 07s 764ms, eta: 12m 07s 665ms
[32m2022-10-18T13:11:12 | mmf.trainers.callbacks.logistics: [39mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0206, train/hateful_memes/cross_entropy/avg: 0.2169, train/total_loss: 0.0206, train/total_loss/avg: 0.2169, max mem: 3429.0, experiment: hmd_a10, epoch: 37, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 4.76, time: 21s 631ms, time_since_start: 01h 27m 29s 396ms, eta: 10m 02s 744ms
[32m2022-10-18T13:11:35 | mmf.trainers.callbacks.logistics: [39mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.2158, train/total_loss: 0.0134, train/total_loss/avg: 0.2158, max mem: 3429.0, experiment: hmd_a10, epoch: 37, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 4.55, time: 22s 718ms, time_since_start: 01h 27m 52s 114ms, eta: 10m 09s 572ms
[32m2022-10-18T13:12:00 | mmf.trainers.callbacks.logistics: [39mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.2150, train/total_loss: 0.0134, train/total_loss/avg: 0.2150, max mem: 3429.0, experiment: hmd_a10, epoch: 37, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 4.17, time: 24s 819ms, time_since_start: 01h 28m 16s 933ms, eta: 10m 40s 337ms
[32m2022-10-18T13:12:29 | mmf.trainers.callbacks.logistics: [39mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.2140, train/total_loss: 0.0134, train/total_loss/avg: 0.2140, max mem: 3429.0, experiment: hmd_a10, epoch: 37, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 3.45, time: 29s 490ms, time_since_start: 01h 28m 46s 424ms, eta: 12m 10s 415ms
[32m2022-10-18T13:12:58 | mmf.trainers.callbacks.logistics: [39mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.2133, train/total_loss: 0.0134, train/total_loss/avg: 0.2133, max mem: 3429.0, experiment: hmd_a10, epoch: 38, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 3.57, time: 28s 729ms, time_since_start: 01h 29m 15s 153ms, eta: 11m 21s 912ms
[32m2022-10-18T13:13:16 | mmf.trainers.callbacks.logistics: [39mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2122, train/total_loss: 0.0120, train/total_loss/avg: 0.2122, max mem: 3429.0, experiment: hmd_a10, epoch: 38, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 5.88, time: 17s 853ms, time_since_start: 01h 29m 33s 006ms, eta: 06m 45s 349ms
[32m2022-10-18T13:13:34 | mmf.trainers.callbacks.logistics: [39mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2112, train/total_loss: 0.0120, train/total_loss/avg: 0.2112, max mem: 3429.0, experiment: hmd_a10, epoch: 38, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 5.88, time: 17s 860ms, time_since_start: 01h 29m 50s 867ms, eta: 06m 27s 074ms
[32m2022-10-18T13:13:52 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T13:13:52 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:13:52 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:13:55 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:13:55 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2106, train/total_loss: 0.0120, train/total_loss/avg: 0.2106, max mem: 3429.0, experiment: hmd_a10, epoch: 38, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 4.76, time: 21s 339ms, time_since_start: 01h 30m 12s 207ms, eta: 07m 20s 457ms
[32m2022-10-18T13:13:55 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T13:13:55 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T13:13:55 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T13:14:00 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T13:14:00 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T13:14:00 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:14:04 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:14:06 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:14:06 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, val/hateful_memes/cross_entropy: 2.2382, val/total_loss: 2.2382, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.5531, val/hateful_memes/roc_auc: 0.7268, num_updates: 20000, epoch: 38, iterations: 20000, max_updates: 22000, val_time: 11s 410ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.728077
[32m2022-10-18T13:14:30 | mmf.trainers.callbacks.logistics: [39mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2099, train/total_loss: 0.0120, train/total_loss/avg: 0.2099, max mem: 3429.0, experiment: hmd_a10, epoch: 38, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 4.35, time: 23s 312ms, time_since_start: 01h 30m 46s 936ms, eta: 07m 37s 109ms
[32m2022-10-18T13:14:57 | mmf.trainers.callbacks.logistics: [39mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0160, train/hateful_memes/cross_entropy/avg: 0.2089, train/total_loss: 0.0160, train/total_loss/avg: 0.2089, max mem: 3429.0, experiment: hmd_a10, epoch: 38, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 3.70, time: 27s 624ms, time_since_start: 01h 31m 14s 560ms, eta: 08m 33s 148ms
[32m2022-10-18T13:15:19 | mmf.trainers.callbacks.logistics: [39mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2079, train/total_loss: 0.0120, train/total_loss/avg: 0.2079, max mem: 3429.0, experiment: hmd_a10, epoch: 39, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 4.76, time: 21s 164ms, time_since_start: 01h 31m 35s 725ms, eta: 06m 11s 318ms
[32m2022-10-18T13:15:38 | mmf.trainers.callbacks.logistics: [39mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2069, train/total_loss: 0.0120, train/total_loss/avg: 0.2069, max mem: 3429.0, experiment: hmd_a10, epoch: 39, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 5.26, time: 19s 513ms, time_since_start: 01h 31m 55s 239ms, eta: 05m 22s 210ms
[32m2022-10-18T13:15:59 | mmf.trainers.callbacks.logistics: [39mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0087, train/hateful_memes/cross_entropy/avg: 0.2060, train/total_loss: 0.0087, train/total_loss/avg: 0.2060, max mem: 3429.0, experiment: hmd_a10, epoch: 39, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 5.00, time: 20s 701ms, time_since_start: 01h 32m 15s 940ms, eta: 05m 20s 454ms
[32m2022-10-18T13:16:21 | mmf.trainers.callbacks.logistics: [39mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2051, train/total_loss: 0.0120, train/total_loss/avg: 0.2051, max mem: 3429.0, experiment: hmd_a10, epoch: 39, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 4.55, time: 22s 228ms, time_since_start: 01h 32m 38s 168ms, eta: 05m 21s 150ms
[32m2022-10-18T13:16:45 | mmf.trainers.callbacks.logistics: [39mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.2042, train/total_loss: 0.0134, train/total_loss/avg: 0.2042, max mem: 3429.0, experiment: hmd_a10, epoch: 39, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 4.35, time: 23s 995ms, time_since_start: 01h 33m 02s 164ms, eta: 05m 21s 928ms
[32m2022-10-18T13:17:06 | mmf.trainers.callbacks.logistics: [39mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2033, train/total_loss: 0.0120, train/total_loss/avg: 0.2033, max mem: 3429.0, experiment: hmd_a10, epoch: 40, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 5.00, time: 20s 899ms, time_since_start: 01h 33m 23s 064ms, eta: 04m 18s 818ms
[32m2022-10-18T13:17:24 | mmf.trainers.callbacks.logistics: [39mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2028, train/total_loss: 0.0120, train/total_loss/avg: 0.2028, max mem: 3429.0, experiment: hmd_a10, epoch: 40, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 5.88, time: 17s 865ms, time_since_start: 01h 33m 40s 929ms, eta: 03m 22s 805ms
[32m2022-10-18T13:17:42 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T13:17:42 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:17:42 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:17:45 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:17:45 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.2021, train/total_loss: 0.0134, train/total_loss/avg: 0.2021, max mem: 3429.0, experiment: hmd_a10, epoch: 40, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 4.76, time: 21s 220ms, time_since_start: 01h 34m 02s 150ms, eta: 03m 39s 001ms
[32m2022-10-18T13:17:45 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T13:17:45 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T13:17:45 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T13:17:54 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T13:17:54 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T13:17:54 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:17:57 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:18:00 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:18:00 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, val/hateful_memes/cross_entropy: 2.2529, val/total_loss: 2.2529, val/hateful_memes/accuracy: 0.6400, val/hateful_memes/binary_f1: 0.5588, val/hateful_memes/roc_auc: 0.7277, num_updates: 21000, epoch: 40, iterations: 21000, max_updates: 22000, val_time: 14s 999ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.728077
[32m2022-10-18T13:18:22 | mmf.trainers.callbacks.logistics: [39mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0120, train/hateful_memes/cross_entropy/avg: 0.2012, train/total_loss: 0.0120, train/total_loss/avg: 0.2012, max mem: 3429.0, experiment: hmd_a10, epoch: 40, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 4.76, time: 21s 852ms, time_since_start: 01h 34m 39s 008ms, eta: 03m 22s 967ms
[32m2022-10-18T13:18:46 | mmf.trainers.callbacks.logistics: [39mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.2006, train/total_loss: 0.0134, train/total_loss/avg: 0.2006, max mem: 3429.0, experiment: hmd_a10, epoch: 40, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 4.35, time: 23s 770ms, time_since_start: 01h 35m 02s 778ms, eta: 03m 16s 247ms
[32m2022-10-18T13:19:09 | mmf.trainers.callbacks.logistics: [39mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.1997, train/total_loss: 0.0134, train/total_loss/avg: 0.1997, max mem: 3429.0, experiment: hmd_a10, epoch: 41, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 4.35, time: 23s 856ms, time_since_start: 01h 35m 26s 635ms, eta: 02m 52s 342ms
[32m2022-10-18T13:19:29 | mmf.trainers.callbacks.logistics: [39mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.1988, train/total_loss: 0.0134, train/total_loss/avg: 0.1988, max mem: 3429.0, experiment: hmd_a10, epoch: 41, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 5.26, time: 19s 533ms, time_since_start: 01h 35m 46s 168ms, eta: 02m 951ms
[32m2022-10-18T13:19:49 | mmf.trainers.callbacks.logistics: [39mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.1981, train/total_loss: 0.0134, train/total_loss/avg: 0.1981, max mem: 3429.0, experiment: hmd_a10, epoch: 41, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 5.26, time: 19s 956ms, time_since_start: 01h 36m 06s 125ms, eta: 01m 42s 976ms
[32m2022-10-18T13:20:10 | mmf.trainers.callbacks.logistics: [39mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.1972, train/total_loss: 0.0134, train/total_loss/avg: 0.1972, max mem: 3429.0, experiment: hmd_a10, epoch: 41, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 5.00, time: 20s 950ms, time_since_start: 01h 36m 27s 075ms, eta: 01m 26s 482ms
[32m2022-10-18T13:20:33 | mmf.trainers.callbacks.logistics: [39mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.1965, train/total_loss: 0.0134, train/total_loss/avg: 0.1965, max mem: 3429.0, experiment: hmd_a10, epoch: 41, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 4.55, time: 22s 882ms, time_since_start: 01h 36m 49s 958ms, eta: 01m 10s 843ms
[32m2022-10-18T13:20:59 | mmf.trainers.callbacks.logistics: [39mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0160, train/hateful_memes/cross_entropy/avg: 0.1962, train/total_loss: 0.0160, train/total_loss/avg: 0.1962, max mem: 3429.0, experiment: hmd_a10, epoch: 41, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 3.85, time: 26s 181ms, time_since_start: 01h 37m 16s 139ms, eta: 54s 037ms
[32m2022-10-18T13:21:17 | mmf.trainers.callbacks.logistics: [39mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0134, train/hateful_memes/cross_entropy/avg: 0.1953, train/total_loss: 0.0134, train/total_loss/avg: 0.1953, max mem: 3429.0, experiment: hmd_a10, epoch: 42, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 5.56, time: 18s 476ms, time_since_start: 01h 37m 34s 615ms, eta: 19s 067ms
[32m2022-10-18T13:21:35 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-18T13:21:35 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:21:36 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:21:39 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:21:39 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0119, train/hateful_memes/cross_entropy/avg: 0.1944, train/total_loss: 0.0119, train/total_loss/avg: 0.1944, max mem: 3439.0, experiment: hmd_a10, epoch: 42, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 4.76, time: 21s 441ms, time_since_start: 01h 37m 56s 057ms, eta: 0ms
[32m2022-10-18T13:21:39 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-18T13:21:39 | mmf.common.test_reporter: [39mPredicting for hateful_memes
[31m[5mWARNING[39m[25m [32m2022-10-18T13:21:39 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-18T13:21:44 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T13:21:44 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T13:21:44 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-18T13:21:47 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-18T13:21:50 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-18T13:21:50 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, val/hateful_memes/cross_entropy: 2.2938, val/total_loss: 2.2938, val/hateful_memes/accuracy: 0.6400, val/hateful_memes/binary_f1: 0.5588, val/hateful_memes/roc_auc: 0.7275, num_updates: 22000, epoch: 42, iterations: 22000, max_updates: 22000, val_time: 11s 256ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.728077
[32m2022-10-18T13:21:50 | mmf.trainers.core.training_loop: [39mStepping into final validation check
[32m2022-10-18T13:21:50 | mmf.utils.checkpoint: [39mRestoring checkpoint
[32m2022-10-18T13:21:50 | mmf.utils.checkpoint: [39mLoading checkpoint
  9% 3/32 [00:00<00:03,  8.08it/s]
[32m2022-10-18T13:21:53 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-18T13:21:53 | mmf.utils.checkpoint: [39mCurrent num updates: 19000
[32m2022-10-18T13:21:53 | mmf.utils.checkpoint: [39mCurrent iteration: 19000
[32m2022-10-18T13:21:53 | mmf.utils.checkpoint: [39mCurrent epoch: 36
[32m2022-10-18T13:21:54 | mmf.utils.checkpoint: [39mSalvando o modelo final..
[32m2022-10-18T13:21:54 | mmf.trainers.mmf_trainer: [39mStarting inference on val set
[32m2022-10-18T13:21:54 | mmf.common.test_reporter: [39mPredicting for hateful_memes


 97% 31/32 [00:04<00:00,  6.90it/s]
[32m2022-10-18T13:21:58 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 32
[32m2022-10-18T13:21:58 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-18T13:21:58 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, val/hateful_memes/cross_entropy: 2.3691, val/total_loss: 2.3691, val/hateful_memes/accuracy: 0.6320, val/hateful_memes/binary_f1: 0.5354, val/hateful_memes/roc_auc: 0.7281

100% 32/32 [00:04<00:00,  7.01it/s]

[32m2022-10-17T16:41:57 | mmf.utils.checkpoint: [39mLoading checkpoint
[31m[5mWARNING[39m[25m [32m2022-10-17T16:42:01 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-17T16:42:01 | mmf: [39mKey distributed is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-17T16:42:01 | mmf: [39mKey data_parallel is not present in registry, returning default value of None
[31m[5mWARNING[39m[25m [32m2022-10-17T16:42:01 | mmf: [39mKey distributed is not present in registry, returning default value of None
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mPretrained model loaded
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCurrent num updates: 0
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCurrent iteration: 0
[32m2022-10-17T16:42:01 | mmf.utils.checkpoint: [39mCurrent epoch: 0
[32m2022-10-17T16:42:01 | mmf.trainers.mmf_trainer: [39m===== Model x=====
[32m2022-10-17T16:42:01 | mmf.trainers.mmf_trainer: [39mVisualBERT(
  (model): VisualBERTForClassification(
    (bert): VisualBERTBase(
      (embeddings): BertVisioLinguisticEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (token_type_embeddings_visual): Embedding(2, 768)
        (position_embeddings_visual): Embedding(512, 768)
        (projection): Linear(in_features=2048, out_features=768, bias=True)
      )
      (encoder): BertEncoderJit(
        (layer): ModuleList(
          (0): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayerJit(
            (attention): BertAttentionJit(
              (self): BertSelfAttentionJit(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Sequential(
      (0): BertPredictionHeadTransform(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (1): Linear(in_features=768, out_features=2, bias=True)
    )
  )
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-10-17T16:42:01 | mmf.utils.general: [39mTotal Parameters: 112044290. Trained Parameters: 112044290
[32m2022-10-17T16:42:01 | mmf.trainers.core.training_loop: [39mStarting training...
[32m2022-10-17T16:43:16 | mmf.trainers.callbacks.logistics: [39mprogress: 100/22000, train/mmh/cross_entropy: 0.7935, train/mmh/cross_entropy/avg: 0.7935, train/total_loss: 0.7935, train/total_loss/avg: 0.7935, max mem: 5043.0, experiment: mmh-ft, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.35, time: 01m 14s 859ms, time_since_start: 01m 26s 493ms, eta: 05h 58m 45s 706ms
[32m2022-10-17T16:44:30 | mmf.trainers.callbacks.logistics: [39mprogress: 200/22000, train/mmh/cross_entropy: 0.7935, train/mmh/cross_entropy/avg: 0.8084, train/total_loss: 0.7935, train/total_loss/avg: 0.8084, max mem: 5043.0, experiment: mmh-ft, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.35, time: 01m 14s 544ms, time_since_start: 02m 41s 038ms, eta: 05h 55m 37s 193ms
[32m2022-10-17T16:45:52 | mmf.trainers.callbacks.logistics: [39mprogress: 300/22000, train/mmh/cross_entropy: 0.7935, train/mmh/cross_entropy/avg: 0.7460, train/total_loss: 0.7935, train/total_loss/avg: 0.7460, max mem: 5043.0, experiment: mmh-ft, epoch: 1, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.23, time: 01m 21s 877ms, time_since_start: 04m 02s 916ms, eta: 06h 28m 48s 739ms
[32m2022-10-17T16:47:12 | mmf.trainers.callbacks.logistics: [39mprogress: 400/22000, train/mmh/cross_entropy: 0.6210, train/mmh/cross_entropy/avg: 0.6993, train/total_loss: 0.6210, train/total_loss/avg: 0.6993, max mem: 5043.0, experiment: mmh-ft, epoch: 1, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 307ms, time_since_start: 05m 22s 223ms, eta: 06h 14m 52s 230ms
[32m2022-10-17T16:48:28 | mmf.trainers.callbacks.logistics: [39mprogress: 500/22000, train/mmh/cross_entropy: 0.7842, train/mmh/cross_entropy/avg: 0.7163, train/total_loss: 0.7842, train/total_loss/avg: 0.7163, max mem: 5043.0, experiment: mmh-ft, epoch: 1, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 959ms, time_since_start: 06m 38s 183ms, eta: 05h 57m 22s 937ms
[32m2022-10-17T16:49:42 | mmf.trainers.callbacks.logistics: [39mprogress: 600/22000, train/mmh/cross_entropy: 0.6354, train/mmh/cross_entropy/avg: 0.7028, train/total_loss: 0.6354, train/total_loss/avg: 0.7028, max mem: 5043.0, experiment: mmh-ft, epoch: 1, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 1.35, time: 01m 14s 957ms, time_since_start: 07m 53s 140ms, eta: 05h 51m 01s 587ms
[32m2022-10-17T16:50:57 | mmf.trainers.callbacks.logistics: [39mprogress: 700/22000, train/mmh/cross_entropy: 0.6599, train/mmh/cross_entropy/avg: 0.6967, train/total_loss: 0.6599, train/total_loss/avg: 0.6967, max mem: 5043.0, experiment: mmh-ft, epoch: 1, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 1.35, time: 01m 14s 429ms, time_since_start: 09m 07s 569ms, eta: 05h 46m 55s 517ms
[32m2022-10-17T16:52:11 | mmf.trainers.callbacks.logistics: [39mprogress: 800/22000, train/mmh/cross_entropy: 0.6354, train/mmh/cross_entropy/avg: 0.6808, train/total_loss: 0.6354, train/total_loss/avg: 0.6808, max mem: 5043.0, experiment: mmh-ft, epoch: 1, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 1.35, time: 01m 14s 038ms, time_since_start: 10m 21s 607ms, eta: 05h 43m 28s 942ms
[32m2022-10-17T16:53:25 | mmf.trainers.callbacks.logistics: [39mprogress: 900/22000, train/mmh/cross_entropy: 0.6599, train/mmh/cross_entropy/avg: 0.6880, train/total_loss: 0.6599, train/total_loss/avg: 0.6880, max mem: 5043.0, experiment: mmh-ft, epoch: 1, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 1.37, time: 01m 13s 727ms, time_since_start: 11m 35s 335ms, eta: 05h 40m 25s 775ms
[32m2022-10-17T16:54:39 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T16:54:39 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T16:54:41 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T16:54:46 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T16:54:46 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, train/mmh/cross_entropy: 0.6354, train/mmh/cross_entropy/avg: 0.6695, train/total_loss: 0.6354, train/total_loss/avg: 0.6695, max mem: 5043.0, experiment: mmh-ft, epoch: 1, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 547ms, time_since_start: 12m 56s 883ms, eta: 06h 14m 45s 173ms
[32m2022-10-17T16:54:46 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T16:54:46 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T16:54:46 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T16:57:54 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T16:57:54 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T16:57:54 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T16:58:05 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T16:58:06 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T16:58:21 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T16:58:21 | mmf.trainers.callbacks.logistics: [39mprogress: 1000/22000, val/mmh/cross_entropy: 0.5467, val/total_loss: 0.5467, val/mmh/accuracy: 0.7438, val/mmh/binary_f1: 0.8531, val/mmh/roc_auc: 0.7087, num_updates: 1000, epoch: 1, iterations: 1000, max_updates: 22000, val_time: 03m 34s 394ms, best_update: 1000, best_iteration: 1000, best_val/mmh/roc_auc: 0.708722
[32m2022-10-17T16:59:41 | mmf.trainers.callbacks.logistics: [39mprogress: 1100/22000, train/mmh/cross_entropy: 0.6354, train/mmh/cross_entropy/avg: 0.6622, train/total_loss: 0.6354, train/total_loss/avg: 0.6622, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 610ms, time_since_start: 17m 51s 893ms, eta: 06h 08m 41s 019ms
[32m2022-10-17T17:00:56 | mmf.trainers.callbacks.logistics: [39mprogress: 1200/22000, train/mmh/cross_entropy: 0.6354, train/mmh/cross_entropy/avg: 0.6650, train/total_loss: 0.6354, train/total_loss/avg: 0.6650, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 1.35, time: 01m 14s 366ms, time_since_start: 19m 06s 260ms, eta: 05h 38m 29s 873ms
[32m2022-10-17T17:02:10 | mmf.trainers.callbacks.logistics: [39mprogress: 1300/22000, train/mmh/cross_entropy: 0.6354, train/mmh/cross_entropy/avg: 0.6598, train/total_loss: 0.6354, train/total_loss/avg: 0.6598, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 1.37, time: 01m 13s 956ms, time_since_start: 20m 20s 216ms, eta: 05h 35m 639ms
[32m2022-10-17T17:03:23 | mmf.trainers.callbacks.logistics: [39mprogress: 1400/22000, train/mmh/cross_entropy: 0.6354, train/mmh/cross_entropy/avg: 0.6640, train/total_loss: 0.6354, train/total_loss/avg: 0.6640, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 1.37, time: 01m 13s 773ms, time_since_start: 21m 33s 990ms, eta: 05h 32m 34s 089ms
[32m2022-10-17T17:04:37 | mmf.trainers.callbacks.logistics: [39mprogress: 1500/22000, train/mmh/cross_entropy: 0.6354, train/mmh/cross_entropy/avg: 0.6531, train/total_loss: 0.6354, train/total_loss/avg: 0.6531, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 139ms, time_since_start: 22m 48s 129ms, eta: 05h 32m 35s 711ms
[32m2022-10-17T17:05:52 | mmf.trainers.callbacks.logistics: [39mprogress: 1600/22000, train/mmh/cross_entropy: 0.6210, train/mmh/cross_entropy/avg: 0.6458, train/total_loss: 0.6210, train/total_loss/avg: 0.6458, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 372ms, time_since_start: 24m 02s 502ms, eta: 05h 32m 867ms
[32m2022-10-17T17:07:06 | mmf.trainers.callbacks.logistics: [39mprogress: 1700/22000, train/mmh/cross_entropy: 0.6303, train/mmh/cross_entropy/avg: 0.6449, train/total_loss: 0.6303, train/total_loss/avg: 0.6449, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 1.37, time: 01m 13s 855ms, time_since_start: 25m 16s 357ms, eta: 05h 28m 05s 401ms
[32m2022-10-17T17:08:19 | mmf.trainers.callbacks.logistics: [39mprogress: 1800/22000, train/mmh/cross_entropy: 0.6303, train/mmh/cross_entropy/avg: 0.6509, train/total_loss: 0.6303, train/total_loss/avg: 0.6509, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 1.37, time: 01m 13s 493ms, time_since_start: 26m 29s 851ms, eta: 05h 24m 52s 513ms
[32m2022-10-17T17:09:33 | mmf.trainers.callbacks.logistics: [39mprogress: 1900/22000, train/mmh/cross_entropy: 0.6303, train/mmh/cross_entropy/avg: 0.6492, train/total_loss: 0.6303, train/total_loss/avg: 0.6492, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 1.37, time: 01m 13s 950ms, time_since_start: 27m 43s 802ms, eta: 05h 25m 16s 519ms
[32m2022-10-17T17:10:47 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T17:10:47 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T17:10:49 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T17:11:04 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T17:11:04 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, train/mmh/cross_entropy: 0.6210, train/mmh/cross_entropy/avg: 0.6447, train/total_loss: 0.6210, train/total_loss/avg: 0.6447, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 652ms, time_since_start: 29m 14s 454ms, eta: 06h 36m 45s 328ms
[32m2022-10-17T17:11:04 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T17:11:04 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T17:11:04 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T17:14:07 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T17:14:07 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T17:14:07 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T17:14:18 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T17:14:29 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T17:14:40 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T17:14:40 | mmf.trainers.callbacks.logistics: [39mprogress: 2000/22000, val/mmh/cross_entropy: 0.5577, val/total_loss: 0.5577, val/mmh/accuracy: 0.6470, val/mmh/binary_f1: 0.7199, val/mmh/roc_auc: 0.7122, num_updates: 2000, epoch: 1, iterations: 2000, max_updates: 22000, val_time: 03m 35s 773ms, best_update: 2000, best_iteration: 2000, best_val/mmh/roc_auc: 0.712194
[32m2022-10-17T17:15:54 | mmf.trainers.callbacks.logistics: [39mprogress: 2100/22000, train/mmh/cross_entropy: 0.6210, train/mmh/cross_entropy/avg: 0.6446, train/total_loss: 0.6210, train/total_loss/avg: 0.6446, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 701ms, time_since_start: 34m 04s 931ms, eta: 05h 25m 18s 436ms
[32m2022-10-17T17:17:10 | mmf.trainers.callbacks.logistics: [39mprogress: 2200/22000, train/mmh/cross_entropy: 0.6190, train/mmh/cross_entropy/avg: 0.6417, train/total_loss: 0.6190, train/total_loss/avg: 0.6417, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 1.33, time: 01m 15s 654ms, time_since_start: 35m 20s 585ms, eta: 05h 27m 48s 106ms
[32m2022-10-17T17:18:24 | mmf.trainers.callbacks.logistics: [39mprogress: 2300/22000, train/mmh/cross_entropy: 0.6190, train/mmh/cross_entropy/avg: 0.6426, train/total_loss: 0.6190, train/total_loss/avg: 0.6426, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 134ms, time_since_start: 36m 34s 720ms, eta: 05h 19m 35s 668ms
[32m2022-10-17T17:19:38 | mmf.trainers.callbacks.logistics: [39mprogress: 2400/22000, train/mmh/cross_entropy: 0.6190, train/mmh/cross_entropy/avg: 0.6411, train/total_loss: 0.6190, train/total_loss/avg: 0.6411, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 1.37, time: 01m 13s 845ms, time_since_start: 37m 48s 566ms, eta: 05h 16m 44s 109ms
[32m2022-10-17T17:20:52 | mmf.trainers.callbacks.logistics: [39mprogress: 2500/22000, train/mmh/cross_entropy: 0.6190, train/mmh/cross_entropy/avg: 0.6464, train/total_loss: 0.6190, train/total_loss/avg: 0.6464, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 1.37, time: 01m 13s 948ms, time_since_start: 39m 02s 514ms, eta: 05h 15m 33s 313ms
[32m2022-10-17T17:22:06 | mmf.trainers.callbacks.logistics: [39mprogress: 2600/22000, train/mmh/cross_entropy: 0.6062, train/mmh/cross_entropy/avg: 0.6413, train/total_loss: 0.6062, train/total_loss/avg: 0.6413, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 068ms, time_since_start: 40m 16s 582ms, eta: 05h 14m 26s 752ms
[32m2022-10-17T17:23:20 | mmf.trainers.callbacks.logistics: [39mprogress: 2700/22000, train/mmh/cross_entropy: 0.5976, train/mmh/cross_entropy/avg: 0.6372, train/total_loss: 0.5976, train/total_loss/avg: 0.6372, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 1.37, time: 01m 13s 864ms, time_since_start: 41m 30s 446ms, eta: 05h 11m 57s 842ms
[32m2022-10-17T17:24:34 | mmf.trainers.callbacks.logistics: [39mprogress: 2800/22000, train/mmh/cross_entropy: 0.6062, train/mmh/cross_entropy/avg: 0.6375, train/total_loss: 0.6062, train/total_loss/avg: 0.6375, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 1.37, time: 01m 13s 928ms, time_since_start: 42m 44s 375ms, eta: 05h 10m 37s 106ms
[32m2022-10-17T17:25:48 | mmf.trainers.callbacks.logistics: [39mprogress: 2900/22000, train/mmh/cross_entropy: 0.5976, train/mmh/cross_entropy/avg: 0.6349, train/total_loss: 0.5976, train/total_loss/avg: 0.6349, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 225ms, time_since_start: 43m 58s 600ms, eta: 05h 10m 14s 475ms
[32m2022-10-17T17:27:02 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T17:27:02 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T17:27:03 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T17:27:19 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T17:27:19 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, train/mmh/cross_entropy: 0.6062, train/mmh/cross_entropy/avg: 0.6367, train/total_loss: 0.6062, train/total_loss/avg: 0.6367, max mem: 5052.0, experiment: mmh-ft, epoch: 1, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 602ms, time_since_start: 45m 29s 202ms, eta: 06h 16m 42s 512ms
[32m2022-10-17T17:27:19 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
                                                Predicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T17:27:19 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T17:30:23 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T17:30:23 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T17:30:23 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T17:30:34 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T17:30:45 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T17:30:45 | mmf.trainers.callbacks.logistics: [39mprogress: 3000/22000, val/mmh/cross_entropy: 0.5401, val/total_loss: 0.5401, val/mmh/accuracy: 0.7438, val/mmh/binary_f1: 0.8531, val/mmh/roc_auc: 0.7043, num_updates: 3000, epoch: 1, iterations: 3000, max_updates: 22000, val_time: 03m 26s 369ms, best_update: 2000, best_iteration: 2000, best_val/mmh/roc_auc: 0.712194
[32m2022-10-17T17:32:00 | mmf.trainers.callbacks.logistics: [39mprogress: 3100/22000, train/mmh/cross_entropy: 0.6190, train/mmh/cross_entropy/avg: 0.6380, train/total_loss: 0.6190, train/total_loss/avg: 0.6380, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 922ms, time_since_start: 50m 10s 496ms, eta: 05h 09m 52s 549ms
[32m2022-10-17T17:33:14 | mmf.trainers.callbacks.logistics: [39mprogress: 3200/22000, train/mmh/cross_entropy: 0.6062, train/mmh/cross_entropy/avg: 0.6333, train/total_loss: 0.6062, train/total_loss/avg: 0.6333, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 527ms, time_since_start: 51m 25s 023ms, eta: 05h 06m 36s 598ms
[32m2022-10-17T17:34:29 | mmf.trainers.callbacks.logistics: [39mprogress: 3300/22000, train/mmh/cross_entropy: 0.6190, train/mmh/cross_entropy/avg: 0.6340, train/total_loss: 0.6190, train/total_loss/avg: 0.6340, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 453ms, time_since_start: 52m 39s 477ms, eta: 05h 04m 40s 673ms
[32m2022-10-17T17:35:43 | mmf.trainers.callbacks.logistics: [39mprogress: 3400/22000, train/mmh/cross_entropy: 0.6190, train/mmh/cross_entropy/avg: 0.6372, train/total_loss: 0.6190, train/total_loss/avg: 0.6372, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 661ms, time_since_start: 53m 54s 139ms, eta: 05h 03m 53s 760ms
[32m2022-10-17T17:36:57 | mmf.trainers.callbacks.logistics: [39mprogress: 3500/22000, train/mmh/cross_entropy: 0.6190, train/mmh/cross_entropy/avg: 0.6338, train/total_loss: 0.6190, train/total_loss/avg: 0.6338, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 1.37, time: 01m 13s 920ms, time_since_start: 55m 08s 059ms, eta: 04h 59m 15s 613ms
[32m2022-10-17T17:38:12 | mmf.trainers.callbacks.logistics: [39mprogress: 3600/22000, train/mmh/cross_entropy: 0.6303, train/mmh/cross_entropy/avg: 0.6375, train/total_loss: 0.6303, train/total_loss/avg: 0.6375, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 249ms, time_since_start: 56m 22s 308ms, eta: 04h 58m 58s 037ms
[32m2022-10-17T17:39:26 | mmf.trainers.callbacks.logistics: [39mprogress: 3700/22000, train/mmh/cross_entropy: 0.6190, train/mmh/cross_entropy/avg: 0.6370, train/total_loss: 0.6190, train/total_loss/avg: 0.6370, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 754ms, time_since_start: 57m 37s 063ms, eta: 04h 59m 22s 023ms
[32m2022-10-17T17:40:41 | mmf.trainers.callbacks.logistics: [39mprogress: 3800/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6361, train/total_loss: 0.6175, train/total_loss/avg: 0.6361, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 306ms, time_since_start: 58m 51s 370ms, eta: 04h 55m 56s 769ms
[32m2022-10-17T17:41:55 | mmf.trainers.callbacks.logistics: [39mprogress: 3900/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6365, train/total_loss: 0.6175, train/total_loss/avg: 0.6365, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 1.35, time: 01m 14s 536ms, time_since_start: 01h 05s 907ms, eta: 04h 55m 13s 857ms
[32m2022-10-17T17:43:10 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T17:43:10 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T17:43:11 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T17:43:26 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T17:43:26 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, train/mmh/cross_entropy: 0.6430, train/mmh/cross_entropy/avg: 0.6392, train/total_loss: 0.6430, train/total_loss/avg: 0.6392, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 151ms, time_since_start: 01h 01m 37s 058ms, eta: 05h 59m 02s 813ms
[32m2022-10-17T17:43:26 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T17:43:26 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T17:43:26 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T17:46:37 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T17:46:37 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T17:46:38 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T17:46:49 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T17:47:01 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T17:47:12 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T17:47:12 | mmf.trainers.callbacks.logistics: [39mprogress: 4000/22000, val/mmh/cross_entropy: 0.5604, val/total_loss: 0.5604, val/mmh/accuracy: 0.6508, val/mmh/binary_f1: 0.7236, val/mmh/roc_auc: 0.7202, num_updates: 4000, epoch: 1, iterations: 4000, max_updates: 22000, val_time: 03m 45s 276ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.720194
[32m2022-10-17T17:48:27 | mmf.trainers.callbacks.logistics: [39mprogress: 4100/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6377, train/total_loss: 0.6175, train/total_loss/avg: 0.6377, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 189ms, time_since_start: 01h 06m 37s 526ms, eta: 04h 54m 31s 456ms
[32m2022-10-17T17:49:42 | mmf.trainers.callbacks.logistics: [39mprogress: 4200/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6368, train/total_loss: 0.6175, train/total_loss/avg: 0.6368, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 877ms, time_since_start: 01h 07m 52s 403ms, eta: 04h 51m 39s 934ms
[32m2022-10-17T17:50:57 | mmf.trainers.callbacks.logistics: [39mprogress: 4300/22000, train/mmh/cross_entropy: 0.6062, train/mmh/cross_entropy/avg: 0.6356, train/total_loss: 0.6062, train/total_loss/avg: 0.6356, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 881ms, time_since_start: 01h 09m 07s 285ms, eta: 04h 50m 02s 636ms
[32m2022-10-17T17:52:12 | mmf.trainers.callbacks.logistics: [39mprogress: 4400/22000, train/mmh/cross_entropy: 0.6013, train/mmh/cross_entropy/avg: 0.6326, train/total_loss: 0.6013, train/total_loss/avg: 0.6326, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 943ms, time_since_start: 01h 10m 22s 229ms, eta: 04h 48m 38s 551ms
[32m2022-10-17T17:53:27 | mmf.trainers.callbacks.logistics: [39mprogress: 4500/22000, train/mmh/cross_entropy: 0.6013, train/mmh/cross_entropy/avg: 0.6338, train/total_loss: 0.6013, train/total_loss/avg: 0.6338, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 989ms, time_since_start: 01h 11m 37s 218ms, eta: 04h 47m 10s 716ms
[32m2022-10-17T17:54:42 | mmf.trainers.callbacks.logistics: [39mprogress: 4600/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6385, train/total_loss: 0.6175, train/total_loss/avg: 0.6385, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 291ms, time_since_start: 01h 12m 52s 510ms, eta: 04h 46m 41s 358ms
[32m2022-10-17T17:55:57 | mmf.trainers.callbacks.logistics: [39mprogress: 4700/22000, train/mmh/cross_entropy: 0.6462, train/mmh/cross_entropy/avg: 0.6394, train/total_loss: 0.6462, train/total_loss/avg: 0.6394, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 266ms, time_since_start: 01h 14m 07s 776ms, eta: 04h 44m 56s 623ms
[32m2022-10-17T17:57:12 | mmf.trainers.callbacks.logistics: [39mprogress: 4800/22000, train/mmh/cross_entropy: 0.6521, train/mmh/cross_entropy/avg: 0.6402, train/total_loss: 0.6521, train/total_loss/avg: 0.6402, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 867ms, time_since_start: 01h 15m 22s 644ms, eta: 04h 41m 47s 890ms
[32m2022-10-17T17:58:27 | mmf.trainers.callbacks.logistics: [39mprogress: 4900/22000, train/mmh/cross_entropy: 0.6521, train/mmh/cross_entropy/avg: 0.6386, train/total_loss: 0.6521, train/total_loss/avg: 0.6386, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 857ms, time_since_start: 01h 16m 37s 502ms, eta: 04h 40m 07s 254ms
[32m2022-10-17T17:59:42 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T17:59:42 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T17:59:43 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T17:59:59 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T17:59:59 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, train/mmh/cross_entropy: 0.6521, train/mmh/cross_entropy/avg: 0.6396, train/total_loss: 0.6521, train/total_loss/avg: 0.6396, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 1.09, time: 01m 32s 024ms, time_since_start: 01h 18m 09s 527ms, eta: 05h 42m 20s 867ms
[32m2022-10-17T17:59:59 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T17:59:59 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T17:59:59 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T18:03:01 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T18:03:01 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T18:03:01 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T18:03:12 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T18:03:23 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T18:03:23 | mmf.trainers.callbacks.logistics: [39mprogress: 5000/22000, val/mmh/cross_entropy: 0.5209, val/total_loss: 0.5209, val/mmh/accuracy: 0.7438, val/mmh/binary_f1: 0.8531, val/mmh/roc_auc: 0.7171, num_updates: 5000, epoch: 1, iterations: 5000, max_updates: 22000, val_time: 03m 24s 377ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.720194
[32m2022-10-17T18:04:39 | mmf.trainers.callbacks.logistics: [39mprogress: 5100/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6375, train/total_loss: 0.6175, train/total_loss/avg: 0.6375, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 321ms, time_since_start: 01h 22m 49s 228ms, eta: 04h 38m 33s 686ms
[32m2022-10-17T18:05:54 | mmf.trainers.callbacks.logistics: [39mprogress: 5200/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6354, train/total_loss: 0.6175, train/total_loss/avg: 0.6354, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 296ms, time_since_start: 01h 24m 04s 525ms, eta: 04h 36m 49s 160ms
[32m2022-10-17T18:07:09 | mmf.trainers.callbacks.logistics: [39mprogress: 5300/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6364, train/total_loss: 0.6175, train/total_loss/avg: 0.6364, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 155ms, time_since_start: 01h 25m 19s 680ms, eta: 04h 34m 39s 360ms
[32m2022-10-17T18:08:24 | mmf.trainers.callbacks.logistics: [39mprogress: 5400/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6367, train/total_loss: 0.6175, train/total_loss/avg: 0.6367, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 366ms, time_since_start: 01h 26m 35s 046ms, eta: 04h 33m 46s 722ms
[32m2022-10-17T18:09:39 | mmf.trainers.callbacks.logistics: [39mprogress: 5500/22000, train/mmh/cross_entropy: 0.6175, train/mmh/cross_entropy/avg: 0.6335, train/total_loss: 0.6175, train/total_loss/avg: 0.6335, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 1.35, time: 01m 14s 725ms, time_since_start: 01h 27m 49s 771ms, eta: 04h 29m 48s 807ms
[32m2022-10-17T18:10:55 | mmf.trainers.callbacks.logistics: [39mprogress: 5600/22000, train/mmh/cross_entropy: 0.6013, train/mmh/cross_entropy/avg: 0.6326, train/total_loss: 0.6013, train/total_loss/avg: 0.6326, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 426ms, time_since_start: 01h 29m 05s 197ms, eta: 04h 30m 41s 644ms
[32m2022-10-17T18:12:10 | mmf.trainers.callbacks.logistics: [39mprogress: 5700/22000, train/mmh/cross_entropy: 0.5986, train/mmh/cross_entropy/avg: 0.6304, train/total_loss: 0.5986, train/total_loss/avg: 0.6304, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 153ms, time_since_start: 01h 30m 20s 351ms, eta: 04h 28m 04s 327ms
[32m2022-10-17T18:13:25 | mmf.trainers.callbacks.logistics: [39mprogress: 5800/22000, train/mmh/cross_entropy: 0.5880, train/mmh/cross_entropy/avg: 0.6295, train/total_loss: 0.5880, train/total_loss/avg: 0.6295, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 095ms, time_since_start: 01h 31m 35s 446ms, eta: 04h 26m 13s 228ms
[32m2022-10-17T18:14:40 | mmf.trainers.callbacks.logistics: [39mprogress: 5900/22000, train/mmh/cross_entropy: 0.5880, train/mmh/cross_entropy/avg: 0.6305, train/total_loss: 0.5880, train/total_loss/avg: 0.6305, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 031ms, time_since_start: 01h 32m 50s 478ms, eta: 04h 24m 21s 083ms
[32m2022-10-17T18:15:55 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T18:15:55 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T18:15:57 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T18:16:12 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T18:16:12 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, train/mmh/cross_entropy: 0.5880, train/mmh/cross_entropy/avg: 0.6300, train/total_loss: 0.5880, train/total_loss/avg: 0.6300, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 1.09, time: 01m 32s 401ms, time_since_start: 01h 34m 22s 879ms, eta: 05h 23m 31s 710ms
[32m2022-10-17T18:16:12 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T18:16:12 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T18:16:12 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T18:19:15 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T18:19:15 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T18:19:15 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T18:19:27 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T18:19:38 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T18:19:38 | mmf.trainers.callbacks.logistics: [39mprogress: 6000/22000, val/mmh/cross_entropy: 0.5704, val/total_loss: 0.5704, val/mmh/accuracy: 0.6502, val/mmh/binary_f1: 0.7228, val/mmh/roc_auc: 0.7155, num_updates: 6000, epoch: 1, iterations: 6000, max_updates: 22000, val_time: 03m 25s 413ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.720194
[32m2022-10-17T18:20:53 | mmf.trainers.callbacks.logistics: [39mprogress: 6100/22000, train/mmh/cross_entropy: 0.5880, train/mmh/cross_entropy/avg: 0.6275, train/total_loss: 0.5880, train/total_loss/avg: 0.6275, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 647ms, time_since_start: 01h 39m 03s 942ms, eta: 04h 23m 12s 708ms
[32m2022-10-17T18:22:08 | mmf.trainers.callbacks.logistics: [39mprogress: 6200/22000, train/mmh/cross_entropy: 0.5880, train/mmh/cross_entropy/avg: 0.6303, train/total_loss: 0.5880, train/total_loss/avg: 0.6303, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 104ms, time_since_start: 01h 40m 19s 047ms, eta: 04h 19m 40s 705ms
[32m2022-10-17T18:23:24 | mmf.trainers.callbacks.logistics: [39mprogress: 6300/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6318, train/total_loss: 0.5977, train/total_loss/avg: 0.6318, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 264ms, time_since_start: 01h 41m 34s 311ms, eta: 04h 18m 35s 067ms
[32m2022-10-17T18:24:39 | mmf.trainers.callbacks.logistics: [39mprogress: 6400/22000, train/mmh/cross_entropy: 0.6503, train/mmh/cross_entropy/avg: 0.6323, train/total_loss: 0.6503, train/total_loss/avg: 0.6323, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 451ms, time_since_start: 01h 42m 49s 762ms, eta: 04h 17m 34s 516ms
[32m2022-10-17T18:25:54 | mmf.trainers.callbacks.logistics: [39mprogress: 6500/22000, train/mmh/cross_entropy: 0.6503, train/mmh/cross_entropy/avg: 0.6331, train/total_loss: 0.6503, train/total_loss/avg: 0.6331, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 250ms, time_since_start: 01h 44m 05s 012ms, eta: 04h 15m 14s 537ms
[32m2022-10-17T18:27:10 | mmf.trainers.callbacks.logistics: [39mprogress: 6600/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6315, train/total_loss: 0.5977, train/total_loss/avg: 0.6315, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 284ms, time_since_start: 01h 45m 20s 297ms, eta: 04h 13m 42s 731ms
[32m2022-10-17T18:28:25 | mmf.trainers.callbacks.logistics: [39mprogress: 6700/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6321, train/total_loss: 0.5977, train/total_loss/avg: 0.6321, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 049ms, time_since_start: 01h 46m 35s 347ms, eta: 04h 11m 16s 576ms
[32m2022-10-17T18:29:40 | mmf.trainers.callbacks.logistics: [39mprogress: 6800/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6329, train/total_loss: 0.5977, train/total_loss/avg: 0.6329, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 599ms, time_since_start: 01h 47m 50s 946ms, eta: 04h 11m 27s 752ms
[32m2022-10-17T18:30:57 | mmf.trainers.callbacks.logistics: [39mprogress: 6900/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6319, train/total_loss: 0.5977, train/total_loss/avg: 0.6319, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 1.32, time: 01m 16s 425ms, time_since_start: 01h 49m 07s 371ms, eta: 04h 12m 32s 427ms
[32m2022-10-17T18:32:12 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T18:32:12 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T18:32:14 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T18:32:29 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T18:32:29 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, train/mmh/cross_entropy: 0.5880, train/mmh/cross_entropy/avg: 0.6312, train/total_loss: 0.5880, train/total_loss/avg: 0.6312, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 1.09, time: 01m 32s 404ms, time_since_start: 01h 50m 39s 776ms, eta: 05h 03m 19s 024ms
[32m2022-10-17T18:32:29 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T18:32:29 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T18:32:29 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T18:35:33 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T18:35:33 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T18:35:33 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T18:35:44 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T18:35:55 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T18:35:55 | mmf.trainers.callbacks.logistics: [39mprogress: 7000/22000, val/mmh/cross_entropy: 0.5206, val/total_loss: 0.5206, val/mmh/accuracy: 0.6504, val/mmh/binary_f1: 0.7240, val/mmh/roc_auc: 0.7153, num_updates: 7000, epoch: 1, iterations: 7000, max_updates: 22000, val_time: 03m 26s 127ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.720194
[32m2022-10-17T18:37:12 | mmf.trainers.callbacks.logistics: [39mprogress: 7100/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6316, train/total_loss: 0.5977, train/total_loss/avg: 0.6316, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 1.30, time: 01m 17s 142ms, time_since_start: 01h 55m 23s 048ms, eta: 04h 11m 32s 009ms
[32m2022-10-17T18:38:29 | mmf.trainers.callbacks.logistics: [39mprogress: 7200/22000, train/mmh/cross_entropy: 0.6503, train/mmh/cross_entropy/avg: 0.6326, train/total_loss: 0.6503, train/total_loss/avg: 0.6326, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 1.32, time: 01m 16s 667ms, time_since_start: 01h 56m 39s 716ms, eta: 04h 08m 18s 410ms
[32m2022-10-17T18:39:44 | mmf.trainers.callbacks.logistics: [39mprogress: 7300/22000, train/mmh/cross_entropy: 0.6192, train/mmh/cross_entropy/avg: 0.6324, train/total_loss: 0.6192, train/total_loss/avg: 0.6324, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 095ms, time_since_start: 01h 57m 54s 811ms, eta: 04h 01m 34s 200ms
[32m2022-10-17T18:41:00 | mmf.trainers.callbacks.logistics: [39mprogress: 7400/22000, train/mmh/cross_entropy: 0.6000, train/mmh/cross_entropy/avg: 0.6319, train/total_loss: 0.6000, train/total_loss/avg: 0.6319, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 1.32, time: 01m 16s 081ms, time_since_start: 01h 59m 10s 892ms, eta: 04h 03m 04s 632ms
[32m2022-10-17T18:42:15 | mmf.trainers.callbacks.logistics: [39mprogress: 7500/22000, train/mmh/cross_entropy: 0.6192, train/mmh/cross_entropy/avg: 0.6334, train/total_loss: 0.6192, train/total_loss/avg: 0.6334, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 129ms, time_since_start: 02h 26s 022ms, eta: 03h 58m 23s 503ms
[32m2022-10-17T18:43:31 | mmf.trainers.callbacks.logistics: [39mprogress: 7600/22000, train/mmh/cross_entropy: 0.6192, train/mmh/cross_entropy/avg: 0.6322, train/total_loss: 0.6192, train/total_loss/avg: 0.6322, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 152ms, time_since_start: 02h 01m 41s 174ms, eta: 03h 56m 49s 203ms
[32m2022-10-17T18:44:46 | mmf.trainers.callbacks.logistics: [39mprogress: 7700/22000, train/mmh/cross_entropy: 0.6568, train/mmh/cross_entropy/avg: 0.6348, train/total_loss: 0.6568, train/total_loss/avg: 0.6348, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 748ms, time_since_start: 02h 02m 56s 922ms, eta: 03h 57m 02s 435ms
[32m2022-10-17T18:46:02 | mmf.trainers.callbacks.logistics: [39mprogress: 7800/22000, train/mmh/cross_entropy: 0.6649, train/mmh/cross_entropy/avg: 0.6354, train/total_loss: 0.6649, train/total_loss/avg: 0.6354, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 365ms, time_since_start: 02h 04m 12s 288ms, eta: 03h 54m 11s 573ms
[32m2022-10-17T18:47:17 | mmf.trainers.callbacks.logistics: [39mprogress: 7900/22000, train/mmh/cross_entropy: 0.6568, train/mmh/cross_entropy/avg: 0.6339, train/total_loss: 0.6568, train/total_loss/avg: 0.6339, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 1.33, time: 01m 15s 348ms, time_since_start: 02h 05m 27s 636ms, eta: 03h 52m 29s 420ms
[32m2022-10-17T18:48:32 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T18:48:32 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T18:48:34 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T18:48:49 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T18:48:49 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, train/mmh/cross_entropy: 0.6649, train/mmh/cross_entropy/avg: 0.6345, train/total_loss: 0.6649, train/total_loss/avg: 0.6345, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 249ms, time_since_start: 02h 06m 59s 885ms, eta: 04h 42m 37s 256ms
[32m2022-10-17T18:48:49 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T18:48:49 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T18:48:49 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T18:51:53 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T18:51:53 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T18:51:53 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T18:52:04 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T18:52:16 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T18:52:16 | mmf.trainers.callbacks.logistics: [39mprogress: 8000/22000, val/mmh/cross_entropy: 0.5364, val/total_loss: 0.5364, val/mmh/accuracy: 0.6484, val/mmh/binary_f1: 0.7215, val/mmh/roc_auc: 0.7150, num_updates: 8000, epoch: 1, iterations: 8000, max_updates: 22000, val_time: 03m 26s 333ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.720194
[32m2022-10-17T18:53:32 | mmf.trainers.callbacks.logistics: [39mprogress: 8100/22000, train/mmh/cross_entropy: 0.6725, train/mmh/cross_entropy/avg: 0.6358, train/total_loss: 0.6725, train/total_loss/avg: 0.6358, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 987ms, time_since_start: 02h 11m 42s 213ms, eta: 03h 51m 08s 329ms
[32m2022-10-17T18:54:48 | mmf.trainers.callbacks.logistics: [39mprogress: 8200/22000, train/mmh/cross_entropy: 0.6649, train/mmh/cross_entropy/avg: 0.6343, train/total_loss: 0.6649, train/total_loss/avg: 0.6343, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 965ms, time_since_start: 02h 12m 58s 178ms, eta: 03h 49m 24s 411ms
[32m2022-10-17T18:56:03 | mmf.trainers.callbacks.logistics: [39mprogress: 8300/22000, train/mmh/cross_entropy: 0.6568, train/mmh/cross_entropy/avg: 0.6341, train/total_loss: 0.6568, train/total_loss/avg: 0.6341, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 706ms, time_since_start: 02h 14m 13s 884ms, eta: 03h 46m 58s 197ms
[32m2022-10-17T18:57:20 | mmf.trainers.callbacks.logistics: [39mprogress: 8400/22000, train/mmh/cross_entropy: 0.6300, train/mmh/cross_entropy/avg: 0.6340, train/total_loss: 0.6300, train/total_loss/avg: 0.6340, max mem: 5053.0, experiment: mmh-ft, epoch: 1, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 1.32, time: 01m 16s 416ms, time_since_start: 02h 15m 30s 301ms, eta: 03h 47m 25s 610ms
[32m2022-10-17T18:58:34 | mmf.trainers.callbacks.logistics: [39mprogress: 8500/22000, train/mmh/cross_entropy: 0.6300, train/mmh/cross_entropy/avg: 0.6341, train/total_loss: 0.6300, train/total_loss/avg: 0.6341, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 1.35, time: 01m 14s 422ms, time_since_start: 02h 16m 44s 724ms, eta: 03h 39m 51s 787ms
[32m2022-10-17T18:59:49 | mmf.trainers.callbacks.logistics: [39mprogress: 8600/22000, train/mmh/cross_entropy: 0.6300, train/mmh/cross_entropy/avg: 0.6333, train/total_loss: 0.6300, train/total_loss/avg: 0.6333, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 089ms, time_since_start: 02h 17m 59s 813ms, eta: 03h 40m 11s 399ms
[32m2022-10-17T19:01:05 | mmf.trainers.callbacks.logistics: [39mprogress: 8700/22000, train/mmh/cross_entropy: 0.6300, train/mmh/cross_entropy/avg: 0.6343, train/total_loss: 0.6300, train/total_loss/avg: 0.6343, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 668ms, time_since_start: 02h 19m 15s 482ms, eta: 03h 40m 13s 936ms
[32m2022-10-17T19:02:21 | mmf.trainers.callbacks.logistics: [39mprogress: 8800/22000, train/mmh/cross_entropy: 0.6192, train/mmh/cross_entropy/avg: 0.6337, train/total_loss: 0.6192, train/total_loss/avg: 0.6337, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 1.32, time: 01m 16s 275ms, time_since_start: 02h 20m 31s 758ms, eta: 03h 40m 19s 778ms
[32m2022-10-17T19:03:37 | mmf.trainers.callbacks.logistics: [39mprogress: 8900/22000, train/mmh/cross_entropy: 0.6192, train/mmh/cross_entropy/avg: 0.6334, train/total_loss: 0.6192, train/total_loss/avg: 0.6334, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 1.32, time: 01m 16s 337ms, time_since_start: 02h 21m 48s 095ms, eta: 03h 38m 50s 229ms
[32m2022-10-17T19:04:54 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T19:04:54 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T19:04:55 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T19:05:11 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T19:05:11 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, train/mmh/cross_entropy: 0.6192, train/mmh/cross_entropy/avg: 0.6325, train/total_loss: 0.6192, train/total_loss/avg: 0.6325, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 1.08, time: 01m 33s 869ms, time_since_start: 02h 23m 21s 965ms, eta: 04h 27m 02s 679ms
[32m2022-10-17T19:05:11 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T19:05:11 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T19:05:11 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T19:08:13 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T19:08:13 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T19:08:13 | mmf.trainers.callbacks.logistics: [39mprogress: 9000/22000, val/mmh/cross_entropy: 0.5425, val/total_loss: 0.5425, val/mmh/accuracy: 0.6406, val/mmh/binary_f1: 0.7117, val/mmh/roc_auc: 0.7167, num_updates: 9000, epoch: 2, iterations: 9000, max_updates: 22000, val_time: 03m 01s 452ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.720194
[32m2022-10-17T19:09:32 | mmf.trainers.callbacks.logistics: [39mprogress: 9100/22000, train/mmh/cross_entropy: 0.6192, train/mmh/cross_entropy/avg: 0.6324, train/total_loss: 0.6192, train/total_loss/avg: 0.6324, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 1.27, time: 01m 19s 059ms, time_since_start: 02h 27m 42s 479ms, eta: 03h 43m 10s 794ms
[32m2022-10-17T19:10:50 | mmf.trainers.callbacks.logistics: [39mprogress: 9200/22000, train/mmh/cross_entropy: 0.6178, train/mmh/cross_entropy/avg: 0.6316, train/total_loss: 0.6178, train/total_loss/avg: 0.6316, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 1.28, time: 01m 18s 126ms, time_since_start: 02h 29m 606ms, eta: 03h 38m 50s 303ms
[32m2022-10-17T19:12:10 | mmf.trainers.callbacks.logistics: [39mprogress: 9300/22000, train/mmh/cross_entropy: 0.6178, train/mmh/cross_entropy/avg: 0.6319, train/total_loss: 0.6178, train/total_loss/avg: 0.6319, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 1.25, time: 01m 20s 441ms, time_since_start: 02h 30m 21s 048ms, eta: 03h 43m 33s 745ms
[32m2022-10-17T19:13:32 | mmf.trainers.callbacks.logistics: [39mprogress: 9400/22000, train/mmh/cross_entropy: 0.6178, train/mmh/cross_entropy/avg: 0.6305, train/total_loss: 0.6178, train/total_loss/avg: 0.6305, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 1.23, time: 01m 21s 209ms, time_since_start: 02h 31m 42s 257ms, eta: 03h 43m 55s 123ms
[32m2022-10-17T19:14:49 | mmf.trainers.callbacks.logistics: [39mprogress: 9500/22000, train/mmh/cross_entropy: 0.6178, train/mmh/cross_entropy/avg: 0.6315, train/total_loss: 0.6178, train/total_loss/avg: 0.6315, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 1.30, time: 01m 17s 124ms, time_since_start: 02h 32m 59s 382ms, eta: 03h 30m 58s 123ms
[32m2022-10-17T19:16:05 | mmf.trainers.callbacks.logistics: [39mprogress: 9600/22000, train/mmh/cross_entropy: 0.6178, train/mmh/cross_entropy/avg: 0.6311, train/total_loss: 0.6178, train/total_loss/avg: 0.6311, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 1.32, time: 01m 16s 460ms, time_since_start: 02h 34m 15s 842ms, eta: 03h 27m 28s 647ms
[32m2022-10-17T19:17:21 | mmf.trainers.callbacks.logistics: [39mprogress: 9700/22000, train/mmh/cross_entropy: 0.6038, train/mmh/cross_entropy/avg: 0.6306, train/total_loss: 0.6038, train/total_loss/avg: 0.6306, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 683ms, time_since_start: 02h 35m 31s 526ms, eta: 03h 23m 42s 856ms
[32m2022-10-17T19:18:36 | mmf.trainers.callbacks.logistics: [39mprogress: 9800/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6296, train/total_loss: 0.5977, train/total_loss/avg: 0.6296, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 1.35, time: 01m 14s 998ms, time_since_start: 02h 36m 46s 525ms, eta: 03h 20m 13s 749ms
[32m2022-10-17T19:19:52 | mmf.trainers.callbacks.logistics: [39mprogress: 9900/22000, train/mmh/cross_entropy: 0.6038, train/mmh/cross_entropy/avg: 0.6318, train/total_loss: 0.6038, train/total_loss/avg: 0.6318, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 700ms, time_since_start: 02h 38m 02s 225ms, eta: 03h 20m 26s 728ms
[32m2022-10-17T19:21:07 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T19:21:07 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T19:21:09 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T19:21:25 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T19:21:25 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6304, train/total_loss: 0.5977, train/total_loss/avg: 0.6304, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 1.08, time: 01m 33s 800ms, time_since_start: 02h 39m 36s 026ms, eta: 04h 06m 19s 233ms
[32m2022-10-17T19:21:25 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T19:21:25 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T19:21:25 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T19:24:23 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T19:24:23 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T19:24:24 | mmf.trainers.callbacks.logistics: [39mprogress: 10000/22000, val/mmh/cross_entropy: 0.5439, val/total_loss: 0.5439, val/mmh/accuracy: 0.7438, val/mmh/binary_f1: 0.8531, val/mmh/roc_auc: 0.6772, num_updates: 10000, epoch: 2, iterations: 10000, max_updates: 22000, val_time: 02m 58s 159ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.720194
[32m2022-10-17T19:25:39 | mmf.trainers.callbacks.logistics: [39mprogress: 10100/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6306, train/total_loss: 0.5977, train/total_loss/avg: 0.6306, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 636ms, time_since_start: 02h 43m 49s 824ms, eta: 03h 16m 57s 960ms
[32m2022-10-17T19:26:55 | mmf.trainers.callbacks.logistics: [39mprogress: 10200/22000, train/mmh/cross_entropy: 0.6038, train/mmh/cross_entropy/avg: 0.6308, train/total_loss: 0.6038, train/total_loss/avg: 0.6308, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 710ms, time_since_start: 02h 45m 05s 535ms, eta: 03h 15m 30s 160ms
[32m2022-10-17T19:28:10 | mmf.trainers.callbacks.logistics: [39mprogress: 10300/22000, train/mmh/cross_entropy: 0.6038, train/mmh/cross_entropy/avg: 0.6314, train/total_loss: 0.6038, train/total_loss/avg: 0.6314, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 464ms, time_since_start: 02h 46m 21s 000ms, eta: 03h 13m 12s 942ms
[32m2022-10-17T19:29:26 | mmf.trainers.callbacks.logistics: [39mprogress: 10400/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6307, train/total_loss: 0.5977, train/total_loss/avg: 0.6307, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 274ms, time_since_start: 02h 47m 36s 274ms, eta: 03h 11m 04s 952ms
[32m2022-10-17T19:30:42 | mmf.trainers.callbacks.logistics: [39mprogress: 10500/22000, train/mmh/cross_entropy: 0.5798, train/mmh/cross_entropy/avg: 0.6288, train/total_loss: 0.5798, train/total_loss/avg: 0.6288, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 1.32, time: 01m 16s 602ms, time_since_start: 02h 48m 52s 877ms, eta: 03h 12m 46s 658ms
[32m2022-10-17T19:31:58 | mmf.trainers.callbacks.logistics: [39mprogress: 10600/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6290, train/total_loss: 0.5977, train/total_loss/avg: 0.6290, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 276ms, time_since_start: 02h 50m 08s 153ms, eta: 03h 07m 47s 472ms
[32m2022-10-17T19:33:13 | mmf.trainers.callbacks.logistics: [39mprogress: 10700/22000, train/mmh/cross_entropy: 0.5798, train/mmh/cross_entropy/avg: 0.6283, train/total_loss: 0.5798, train/total_loss/avg: 0.6283, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 111ms, time_since_start: 02h 51m 23s 265ms, eta: 03h 05m 44s 234ms
[32m2022-10-17T19:34:29 | mmf.trainers.callbacks.logistics: [39mprogress: 10800/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6289, train/total_loss: 0.5977, train/total_loss/avg: 0.6289, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 895ms, time_since_start: 02h 52m 39s 161ms, eta: 03h 06m 911ms
[32m2022-10-17T19:35:44 | mmf.trainers.callbacks.logistics: [39mprogress: 10900/22000, train/mmh/cross_entropy: 0.5796, train/mmh/cross_entropy/avg: 0.6277, train/total_loss: 0.5796, train/total_loss/avg: 0.6277, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 181ms, time_since_start: 02h 53m 54s 342ms, eta: 03h 02m 37s 190ms
[32m2022-10-17T19:36:59 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T19:36:59 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T19:37:01 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T19:37:14 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T19:37:14 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6292, train/total_loss: 0.5977, train/total_loss/avg: 0.6292, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 485ms, time_since_start: 02h 55m 24s 827ms, eta: 03h 37m 48s 771ms
[32m2022-10-17T19:37:14 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T19:37:14 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T19:37:14 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T19:40:17 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T19:40:17 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T19:40:17 | mmf.trainers.callbacks.logistics: [39mprogress: 11000/22000, val/mmh/cross_entropy: 0.5259, val/total_loss: 0.5259, val/mmh/accuracy: 0.7438, val/mmh/binary_f1: 0.8531, val/mmh/roc_auc: 0.7136, num_updates: 11000, epoch: 2, iterations: 11000, max_updates: 22000, val_time: 03m 02s 960ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.720194
[32m2022-10-17T19:41:33 | mmf.trainers.callbacks.logistics: [39mprogress: 11100/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6290, train/total_loss: 0.5977, train/total_loss/avg: 0.6290, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 525ms, time_since_start: 02h 59m 43s 314ms, eta: 03h 08s 924ms
[32m2022-10-17T19:42:49 | mmf.trainers.callbacks.logistics: [39mprogress: 11200/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6286, train/total_loss: 0.5977, train/total_loss/avg: 0.6286, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 1.32, time: 01m 16s 024ms, time_since_start: 03h 59s 338ms, eta: 02h 59m 40s 536ms
[32m2022-10-17T19:44:04 | mmf.trainers.callbacks.logistics: [39mprogress: 11300/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6286, train/total_loss: 0.5977, train/total_loss/avg: 0.6286, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 197ms, time_since_start: 03h 02m 14s 536ms, eta: 02h 56m 04s 602ms
[32m2022-10-17T19:45:20 | mmf.trainers.callbacks.logistics: [39mprogress: 11400/22000, train/mmh/cross_entropy: 0.6118, train/mmh/cross_entropy/avg: 0.6295, train/total_loss: 0.6118, train/total_loss/avg: 0.6295, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 788ms, time_since_start: 03h 03m 30s 325ms, eta: 02h 55m 48s 140ms
[32m2022-10-17T19:46:36 | mmf.trainers.callbacks.logistics: [39mprogress: 11500/22000, train/mmh/cross_entropy: 0.5977, train/mmh/cross_entropy/avg: 0.6284, train/total_loss: 0.5977, train/total_loss/avg: 0.6284, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 1.32, time: 01m 16s 484ms, time_since_start: 03h 04m 46s 809ms, eta: 02h 55m 44s 474ms
[32m2022-10-17T19:47:51 | mmf.trainers.callbacks.logistics: [39mprogress: 11600/22000, train/mmh/cross_entropy: 0.6118, train/mmh/cross_entropy/avg: 0.6293, train/total_loss: 0.6118, train/total_loss/avg: 0.6293, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 229ms, time_since_start: 03h 06m 02s 038ms, eta: 02h 51m 12s 701ms
[32m2022-10-17T19:49:07 | mmf.trainers.callbacks.logistics: [39mprogress: 11700/22000, train/mmh/cross_entropy: 0.6269, train/mmh/cross_entropy/avg: 0.6300, train/total_loss: 0.6269, train/total_loss/avg: 0.6300, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 286ms, time_since_start: 03h 07m 17s 325ms, eta: 02h 49m 41s 697ms
[32m2022-10-17T19:50:22 | mmf.trainers.callbacks.logistics: [39mprogress: 11800/22000, train/mmh/cross_entropy: 0.6269, train/mmh/cross_entropy/avg: 0.6293, train/total_loss: 0.6269, train/total_loss/avg: 0.6293, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 451ms, time_since_start: 03h 08m 32s 777ms, eta: 02h 48m 24s 982ms
[32m2022-10-17T19:51:37 | mmf.trainers.callbacks.logistics: [39mprogress: 11900/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6293, train/total_loss: 0.6223, train/total_loss/avg: 0.6293, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 1.33, time: 01m 15s 328ms, time_since_start: 03h 09m 48s 106ms, eta: 02h 46m 29s 575ms
[32m2022-10-17T19:52:53 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T19:52:53 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T19:52:55 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T19:53:08 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T19:53:08 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, train/mmh/cross_entropy: 0.6269, train/mmh/cross_entropy/avg: 0.6294, train/total_loss: 0.6269, train/total_loss/avg: 0.6294, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 830ms, time_since_start: 03h 11m 18s 936ms, eta: 03h 18m 46s 038ms
[32m2022-10-17T19:53:08 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T19:53:08 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T19:53:08 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T19:56:10 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T19:56:10 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T19:56:10 | mmf.trainers.callbacks.logistics: [39mprogress: 12000/22000, val/mmh/cross_entropy: 0.5363, val/total_loss: 0.5363, val/mmh/accuracy: 0.7458, val/mmh/binary_f1: 0.8539, val/mmh/roc_auc: 0.7136, num_updates: 12000, epoch: 2, iterations: 12000, max_updates: 22000, val_time: 03m 01s 302ms, best_update: 4000, best_iteration: 4000, best_val/mmh/roc_auc: 0.720194
[32m2022-10-17T19:57:26 | mmf.trainers.callbacks.logistics: [39mprogress: 12100/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6289, train/total_loss: 0.6223, train/total_loss/avg: 0.6289, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 998ms, time_since_start: 03h 15m 36s 243ms, eta: 02h 44m 38s 880ms
[32m2022-10-17T19:58:42 | mmf.trainers.callbacks.logistics: [39mprogress: 12200/22000, train/mmh/cross_entropy: 0.6118, train/mmh/cross_entropy/avg: 0.6286, train/total_loss: 0.6118, train/total_loss/avg: 0.6286, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 950ms, time_since_start: 03h 16m 52s 193ms, eta: 02h 42m 52s 793ms
[32m2022-10-17T19:59:57 | mmf.trainers.callbacks.logistics: [39mprogress: 12300/22000, train/mmh/cross_entropy: 0.6118, train/mmh/cross_entropy/avg: 0.6288, train/total_loss: 0.6118, train/total_loss/avg: 0.6288, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 680ms, time_since_start: 03h 18m 07s 874ms, eta: 02h 40m 38s 765ms
[32m2022-10-17T20:01:13 | mmf.trainers.callbacks.logistics: [39mprogress: 12400/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6298, train/total_loss: 0.6223, train/total_loss/avg: 0.6298, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 479ms, time_since_start: 03h 19m 23s 354ms, eta: 02h 38m 34s 063ms
[32m2022-10-17T20:02:28 | mmf.trainers.callbacks.logistics: [39mprogress: 12500/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6289, train/total_loss: 0.6223, train/total_loss/avg: 0.6289, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 066ms, time_since_start: 03h 20m 38s 420ms, eta: 02h 36m 03s 366ms
[32m2022-10-17T20:03:43 | mmf.trainers.callbacks.logistics: [39mprogress: 12600/22000, train/mmh/cross_entropy: 0.6118, train/mmh/cross_entropy/avg: 0.6284, train/total_loss: 0.6118, train/total_loss/avg: 0.6284, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 396ms, time_since_start: 03h 21m 53s 817ms, eta: 02h 35m 05s 606ms
[32m2022-10-17T20:04:58 | mmf.trainers.callbacks.logistics: [39mprogress: 12700/22000, train/mmh/cross_entropy: 0.6118, train/mmh/cross_entropy/avg: 0.6278, train/total_loss: 0.6118, train/total_loss/avg: 0.6278, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 271ms, time_since_start: 03h 23m 09s 088ms, eta: 02h 33m 11s 277ms
[32m2022-10-17T20:06:14 | mmf.trainers.callbacks.logistics: [39mprogress: 12800/22000, train/mmh/cross_entropy: 0.5888, train/mmh/cross_entropy/avg: 0.6274, train/total_loss: 0.5888, train/total_loss/avg: 0.6274, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 145ms, time_since_start: 03h 24m 24s 233ms, eta: 02h 31m 17s 302ms
[32m2022-10-17T20:07:29 | mmf.trainers.callbacks.logistics: [39mprogress: 12900/22000, train/mmh/cross_entropy: 0.5888, train/mmh/cross_entropy/avg: 0.6266, train/total_loss: 0.5888, train/total_loss/avg: 0.6266, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 449ms, time_since_start: 03h 25m 39s 683ms, eta: 02h 30m 14s 993ms
[32m2022-10-17T20:08:45 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T20:08:45 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T20:08:46 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T20:08:58 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T20:08:58 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, train/mmh/cross_entropy: 0.5888, train/mmh/cross_entropy/avg: 0.6268, train/total_loss: 0.5888, train/total_loss/avg: 0.6268, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 342ms, time_since_start: 03h 27m 09s 026ms, eta: 02h 55m 57s 582ms
[32m2022-10-17T20:08:58 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T20:08:58 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T20:08:58 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T20:11:59 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T20:11:59 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T20:11:59 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T20:12:08 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T20:12:16 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T20:12:24 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T20:12:24 | mmf.trainers.callbacks.logistics: [39mprogress: 13000/22000, val/mmh/cross_entropy: 0.5351, val/total_loss: 0.5351, val/mmh/accuracy: 0.6488, val/mmh/binary_f1: 0.7205, val/mmh/roc_auc: 0.7205, num_updates: 13000, epoch: 2, iterations: 13000, max_updates: 22000, val_time: 03m 25s 971ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.720545
[32m2022-10-17T20:13:40 | mmf.trainers.callbacks.logistics: [39mprogress: 13100/22000, train/mmh/cross_entropy: 0.5888, train/mmh/cross_entropy/avg: 0.6273, train/total_loss: 0.5888, train/total_loss/avg: 0.6273, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 540ms, time_since_start: 03h 31m 50s 544ms, eta: 02h 27m 07s 476ms
[32m2022-10-17T20:14:56 | mmf.trainers.callbacks.logistics: [39mprogress: 13200/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6284, train/total_loss: 0.6223, train/total_loss/avg: 0.6284, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 1.32, time: 01m 16s 098ms, time_since_start: 03h 33m 06s 643ms, eta: 02h 26m 32s 758ms
[32m2022-10-17T20:16:12 | mmf.trainers.callbacks.logistics: [39mprogress: 13300/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6290, train/total_loss: 0.6223, train/total_loss/avg: 0.6290, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 1.32, time: 01m 16s 390ms, time_since_start: 03h 34m 23s 033ms, eta: 02h 25m 26s 163ms
[32m2022-10-17T20:17:27 | mmf.trainers.callbacks.logistics: [39mprogress: 13400/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6300, train/total_loss: 0.6223, train/total_loss/avg: 0.6300, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 104ms, time_since_start: 03h 35m 38s 137ms, eta: 02h 21m 20s 627ms
[32m2022-10-17T20:18:44 | mmf.trainers.callbacks.logistics: [39mprogress: 13500/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6290, train/total_loss: 0.6223, train/total_loss/avg: 0.6290, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 1.32, time: 01m 16s 358ms, time_since_start: 03h 36m 54s 496ms, eta: 02h 22m 02s 044ms
[32m2022-10-17T20:20:00 | mmf.trainers.callbacks.logistics: [39mprogress: 13600/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6290, train/total_loss: 0.6223, train/total_loss/avg: 0.6290, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 835ms, time_since_start: 03h 38m 10s 332ms, eta: 02h 19m 24s 085ms
[32m2022-10-17T20:21:17 | mmf.trainers.callbacks.logistics: [39mprogress: 13700/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6300, train/total_loss: 0.6223, train/total_loss/avg: 0.6300, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 1.32, time: 01m 16s 987ms, time_since_start: 03h 39m 27s 319ms, eta: 02h 19m 49s 974ms
[32m2022-10-17T20:22:32 | mmf.trainers.callbacks.logistics: [39mprogress: 13800/22000, train/mmh/cross_entropy: 0.6223, train/mmh/cross_entropy/avg: 0.6295, train/total_loss: 0.6223, train/total_loss/avg: 0.6295, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 542ms, time_since_start: 03h 40m 42s 861ms, eta: 02h 15m 33s 321ms
[32m2022-10-17T20:23:48 | mmf.trainers.callbacks.logistics: [39mprogress: 13900/22000, train/mmh/cross_entropy: 0.6398, train/mmh/cross_entropy/avg: 0.6299, train/total_loss: 0.6398, train/total_loss/avg: 0.6299, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 872ms, time_since_start: 03h 41m 58s 734ms, eta: 02h 14m 29s 264ms
[32m2022-10-17T20:25:03 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T20:25:03 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T20:25:05 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T20:25:16 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T20:25:16 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, train/mmh/cross_entropy: 0.5888, train/mmh/cross_entropy/avg: 0.6293, train/total_loss: 0.5888, train/total_loss/avg: 0.6293, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 1.15, time: 01m 27s 669ms, time_since_start: 03h 43m 26s 403ms, eta: 02h 33m 28s 771ms
[32m2022-10-17T20:25:16 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T20:25:16 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T20:25:16 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T20:28:17 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T20:28:17 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T20:28:17 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T20:28:26 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T20:28:34 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T20:28:34 | mmf.trainers.callbacks.logistics: [39mprogress: 14000/22000, val/mmh/cross_entropy: 0.5329, val/total_loss: 0.5329, val/mmh/accuracy: 0.7262, val/mmh/binary_f1: 0.8281, val/mmh/roc_auc: 0.7177, num_updates: 14000, epoch: 2, iterations: 14000, max_updates: 22000, val_time: 03m 18s 158ms, best_update: 13000, best_iteration: 13000, best_val/mmh/roc_auc: 0.720545
[32m2022-10-17T20:29:50 | mmf.trainers.callbacks.logistics: [39mprogress: 14100/22000, train/mmh/cross_entropy: 0.6398, train/mmh/cross_entropy/avg: 0.6301, train/total_loss: 0.6398, train/total_loss/avg: 0.6301, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 1.32, time: 01m 16s 417ms, time_since_start: 03h 48m 985ms, eta: 02h 12m 06s 582ms
[32m2022-10-17T20:31:08 | mmf.trainers.callbacks.logistics: [39mprogress: 14200/22000, train/mmh/cross_entropy: 0.6398, train/mmh/cross_entropy/avg: 0.6291, train/total_loss: 0.6398, train/total_loss/avg: 0.6291, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 1.30, time: 01m 17s 210ms, time_since_start: 03h 49m 18s 196ms, eta: 02h 11m 47s 451ms
[32m2022-10-17T20:32:23 | mmf.trainers.callbacks.logistics: [39mprogress: 14300/22000, train/mmh/cross_entropy: 0.6398, train/mmh/cross_entropy/avg: 0.6294, train/total_loss: 0.6398, train/total_loss/avg: 0.6294, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 478ms, time_since_start: 03h 50m 33s 674ms, eta: 02h 07m 10s 984ms
[32m2022-10-17T20:33:39 | mmf.trainers.callbacks.logistics: [39mprogress: 14400/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6293, train/total_loss: 0.6080, train/total_loss/avg: 0.6293, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 730ms, time_since_start: 03h 51m 49s 405ms, eta: 02h 05m 57s 009ms
[32m2022-10-17T20:34:55 | mmf.trainers.callbacks.logistics: [39mprogress: 14500/22000, train/mmh/cross_entropy: 0.6230, train/mmh/cross_entropy/avg: 0.6292, train/total_loss: 0.6230, train/total_loss/avg: 0.6292, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 1.32, time: 01m 16s 354ms, time_since_start: 03h 53m 05s 760ms, eta: 02h 05m 19s 034ms
[32m2022-10-17T20:36:11 | mmf.trainers.callbacks.logistics: [39mprogress: 14600/22000, train/mmh/cross_entropy: 0.6230, train/mmh/cross_entropy/avg: 0.6287, train/total_loss: 0.6230, train/total_loss/avg: 0.6287, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 383ms, time_since_start: 03h 54m 21s 144ms, eta: 02h 02m 04s 429ms
[32m2022-10-17T20:37:25 | mmf.trainers.callbacks.logistics: [39mprogress: 14700/22000, train/mmh/cross_entropy: 0.6230, train/mmh/cross_entropy/avg: 0.6278, train/total_loss: 0.6230, train/total_loss/avg: 0.6278, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 1.35, time: 01m 14s 834ms, time_since_start: 03h 55m 35s 978ms, eta: 01h 59m 32s 819ms
[32m2022-10-17T20:38:40 | mmf.trainers.callbacks.logistics: [39mprogress: 14800/22000, train/mmh/cross_entropy: 0.6230, train/mmh/cross_entropy/avg: 0.6265, train/total_loss: 0.6230, train/total_loss/avg: 0.6265, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 1.35, time: 01m 14s 877ms, time_since_start: 03h 56m 50s 855ms, eta: 01h 57m 58s 578ms
[32m2022-10-17T20:39:57 | mmf.trainers.callbacks.logistics: [39mprogress: 14900/22000, train/mmh/cross_entropy: 0.6230, train/mmh/cross_entropy/avg: 0.6259, train/total_loss: 0.6230, train/total_loss/avg: 0.6259, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 1.32, time: 01m 16s 499ms, time_since_start: 03h 58m 07s 354ms, eta: 01h 58m 51s 471ms
[32m2022-10-17T20:41:12 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T20:41:12 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T20:41:13 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T20:41:25 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T20:41:25 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, train/mmh/cross_entropy: 0.6230, train/mmh/cross_entropy/avg: 0.6262, train/total_loss: 0.6230, train/total_loss/avg: 0.6262, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 301ms, time_since_start: 03h 59m 35s 656ms, eta: 02h 15m 15s 797ms
[32m2022-10-17T20:41:25 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T20:41:25 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T20:41:25 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T20:44:26 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T20:44:26 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T20:44:26 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T20:44:35 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T20:44:43 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T20:44:52 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T20:44:52 | mmf.trainers.callbacks.logistics: [39mprogress: 15000/22000, val/mmh/cross_entropy: 0.5319, val/total_loss: 0.5319, val/mmh/accuracy: 0.6590, val/mmh/binary_f1: 0.7337, val/mmh/roc_auc: 0.7234, num_updates: 15000, epoch: 2, iterations: 15000, max_updates: 22000, val_time: 03m 27s 001ms, best_update: 15000, best_iteration: 15000, best_val/mmh/roc_auc: 0.723409
[32m2022-10-17T20:46:08 | mmf.trainers.callbacks.logistics: [39mprogress: 15100/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6259, train/total_loss: 0.6080, train/total_loss/avg: 0.6259, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 1.32, time: 01m 16s 378ms, time_since_start: 04h 04m 19s 038ms, eta: 01h 55m 19s 631ms
[32m2022-10-17T20:47:24 | mmf.trainers.callbacks.logistics: [39mprogress: 15200/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6258, train/total_loss: 0.6080, train/total_loss/avg: 0.6258, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 885ms, time_since_start: 04h 05m 34s 924ms, eta: 01h 52m 55s 397ms
[32m2022-10-17T20:48:43 | mmf.trainers.callbacks.logistics: [39mprogress: 15300/22000, train/mmh/cross_entropy: 0.6080, train/mmh/cross_entropy/avg: 0.6260, train/total_loss: 0.6080, train/total_loss/avg: 0.6260, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 1.28, time: 01m 18s 551ms, time_since_start: 04h 06m 53s 475ms, eta: 01h 55m 10s 229ms
[32m2022-10-17T20:49:59 | mmf.trainers.callbacks.logistics: [39mprogress: 15400/22000, train/mmh/cross_entropy: 0.6064, train/mmh/cross_entropy/avg: 0.6259, train/total_loss: 0.6064, train/total_loss/avg: 0.6259, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 1.32, time: 01m 16s 008ms, time_since_start: 04h 08m 09s 484ms, eta: 01h 49m 46s 755ms
[32m2022-10-17T20:51:14 | mmf.trainers.callbacks.logistics: [39mprogress: 15500/22000, train/mmh/cross_entropy: 0.6064, train/mmh/cross_entropy/avg: 0.6255, train/total_loss: 0.6064, train/total_loss/avg: 0.6255, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 511ms, time_since_start: 04h 09m 24s 995ms, eta: 01h 47m 24s 497ms
[32m2022-10-17T20:52:30 | mmf.trainers.callbacks.logistics: [39mprogress: 15600/22000, train/mmh/cross_entropy: 0.6060, train/mmh/cross_entropy/avg: 0.6253, train/total_loss: 0.6060, train/total_loss/avg: 0.6253, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 401ms, time_since_start: 04h 10m 40s 397ms, eta: 01h 45m 36s 177ms
[32m2022-10-17T20:53:45 | mmf.trainers.callbacks.logistics: [39mprogress: 15700/22000, train/mmh/cross_entropy: 0.5855, train/mmh/cross_entropy/avg: 0.6251, train/total_loss: 0.5855, train/total_loss/avg: 0.6251, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 224ms, time_since_start: 04h 11m 55s 622ms, eta: 01h 43m 42s 496ms
[32m2022-10-17T20:55:00 | mmf.trainers.callbacks.logistics: [39mprogress: 15800/22000, train/mmh/cross_entropy: 0.6060, train/mmh/cross_entropy/avg: 0.6251, train/total_loss: 0.6060, train/total_loss/avg: 0.6251, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 263ms, time_since_start: 04h 13m 10s 885ms, eta: 01h 42m 06s 876ms
[32m2022-10-17T20:56:16 | mmf.trainers.callbacks.logistics: [39mprogress: 15900/22000, train/mmh/cross_entropy: 0.6060, train/mmh/cross_entropy/avg: 0.6261, train/total_loss: 0.6060, train/total_loss/avg: 0.6261, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 1.33, time: 01m 15s 339ms, time_since_start: 04h 14m 26s 224ms, eta: 01h 40m 34s 147ms
[32m2022-10-17T20:57:31 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T20:57:31 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T20:57:32 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T20:57:44 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T20:57:44 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, train/mmh/cross_entropy: 0.6064, train/mmh/cross_entropy/avg: 0.6273, train/total_loss: 0.6064, train/total_loss/avg: 0.6273, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 206ms, time_since_start: 04h 15m 54s 430ms, eta: 01h 55m 48s 890ms
[32m2022-10-17T20:57:44 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T20:57:44 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T20:57:44 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T21:00:45 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T21:00:45 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T21:00:45 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T21:00:53 | mmf.utils.checkpoint: [39mSaving best checkpoint
[32m2022-10-17T21:01:02 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T21:01:10 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T21:01:10 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, val/mmh/cross_entropy: 0.5402, val/total_loss: 0.5402, val/mmh/accuracy: 0.6602, val/mmh/binary_f1: 0.7336, val/mmh/roc_auc: 0.7244, num_updates: 16000, epoch: 2, iterations: 16000, max_updates: 22000, val_time: 03m 26s 268ms, best_update: 16000, best_iteration: 16000, best_val/mmh/roc_auc: 0.724402
[32m2022-10-17T21:02:26 | mmf.trainers.callbacks.logistics: [39mprogress: 16100/22000, train/mmh/cross_entropy: 0.6060, train/mmh/cross_entropy/avg: 0.6265, train/total_loss: 0.6060, train/total_loss/avg: 0.6265, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 769ms, time_since_start: 04h 20m 36s 471ms, eta: 01h 37m 49s 654ms
[32m2022-10-17T21:03:42 | mmf.trainers.callbacks.logistics: [39mprogress: 16200/22000, train/mmh/cross_entropy: 0.6060, train/mmh/cross_entropy/avg: 0.6260, train/total_loss: 0.6060, train/total_loss/avg: 0.6260, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 754ms, time_since_start: 04h 21m 52s 226ms, eta: 01h 36m 09s 035ms
[32m2022-10-17T21:04:57 | mmf.trainers.callbacks.logistics: [39mprogress: 16300/22000, train/mmh/cross_entropy: 0.5910, train/mmh/cross_entropy/avg: 0.6258, train/total_loss: 0.5910, train/total_loss/avg: 0.6258, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 423ms, time_since_start: 04h 23m 07s 649ms, eta: 01h 34m 04s 758ms
[32m2022-10-17T21:06:12 | mmf.trainers.callbacks.logistics: [39mprogress: 16400/22000, train/mmh/cross_entropy: 0.5855, train/mmh/cross_entropy/avg: 0.6251, train/total_loss: 0.5855, train/total_loss/avg: 0.6251, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 295ms, time_since_start: 04h 24m 22s 944ms, eta: 01h 32m 16s 310ms
[32m2022-10-17T21:07:28 | mmf.trainers.callbacks.logistics: [39mprogress: 16500/22000, train/mmh/cross_entropy: 0.5855, train/mmh/cross_entropy/avg: 0.6256, train/total_loss: 0.5855, train/total_loss/avg: 0.6256, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 255ms, time_since_start: 04h 25m 38s 200ms, eta: 01h 30m 34s 590ms
[32m2022-10-17T21:08:43 | mmf.trainers.callbacks.logistics: [39mprogress: 16600/22000, train/mmh/cross_entropy: 0.5855, train/mmh/cross_entropy/avg: 0.6252, train/total_loss: 0.5855, train/total_loss/avg: 0.6252, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 812ms, time_since_start: 04h 26m 54s 012ms, eta: 01h 29m 35s 247ms
[32m2022-10-17T21:09:58 | mmf.trainers.callbacks.logistics: [39mprogress: 16700/22000, train/mmh/cross_entropy: 0.5910, train/mmh/cross_entropy/avg: 0.6257, train/total_loss: 0.5910, train/total_loss/avg: 0.6257, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 122ms, time_since_start: 04h 28m 09s 135ms, eta: 01h 27m 07s 678ms
[32m2022-10-17T21:11:14 | mmf.trainers.callbacks.logistics: [39mprogress: 16800/22000, train/mmh/cross_entropy: 0.6034, train/mmh/cross_entropy/avg: 0.6256, train/total_loss: 0.6034, train/total_loss/avg: 0.6256, max mem: 5053.0, experiment: mmh-ft, epoch: 2, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 066ms, time_since_start: 04h 29m 24s 202ms, eta: 01h 25m 25s 269ms
[32m2022-10-17T21:12:29 | mmf.trainers.callbacks.logistics: [39mprogress: 16900/22000, train/mmh/cross_entropy: 0.6060, train/mmh/cross_entropy/avg: 0.6260, train/total_loss: 0.6060, train/total_loss/avg: 0.6260, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 826ms, time_since_start: 04h 30m 40s 028ms, eta: 01h 24m 37s 588ms
[32m2022-10-17T21:13:44 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T21:13:44 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T21:13:45 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T21:14:00 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T21:14:00 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, train/mmh/cross_entropy: 0.6034, train/mmh/cross_entropy/avg: 0.6253, train/total_loss: 0.6034, train/total_loss/avg: 0.6253, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 021ms, time_since_start: 04h 32m 11s 050ms, eta: 01h 39m 35s 549ms
[32m2022-10-17T21:14:00 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T21:14:00 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T21:14:00 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T21:17:04 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T21:17:04 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T21:17:04 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T21:17:14 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T21:17:25 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T21:17:25 | mmf.trainers.callbacks.logistics: [39mprogress: 17000/22000, val/mmh/cross_entropy: 0.5261, val/total_loss: 0.5261, val/mmh/accuracy: 0.6808, val/mmh/binary_f1: 0.7655, val/mmh/roc_auc: 0.7227, num_updates: 17000, epoch: 3, iterations: 17000, max_updates: 22000, val_time: 03m 24s 809ms, best_update: 16000, best_iteration: 16000, best_val/mmh/roc_auc: 0.724402
[32m2022-10-17T21:18:42 | mmf.trainers.callbacks.logistics: [39mprogress: 17100/22000, train/mmh/cross_entropy: 0.6034, train/mmh/cross_entropy/avg: 0.6244, train/total_loss: 0.6034, train/total_loss/avg: 0.6244, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 1.32, time: 01m 16s 713ms, time_since_start: 04h 36m 52s 575ms, eta: 01h 22m 15s 513ms
[32m2022-10-17T21:19:58 | mmf.trainers.callbacks.logistics: [39mprogress: 17200/22000, train/mmh/cross_entropy: 0.6034, train/mmh/cross_entropy/avg: 0.6248, train/total_loss: 0.6034, train/total_loss/avg: 0.6248, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 1.32, time: 01m 16s 557ms, time_since_start: 04h 38m 09s 132ms, eta: 01h 20m 24s 949ms
[32m2022-10-17T21:21:16 | mmf.trainers.callbacks.logistics: [39mprogress: 17300/22000, train/mmh/cross_entropy: 0.6034, train/mmh/cross_entropy/avg: 0.6248, train/total_loss: 0.6034, train/total_loss/avg: 0.6248, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 1.30, time: 01m 17s 871ms, time_since_start: 04h 39m 27s 003ms, eta: 01h 20m 05s 501ms
[32m2022-10-17T21:22:41 | mmf.trainers.callbacks.logistics: [39mprogress: 17400/22000, train/mmh/cross_entropy: 0.6034, train/mmh/cross_entropy/avg: 0.6253, train/total_loss: 0.6034, train/total_loss/avg: 0.6253, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 1.19, time: 01m 24s 579ms, time_since_start: 04h 40m 51s 583ms, eta: 01h 25m 08s 453ms
[32m2022-10-17T21:24:00 | mmf.trainers.callbacks.logistics: [39mprogress: 17500/22000, train/mmh/cross_entropy: 0.6034, train/mmh/cross_entropy/avg: 0.6246, train/total_loss: 0.6034, train/total_loss/avg: 0.6246, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 1.27, time: 01m 19s 239ms, time_since_start: 04h 42m 10s 823ms, eta: 01h 18m 01s 872ms
[32m2022-10-17T21:25:17 | mmf.trainers.callbacks.logistics: [39mprogress: 17600/22000, train/mmh/cross_entropy: 0.5910, train/mmh/cross_entropy/avg: 0.6238, train/total_loss: 0.5910, train/total_loss/avg: 0.6238, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 1.32, time: 01m 16s 943ms, time_since_start: 04h 43m 27s 766ms, eta: 01h 14m 05s 195ms
[32m2022-10-17T21:26:33 | mmf.trainers.callbacks.logistics: [39mprogress: 17700/22000, train/mmh/cross_entropy: 0.5910, train/mmh/cross_entropy/avg: 0.6234, train/total_loss: 0.5910, train/total_loss/avg: 0.6234, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 1.32, time: 01m 16s 372ms, time_since_start: 04h 44m 44s 139ms, eta: 01h 11m 51s 901ms
[32m2022-10-17T21:27:50 | mmf.trainers.callbacks.logistics: [39mprogress: 17800/22000, train/mmh/cross_entropy: 0.5849, train/mmh/cross_entropy/avg: 0.6232, train/total_loss: 0.5849, train/total_loss/avg: 0.6232, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 1.32, time: 01m 16s 009ms, time_since_start: 04h 46m 148ms, eta: 01h 09m 51s 615ms
[32m2022-10-17T21:29:06 | mmf.trainers.callbacks.logistics: [39mprogress: 17900/22000, train/mmh/cross_entropy: 0.5652, train/mmh/cross_entropy/avg: 0.6225, train/total_loss: 0.5652, train/total_loss/avg: 0.6225, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 1.32, time: 01m 16s 064ms, time_since_start: 04h 47m 16s 212ms, eta: 01h 08m 14s 761ms
[32m2022-10-17T21:30:22 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T21:30:22 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T21:30:24 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T21:30:36 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T21:30:36 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, train/mmh/cross_entropy: 0.5652, train/mmh/cross_entropy/avg: 0.6228, train/total_loss: 0.5652, train/total_loss/avg: 0.6228, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 663ms, time_since_start: 04h 48m 46s 875ms, eta: 01h 19m 21s 634ms
[32m2022-10-17T21:30:36 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T21:30:36 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T21:30:36 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T21:33:33 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T21:33:33 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T21:33:33 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T21:33:42 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T21:33:52 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T21:33:52 | mmf.trainers.callbacks.logistics: [39mprogress: 18000/22000, val/mmh/cross_entropy: 0.5250, val/total_loss: 0.5250, val/mmh/accuracy: 0.7060, val/mmh/binary_f1: 0.8063, val/mmh/roc_auc: 0.7144, num_updates: 18000, epoch: 3, iterations: 18000, max_updates: 22000, val_time: 03m 15s 991ms, best_update: 16000, best_iteration: 16000, best_val/mmh/roc_auc: 0.724402
[32m2022-10-17T21:35:09 | mmf.trainers.callbacks.logistics: [39mprogress: 18100/22000, train/mmh/cross_entropy: 0.5849, train/mmh/cross_entropy/avg: 0.6230, train/total_loss: 0.5849, train/total_loss/avg: 0.6230, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 1.30, time: 01m 17s 265ms, time_since_start: 04h 53m 20s 138ms, eta: 01h 05m 56s 518ms
[32m2022-10-17T21:36:26 | mmf.trainers.callbacks.logistics: [39mprogress: 18200/22000, train/mmh/cross_entropy: 0.5910, train/mmh/cross_entropy/avg: 0.6233, train/total_loss: 0.5910, train/total_loss/avg: 0.6233, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 1.32, time: 01m 16s 064ms, time_since_start: 04h 54m 36s 202ms, eta: 01h 03m 15s 166ms
[32m2022-10-17T21:37:41 | mmf.trainers.callbacks.logistics: [39mprogress: 18300/22000, train/mmh/cross_entropy: 0.6034, train/mmh/cross_entropy/avg: 0.6235, train/total_loss: 0.6034, train/total_loss/avg: 0.6235, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 886ms, time_since_start: 04h 55m 52s 088ms, eta: 01h 01m 26s 622ms
[32m2022-10-17T21:38:57 | mmf.trainers.callbacks.logistics: [39mprogress: 18400/22000, train/mmh/cross_entropy: 0.6359, train/mmh/cross_entropy/avg: 0.6239, train/total_loss: 0.6359, train/total_loss/avg: 0.6239, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 205ms, time_since_start: 04h 57m 07s 293ms, eta: 59m 14s 793ms
[32m2022-10-17T21:40:12 | mmf.trainers.callbacks.logistics: [39mprogress: 18500/22000, train/mmh/cross_entropy: 0.6034, train/mmh/cross_entropy/avg: 0.6230, train/total_loss: 0.6034, train/total_loss/avg: 0.6230, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 196ms, time_since_start: 04h 58m 22s 490ms, eta: 57m 35s 665ms
[32m2022-10-17T21:41:27 | mmf.trainers.callbacks.logistics: [39mprogress: 18600/22000, train/mmh/cross_entropy: 0.6359, train/mmh/cross_entropy/avg: 0.6232, train/total_loss: 0.6359, train/total_loss/avg: 0.6232, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 332ms, time_since_start: 04h 59m 37s 822ms, eta: 56m 02s 984ms
[32m2022-10-17T21:42:42 | mmf.trainers.callbacks.logistics: [39mprogress: 18700/22000, train/mmh/cross_entropy: 0.6214, train/mmh/cross_entropy/avg: 0.6232, train/total_loss: 0.6214, train/total_loss/avg: 0.6232, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 058ms, time_since_start: 05h 52s 881ms, eta: 54m 12s 206ms
[32m2022-10-17T21:43:58 | mmf.trainers.callbacks.logistics: [39mprogress: 18800/22000, train/mmh/cross_entropy: 0.6323, train/mmh/cross_entropy/avg: 0.6233, train/total_loss: 0.6323, train/total_loss/avg: 0.6233, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 488ms, time_since_start: 05h 02m 08s 370ms, eta: 52m 51s 742ms
[32m2022-10-17T21:45:13 | mmf.trainers.callbacks.logistics: [39mprogress: 18900/22000, train/mmh/cross_entropy: 0.6274, train/mmh/cross_entropy/avg: 0.6233, train/total_loss: 0.6274, train/total_loss/avg: 0.6233, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 303ms, time_since_start: 05h 03m 23s 673ms, eta: 51m 05s 060ms
[32m2022-10-17T21:46:28 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T21:46:28 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T21:46:30 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T21:46:44 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T21:46:44 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, train/mmh/cross_entropy: 0.6274, train/mmh/cross_entropy/avg: 0.6228, train/total_loss: 0.6274, train/total_loss/avg: 0.6228, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 327ms, time_since_start: 05h 04m 55s 000ms, eta: 59m 57s 390ms
[32m2022-10-17T21:46:44 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T21:46:44 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T21:46:44 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T21:49:46 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T21:49:46 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T21:49:46 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T21:49:57 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T21:50:06 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T21:50:06 | mmf.trainers.callbacks.logistics: [39mprogress: 19000/22000, val/mmh/cross_entropy: 0.5379, val/total_loss: 0.5379, val/mmh/accuracy: 0.6656, val/mmh/binary_f1: 0.7460, val/mmh/roc_auc: 0.7154, num_updates: 19000, epoch: 3, iterations: 19000, max_updates: 22000, val_time: 03m 21s 319ms, best_update: 16000, best_iteration: 16000, best_val/mmh/roc_auc: 0.724402
[32m2022-10-17T21:51:21 | mmf.trainers.callbacks.logistics: [39mprogress: 19100/22000, train/mmh/cross_entropy: 0.6323, train/mmh/cross_entropy/avg: 0.6234, train/total_loss: 0.6323, train/total_loss/avg: 0.6234, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 715ms, time_since_start: 05h 09m 32s 040ms, eta: 48m 03s 006ms
[32m2022-10-17T21:52:37 | mmf.trainers.callbacks.logistics: [39mprogress: 19200/22000, train/mmh/cross_entropy: 0.6274, train/mmh/cross_entropy/avg: 0.6233, train/total_loss: 0.6274, train/total_loss/avg: 0.6233, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 835ms, time_since_start: 05h 10m 47s 876ms, eta: 46m 28s 019ms
[32m2022-10-17T21:53:53 | mmf.trainers.callbacks.logistics: [39mprogress: 19300/22000, train/mmh/cross_entropy: 0.6214, train/mmh/cross_entropy/avg: 0.6224, train/total_loss: 0.6214, train/total_loss/avg: 0.6224, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 620ms, time_since_start: 05h 12m 03s 497ms, eta: 44m 40s 828ms
[32m2022-10-17T21:55:08 | mmf.trainers.callbacks.logistics: [39mprogress: 19400/22000, train/mmh/cross_entropy: 0.6214, train/mmh/cross_entropy/avg: 0.6225, train/total_loss: 0.6214, train/total_loss/avg: 0.6225, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 181ms, time_since_start: 05h 13m 18s 678ms, eta: 42m 46s 552ms
[32m2022-10-17T21:56:23 | mmf.trainers.callbacks.logistics: [39mprogress: 19500/22000, train/mmh/cross_entropy: 0.6274, train/mmh/cross_entropy/avg: 0.6226, train/total_loss: 0.6274, train/total_loss/avg: 0.6226, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 395ms, time_since_start: 05h 14m 34s 074ms, eta: 41m 14s 855ms
[32m2022-10-17T21:57:39 | mmf.trainers.callbacks.logistics: [39mprogress: 19600/22000, train/mmh/cross_entropy: 0.6274, train/mmh/cross_entropy/avg: 0.6220, train/total_loss: 0.6274, train/total_loss/avg: 0.6220, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 566ms, time_since_start: 05h 15m 49s 641ms, eta: 39m 41s 261ms
[32m2022-10-17T21:58:54 | mmf.trainers.callbacks.logistics: [39mprogress: 19700/22000, train/mmh/cross_entropy: 0.6274, train/mmh/cross_entropy/avg: 0.6215, train/total_loss: 0.6274, train/total_loss/avg: 0.6215, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 011ms, time_since_start: 05h 17m 04s 652ms, eta: 37m 45s 278ms
[32m2022-10-17T22:00:09 | mmf.trainers.callbacks.logistics: [39mprogress: 19800/22000, train/mmh/cross_entropy: 0.6274, train/mmh/cross_entropy/avg: 0.6210, train/total_loss: 0.6274, train/total_loss/avg: 0.6210, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 006ms, time_since_start: 05h 18m 19s 659ms, eta: 36m 06s 633ms
[32m2022-10-17T22:01:24 | mmf.trainers.callbacks.logistics: [39mprogress: 19900/22000, train/mmh/cross_entropy: 0.6274, train/mmh/cross_entropy/avg: 0.6209, train/total_loss: 0.6274, train/total_loss/avg: 0.6209, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 1.33, time: 01m 15s 232ms, time_since_start: 05h 19m 34s 891ms, eta: 34m 34s 390ms
[32m2022-10-17T22:02:39 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T22:02:39 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T22:02:41 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T22:02:53 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T22:02:53 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, train/mmh/cross_entropy: 0.6214, train/mmh/cross_entropy/avg: 0.6205, train/total_loss: 0.6214, train/total_loss/avg: 0.6205, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 357ms, time_since_start: 05h 21m 03s 249ms, eta: 38m 40s 257ms
[32m2022-10-17T22:02:53 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T22:02:53 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T22:02:53 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T22:05:54 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T22:05:54 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T22:05:54 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T22:06:03 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T22:06:12 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T22:06:12 | mmf.trainers.callbacks.logistics: [39mprogress: 20000/22000, val/mmh/cross_entropy: 0.5424, val/total_loss: 0.5424, val/mmh/accuracy: 0.6682, val/mmh/binary_f1: 0.7518, val/mmh/roc_auc: 0.7140, num_updates: 20000, epoch: 3, iterations: 20000, max_updates: 22000, val_time: 03m 19s 066ms, best_update: 16000, best_iteration: 16000, best_val/mmh/roc_auc: 0.724402
[32m2022-10-17T22:07:27 | mmf.trainers.callbacks.logistics: [39mprogress: 20100/22000, train/mmh/cross_entropy: 0.6214, train/mmh/cross_entropy/avg: 0.6207, train/total_loss: 0.6214, train/total_loss/avg: 0.6207, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 607ms, time_since_start: 05h 25m 37s 929ms, eta: 31m 26s 191ms
[32m2022-10-17T22:08:43 | mmf.trainers.callbacks.logistics: [39mprogress: 20200/22000, train/mmh/cross_entropy: 0.6011, train/mmh/cross_entropy/avg: 0.6203, train/total_loss: 0.6011, train/total_loss/avg: 0.6203, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 366ms, time_since_start: 05h 26m 53s 296ms, eta: 29m 41s 208ms
[32m2022-10-17T22:09:58 | mmf.trainers.callbacks.logistics: [39mprogress: 20300/22000, train/mmh/cross_entropy: 0.5959, train/mmh/cross_entropy/avg: 0.6200, train/total_loss: 0.5959, train/total_loss/avg: 0.6200, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.35, time: 01m 14s 886ms, time_since_start: 05h 28m 08s 182ms, eta: 27m 51s 542ms
[32m2022-10-17T22:11:13 | mmf.trainers.callbacks.logistics: [39mprogress: 20400/22000, train/mmh/cross_entropy: 0.5826, train/mmh/cross_entropy/avg: 0.6198, train/total_loss: 0.5826, train/total_loss/avg: 0.6198, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 137ms, time_since_start: 05h 29m 23s 320ms, eta: 26m 18s 487ms
[32m2022-10-17T22:12:28 | mmf.trainers.callbacks.logistics: [39mprogress: 20500/22000, train/mmh/cross_entropy: 0.5826, train/mmh/cross_entropy/avg: 0.6196, train/total_loss: 0.5826, train/total_loss/avg: 0.6196, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 133ms, time_since_start: 05h 30m 38s 453ms, eta: 24m 39s 757ms
[32m2022-10-17T22:13:43 | mmf.trainers.callbacks.logistics: [39mprogress: 20600/22000, train/mmh/cross_entropy: 0.5826, train/mmh/cross_entropy/avg: 0.6199, train/total_loss: 0.5826, train/total_loss/avg: 0.6199, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 387ms, time_since_start: 05h 31m 53s 841ms, eta: 23m 05s 776ms
[32m2022-10-17T22:14:59 | mmf.trainers.callbacks.logistics: [39mprogress: 20700/22000, train/mmh/cross_entropy: 0.5826, train/mmh/cross_entropy/avg: 0.6200, train/total_loss: 0.5826, train/total_loss/avg: 0.6200, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 728ms, time_since_start: 05h 33m 09s 570ms, eta: 21m 32s 611ms
[32m2022-10-17T22:16:15 | mmf.trainers.callbacks.logistics: [39mprogress: 20800/22000, train/mmh/cross_entropy: 0.5826, train/mmh/cross_entropy/avg: 0.6205, train/total_loss: 0.5826, train/total_loss/avg: 0.6205, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 618ms, time_since_start: 05h 34m 25s 188ms, eta: 19m 51s 443ms
[32m2022-10-17T22:17:30 | mmf.trainers.callbacks.logistics: [39mprogress: 20900/22000, train/mmh/cross_entropy: 0.5720, train/mmh/cross_entropy/avg: 0.6202, train/total_loss: 0.5720, train/total_loss/avg: 0.6202, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 657ms, time_since_start: 05h 35m 40s 845ms, eta: 18m 12s 714ms
[32m2022-10-17T22:18:46 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T22:18:46 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T22:18:48 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T22:18:59 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T22:18:59 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, train/mmh/cross_entropy: 0.5826, train/mmh/cross_entropy/avg: 0.6213, train/total_loss: 0.5826, train/total_loss/avg: 0.6213, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 693ms, time_since_start: 05h 37m 09s 538ms, eta: 19m 24s 540ms
[32m2022-10-17T22:18:59 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T22:18:59 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T22:18:59 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T22:22:01 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T22:22:01 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T22:22:01 | mmf.trainers.callbacks.logistics: [39mprogress: 21000/22000, val/mmh/cross_entropy: 0.5333, val/total_loss: 0.5333, val/mmh/accuracy: 0.6834, val/mmh/binary_f1: 0.7741, val/mmh/roc_auc: 0.7131, num_updates: 21000, epoch: 3, iterations: 21000, max_updates: 22000, val_time: 03m 02s 263ms, best_update: 16000, best_iteration: 16000, best_val/mmh/roc_auc: 0.724402
[32m2022-10-17T22:23:17 | mmf.trainers.callbacks.logistics: [39mprogress: 21100/22000, train/mmh/cross_entropy: 0.5720, train/mmh/cross_entropy/avg: 0.6209, train/total_loss: 0.5720, train/total_loss/avg: 0.6209, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 963ms, time_since_start: 05h 41m 27s 768ms, eta: 14m 57s 661ms
[32m2022-10-17T22:24:33 | mmf.trainers.callbacks.logistics: [39mprogress: 21200/22000, train/mmh/cross_entropy: 0.5720, train/mmh/cross_entropy/avg: 0.6207, train/total_loss: 0.5720, train/total_loss/avg: 0.6207, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 505ms, time_since_start: 05h 42m 43s 273ms, eta: 13m 13s 110ms
[32m2022-10-17T22:25:48 | mmf.trainers.callbacks.logistics: [39mprogress: 21300/22000, train/mmh/cross_entropy: 0.5766, train/mmh/cross_entropy/avg: 0.6207, train/total_loss: 0.5766, train/total_loss/avg: 0.6207, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.35, time: 01m 14s 950ms, time_since_start: 05h 43m 58s 223ms, eta: 11m 28s 865ms
[32m2022-10-17T22:27:03 | mmf.trainers.callbacks.logistics: [39mprogress: 21400/22000, train/mmh/cross_entropy: 0.5766, train/mmh/cross_entropy/avg: 0.6212, train/total_loss: 0.5766, train/total_loss/avg: 0.6212, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 130ms, time_since_start: 05h 45m 13s 354ms, eta: 09m 51s 877ms
[32m2022-10-17T22:28:18 | mmf.trainers.callbacks.logistics: [39mprogress: 21500/22000, train/mmh/cross_entropy: 0.5720, train/mmh/cross_entropy/avg: 0.6209, train/total_loss: 0.5720, train/total_loss/avg: 0.6209, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 278ms, time_since_start: 05h 46m 28s 632ms, eta: 08m 14s 202ms
[32m2022-10-17T22:29:34 | mmf.trainers.callbacks.logistics: [39mprogress: 21600/22000, train/mmh/cross_entropy: 0.5766, train/mmh/cross_entropy/avg: 0.6208, train/total_loss: 0.5766, train/total_loss/avg: 0.6208, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 871ms, time_since_start: 05h 47m 44s 504ms, eta: 06m 38s 475ms
[32m2022-10-17T22:30:49 | mmf.trainers.callbacks.logistics: [39mprogress: 21700/22000, train/mmh/cross_entropy: 0.5826, train/mmh/cross_entropy/avg: 0.6210, train/total_loss: 0.5826, train/total_loss/avg: 0.6210, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 334ms, time_since_start: 05h 48m 59s 838ms, eta: 04m 56s 744ms
[32m2022-10-17T22:32:05 | mmf.trainers.callbacks.logistics: [39mprogress: 21800/22000, train/mmh/cross_entropy: 0.6011, train/mmh/cross_entropy/avg: 0.6218, train/total_loss: 0.6011, train/total_loss/avg: 0.6218, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 587ms, time_since_start: 05h 50m 15s 425ms, eta: 03m 18s 491ms
[32m2022-10-17T22:33:20 | mmf.trainers.callbacks.logistics: [39mprogress: 21900/22000, train/mmh/cross_entropy: 0.6072, train/mmh/cross_entropy/avg: 0.6219, train/total_loss: 0.6072, train/total_loss/avg: 0.6219, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.33, time: 01m 15s 051ms, time_since_start: 05h 51m 30s 477ms, eta: 01m 38s 542ms
[32m2022-10-17T22:34:35 | mmf.trainers.callbacks.checkpoint: [39mCheckpoint time. Saving a checkpoint.
[32m2022-10-17T22:34:35 | mmf.utils.checkpoint: [39mCheckpoint save operation started!
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[32m2022-10-17T22:34:37 | mmf.utils.checkpoint: [39mSaving current checkpoint
[32m2022-10-17T22:34:52 | mmf.utils.checkpoint: [39mCheckpoint save operation finished!
[32m2022-10-17T22:34:52 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, train/mmh/cross_entropy: 0.6072, train/mmh/cross_entropy/avg: 0.6215, train/total_loss: 0.6072, train/total_loss/avg: 0.6215, max mem: 5053.0, experiment: mmh-ft, epoch: 3, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 1.09, time: 01m 32s 010ms, time_since_start: 05h 53m 02s 488ms, eta: 0ms
[32m2022-10-17T22:34:52 | mmf.trainers.core.training_loop: [39mEvaluation time. Running on full validation set...
[32m2022-10-17T22:34:52 | mmf.common.test_reporter: [39mPredicting for mmh
[31m[5mWARNING[39m[25m [32m2022-10-17T22:34:52 | mmf.utils.build: [39mpersistent_workers cannot be used together with num_workers == 0; setting persistent_workers to False
[32m2022-10-17T22:37:55 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T22:37:55 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T22:37:55 | mmf.trainers.callbacks.logistics: [39mprogress: 22000/22000, val/mmh/cross_entropy: 0.5363, val/total_loss: 0.5363, val/mmh/accuracy: 0.6804, val/mmh/binary_f1: 0.7658, val/mmh/roc_auc: 0.7137, num_updates: 22000, epoch: 3, iterations: 22000, max_updates: 22000, val_time: 03m 03s 344ms, best_update: 16000, best_iteration: 16000, best_val/mmh/roc_auc: 0.724402
[32m2022-10-17T22:37:55 | mmf.trainers.core.training_loop: [39mStepping into final validation check
[32m2022-10-17T22:37:55 | mmf.utils.checkpoint: [39mRestoring checkpoint
[32m2022-10-17T22:37:55 | mmf.utils.checkpoint: [39mLoading checkpoint
[32m2022-10-17T22:38:03 | mmf.utils.checkpoint: [39mCheckpoint loaded.
[32m2022-10-17T22:38:03 | mmf.utils.checkpoint: [39mCurrent num updates: 16000
[32m2022-10-17T22:38:03 | mmf.utils.checkpoint: [39mCurrent iteration: 16000
[32m2022-10-17T22:38:03 | mmf.utils.checkpoint: [39mCurrent epoch: 2
  1% 3/313 [00:00<00:33,  9.33it/s]
[32m2022-10-17T22:38:03 | mmf.utils.checkpoint: [39mSalvando o modelo final..
[32m2022-10-17T22:38:03 | mmf.trainers.mmf_trainer: [39mStarting inference on val set
[32m2022-10-17T22:38:03 | mmf.common.test_reporter: [39mPredicting for mmh





















 98% 307/313 [00:42<00:00,  7.17it/s]
[32m2022-10-17T22:38:47 | mmf.trainers.core.evaluation_loop: [39mFinished training. Loaded 313
[32m2022-10-17T22:38:47 | mmf.trainers.core.evaluation_loop: [39m -- skipped 0 batches.
[32m2022-10-17T22:38:47 | mmf.trainers.callbacks.logistics: [39mprogress: 16000/22000, val/mmh/cross_entropy: 0.5402, val/total_loss: 0.5402, val/mmh/accuracy: 0.6602, val/mmh/binary_f1: 0.7336, val/mmh/roc_auc: 0.7244

100% 313/313 [00:43<00:00,  7.24it/s]